# hadoop完全分布式搭建

## 1. 克隆虚拟机

```shell
集群环境：
centOs6.8：hadoop102，hadoop103，hadoop104
jdk版本：jdk1.8.0_144
hadoop版本：Hadoop 2.7.2
首先准备三台客户机（hadoop102，hadoop103，hadoop104），关闭防火墙，修改为静态ip和ip地址映射

1 删除重复的eth0
	vim /etc/udev/rules.d/70-persistent-net.rules
2 修改静态IP
	vim /etc/sysconfig/network-scripts/ifcfg-eht0
3 修改主机名
	vim /etc/sysconfig/network
4 查看主机映射(母机和win10上已经配置过了)
	vim /etc/hosts
5 查看防火墙状态(母机上是关闭状态)
	service iptables status
6 解压/opt/software下 jdk和hadoop
	如母机没有jdk和hadoop手动上传一下
	tar -zxvf RPM包名 -C 解压路径
7 查看环境变量(母机上已经配置过了)
	vim /etc/profile
```

## 2. 集群部署规划

<img src="C:\Users\childwen\AppData\Roaming\Typora\typora-user-images\image-20200218233654245.png" alt="image-20200218233654245" style="zoom:80%;" />

```
注意: 明确NameNode、SecondDaryNode和ResourceManager所在节点
```



## 3. SSH无密登录配置

#### 生成公钥和秘钥

```shell
ssh-keygen -t rsa
敲三个回车就会生成两个文件
id_rsa（私钥）、id_rsa.pub（公钥）
```

#### 公钥拷贝到客户机

```shell
将公钥拷贝到要免密登录的目标机器上
ssh-copy-id hadoop102
ssh-copy-id hadoop103
ssh-copy-id hadoop104
注意：ssh访问自己也需要输入密码，所以我们需要将公钥也拷贝给102
注意: 配置了NameNode和resourcemanager的客户机都需要无密登录权限
```



## 4. 编写分发脚本

```shell
scp服务器之间数据拷贝
    scp 	-r 	  $pdir/$fname	 	$user@hadoop$host:$pdir/$fname
    命令 	  递归 要拷贝的文件路径/名称	目的用户@主机:目的路径/名称
rsync远程同步
    rsync -av 		$pdir/$fname 		$user@hadoop$host:$pdir/fname
    命令	选项参数	要拷贝的文件路径/名称		目的用户@主机:目的路径/名称
 ------------------------------------------------------------------------------------
 在用户家目录文件夹下创建bin目录根据rsync代码编写分发脚本:
 
#!/bin/bash
#1 获取输入参数个数，如果没有参数，直接退出
pcount=$#
if((pcount==0)); then
echo no args;
exit;
fi

#2 获取文件名称
p1=$1
fname=`basename $p1`
echo fname=$fname

#3 获取上级目录到绝对路径 –P指向实际物理地址，防止软连接
pdir=`cd -P $(dirname $p1); pwd`
echo pdir=$pdir

#4 获取当前用户名称
user=`whoami`

#5 循环
for((host=103; host<105; host++)); do
        echo ------------------- hadoop$host --------------
        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir
done
```

### 4.1 分发脚本到节点

```shell
修改脚本xsync具有执行权限，并调用脚本，将脚本复制到103和104节点
chmod 777 xsync
xsync /home/atguigu/bin
```





## 4. 更改集群配置文件

##### core核心配置文件

```
配置core-site.xml

<!-- 指定HDFS中NameNode的地址 -->
<property>
		<name>fs.defaultFS</name>
      <value>hdfs://hadoop102:9000</value>
</property>

<!-- 指定Hadoop运行时产生文件的存储目录 -->
<property>
		<name>hadoop.tmp.dir</name>
		<value>/opt/module/hadoop-2.7.2/data/tmp</value>
</property>
```

##### HDFS配置文件

```
注意：我们已经在/etc/profile文件中配置了JAVA_HOME，这里为什么还需要配置JAVA_HOME?

答：因为Hadoop运行是守护进程（守护进程是一个在后台运行并且不受任何终端控制的进程。--摘自百度百科），正是因为它后台运行，不接受任何终端控制，所以它读取不到我们配置好的环境变量，所以这里需要单独配置一下。


配置hadoop-env.sh
export JAVA_HOME=/opt/module/jdk1.8.0_144

配置hdfs-site.xml
<property>
		<name>dfs.replication</name>
		<value>3</value>
</property>

<!-- 指定Hadoop辅助名称节点主机配置 -->
<property>
      <name>dfs.namenode.secondary.http-address</name>
      <value>hadoop104:50090</value>
</property>
```

##### YARN配置文件

```
配置yarn-env.sh
export JAVA_HOME=/opt/module/jdk1.8.0_144

配置yarn-site.xml
<!-- Reducer获取数据的方式 -->
<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
</property>

<!-- 指定YARN的ResourceManager的地址 -->
<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>hadoop103</value>
</property>

=========yarn-site.xml配置聚集日志===============

<!-- 日志聚集功能 -->
<property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
</property>

<!-- 日志保留时间设置7天 -->
<property>
    <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
</property>
```

MapReduce配置文件

```
配置mapred-env.sh
export JAVA_HOME=/opt/module/jdk1.8.0_144

配置mapred-site.xml
 
注:第一次配置需要将mapred-site.xml.template文件重命名为mapred-site.xml
<!-- 指定MR运行在Yarn上 -->
<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
</property>

==============mapred-site.xml下配置历史服务器==================

<!-- 历史服务器端地址 -->
<property>
    <name>mapreduce.jobhistory.address</name>
    <value>hadoop102:10020</value>
</property>
<!-- 历史服务器web端地址 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop102:19888</value>
</property>
```

##  5. 分发配置文件

```shell
指定到客户机的配置文件目录
xsync /opt/module/hadoop-2.7.2/etc/hadoop/
```

## 6. 群起集群

##### 配置slaves

```shell
1. 配置slaves
2. 切换目录到：hadoop安装目录/etc/hadoop/
3. 在目录下的slaves文件中添加如下内容
# 注意结尾不能有空格，文件中不能有空行
hadoop102
hadoop103
hadoop104
同步所有节点配置文件
xsync slaves
```

##### 整体启停HDFS

```
start-dfs.sh
stop-dfs.sh
```

##### 整体启停YARN

```
start-yarn.sh
stop-yarn.sh
```

##### 单起HDFS组件

```
hadoop-daemon.sh start namenode/datanode/secondarynamenode
hadoop-daemon.sh stop namenode/datanode/secondarynamenode
```

##### 单起YARN

```
yarn-daemon.sh start resourcemanager/nodemanager
yarn-daemon.sh stop resourcemanager/nodemanager
```

##### 启动历史服务器

```
sbin/mr-jobhistory-daemon.sh start historyserver
```



## 7. 集群时间同步

##### 配置时间服务器

```shell
1 检查ntp是否安装(必须是root用户配置)
rpm -qa|grep ntp
	ntp-4.2.6p5-10.el6.centos.x86_64
    fontpackages-filesystem-1.41-1.1.el6.noarch
    ntpdate-4.2.6p5-10.el6.centos.x86_64
2 修改ntp配置文件
vi /etc/ntp.conf
	修改一: 授权192.168.1.0-192.168.1.255网段上的所有机器可以从这台机器上查询和同步时间
		#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap 
			restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap
	修改二: 集群在局域网中，不使用其他互联网上的时间
        server 0.centos.pool.ntp.org iburst
        server 1.centos.pool.ntp.org iburst
        server 2.centos.pool.ntp.org iburst
        server 3.centos.pool.ntp.org iburst	
        	将上面这四段注销即可
    添加三: 当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步
    	server 127.127.1.0
		fudge 127.127.1.0 stratum 10	
3 修改/etc/sysconfig/ntpd 文件
	vim /etc/sysconfig/ntpd
		增加内容:让硬件时间与系统时间一起同步
		SYNC_HWCLOCK=yes
4 重新启动ntpd服务
	service ntpd status
	service ntpd start
5 设置开机自启动
	chkconfig ntpd on
6 其他机器配置(必须是root用户)
	1)配置10分钟与时间服务器同步一次
		crontab -e
		*/10 * * * * /usr/sbin/ntpdate hadoop102
	2)修改任意机器时间测试
		date -s "2017-9-11 11:11:11"
```

