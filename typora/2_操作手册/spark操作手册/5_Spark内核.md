# 1.  Spark é›†ç¾¤å¯åŠ¨æµç¨‹åˆ†æ

## 1.1 sbin/start-all.sh

```bash
# sbin/start-all.sh ä¸­ä¼šè°ƒç”¨ start-master.shã€start-slaves.sh
    "${SPARK_HOME}/sbin"/start-master.sh
    "${SPARK_HOME}/sbin"/start-slaves.sh
# -----------------------------------------------------------------------------
# start-master.sh 
# ï¼ˆ1ï¼‰é…ç½® ${SPARK_HOME} ä¸‹çš„é…ç½®è„šæœ¬ï¼ˆå¦‚æœæ²¡æœ‰è®¾ç½® SPARK_HOME ç¯å¢ƒå˜é‡ï¼Œå°†å½“å‰è„šæœ¬çš„ä¸Šä¸€çº§ç›®å½•ä½œä¸º SPARK_HOMEï¼‰
    "${SPARK_HOME}/sbin/spark-config.sh"
    "${SPARK_HOME}/bin/load-spark-env.sh"
# ï¼ˆ2ï¼‰é…ç½®ä¸»æœºåï¼ˆé»˜è®¤å½“å‰ä¸»æœºåï¼‰ã€é€šä¿¡ç«¯å£ï¼ˆé»˜è®¤7077ï¼‰ã€webUI ç«¯å£ï¼ˆé»˜è®¤8080ï¼‰
# ï¼ˆ3ï¼‰è°ƒç”¨ spark-daemon.sh è„šæœ¬ï¼Œä¼ å‚ org.apache.spark.deploy.master.Master
    CLASS="org.apache.spark.deploy.master.Master"
        "${SPARK_HOME}/sbin"/spark-daemon.sh start $CLASS 1 \
        --host $SPARK_MASTER_HOST  \
        --port $SPARK_MASTER_PORT  \
        --webui-port $SPARK_MASTER_WEBUI_PORT \
        $ORIGINAL_ARGS
# ------------------------------------------------------------------------------
# spark-daemon.sh 
# é…ç½®æ—¥å¿—ç›®å½•ï¼Œè®°å½•æ—¥å¿—
# ä¸Šä¸€æ­¥çš„å‚æ•°ä¼šè¿›å…¥ run_command å‡½æ•°
    $option=start
    command=$1

    run_command class "$@"

    mode="$1"  # mode=class
# è¿›è€Œè¿›å…¥ execute_command å‡½æ•°ï¼Œè¿è¡Œ spark-class è„šæœ¬
    execute_command nice -n "$SPARK_NICENESS" "${SPARK_HOME}"/bin/spark-class "$command" "$@"
    nohup -- "$@" >> $log 2>&1 < /dev/null &

# ------------------------------------------------------------------------------
# spark-class 
# æœ€ç»ˆæ‰§è¡Œçš„ "${CMD[@]}" å‘½ä»¤åœ¨æ§åˆ¶å°æ‰“å‡ºåæ˜¯ä¸‹æ–¹çš„ä¸€é•¿ä¸²ï¼š
    CMD=("${CMD[@]:0:$LAST}")
    exec "${CMD[@]}"

    /opt/module/jdk1.8.0_144/bin/java 
    -cp /opt/module/spark-standalone/conf/:/opt/module/spark-standalone/jars/* -Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop102:2181,hadoop103:2181,hadoop104:2181 -Dspark.deploy.zookeeper.dir=/spark1128 -Xmx1g 
    org.apache.spark.deploy.master.Master 
    --host hadoop102 --port 7077 --webui-port 8080
# è‡³æ­¤ï¼Œå®Œæˆ Master ç±»çš„å¯åŠ¨
#--------------------------------------------------------------------------------
# Worker ç±»çš„å¯åŠ¨
# start-slaves.sh
    "${SPARK_HOME}/sbin"/start-slaves.sh
#--------------------------------------------------------------------------------
# éšåè°ƒç”¨ ssh è°ƒç”¨å„ä¸ªèŠ‚ç‚¹ä¸Šçš„ start-slave.sh
# å®é™…æ˜¯æ‰§è¡Œçš„java -cp Worker
# åé¢çš„æ­¥éª¤ä¸ Master ç±»çš„å¯åŠ¨ç±»ä¼¼ï¼Œæ­¤å¤„ä¸å†èµ˜è¿°ã€‚æœ€åæ‰§è¡Œçš„å†…å®¹ä¸ºï¼š
opt/module/jdk1.8.0_144/bin/java 
-cp /opt/module/spark-standalone/conf/:/opt/module/spark-standalone/jars/* 
-Dspark.deploy.recoveryMode=ZOOKEEPER 
-Dspark.deploy.zookeeper.url=hadoop102:2181,hadoop103:2181,hadoop104:2181 
-Dspark.deploy.zookeeper.dir=/spark1128 -Xmx1g 
org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://hadoop102:7077

```

## 1.2 å¯åŠ¨ Master

```scala
//  The life-cycle of an endpoint is:
//  constructor -> onStart -> receive* -> onStop
// constructor -----------------------------------------------------------------
// ä¼´ç”Ÿå¯¹è±¡ä¸­çš„ main æ ¹æ®é€šä¿¡å‚æ•°åˆ›å»º Master ç«¯çš„ RpcEnv ç¯å¢ƒ, å¹¶å¯åŠ¨ RpcEnv
val rpcEnv: RpcEnv = RpcEnv.create(SYSTEM_NAME, host, port, conf, securityMgr)
// æœ¬è´¨æ˜¯ NettyRpcEnv

// æ ¹æ® RpcEnv åˆ›å»ºä¸€ä¸ª Master å¯¹è±¡
val masterEndpoint: RpcEndpointRef = rpcEnv.setupEndpoint(ENDPOINT_NAME,
            new Master(rpcEnv, rpcEnv.address, webUiPort, securityMgr, conf)) 

// java ä¸­çš„çº¿ç¨‹æ± ï¼šåªèƒ½è·‘ä¸€ä¸ªåå°çº¿ç¨‹çš„çº¿ç¨‹æ± ï¼ˆ Master å¯¹è±¡å±æ€§ï¼‰
private val forwardMessageThread: ScheduledExecutorService =
    ThreadUtils.newDaemonSingleThreadScheduledExecutor("master-forward-message-thread")
// onStart ----------------------------------------------------------------------
// ä¼ é€’çš„çº¿ç¨‹, ä¼šæ¯éš”60sæ‰§è¡Œä¸€æ¬¡!
checkForWorkerTimeOutTask = forwardMessageThread.scheduleAtFixedRate(new Runnable {
            override def run(): Unit = Utils.tryLogNonFatalError {
                self.send(CheckForWorkerTimeOut) // ç»™è‡ªå·±å‘é€ä¿¡æ¯
            }
        }, 0, WORKER_TIMEOUT_MS, TimeUnit.MILLISECONDS)
// 0: å»¶è¿Ÿæ‰§è¡Œçš„æ—¶é—´
// WORKER_TIMEOUT_MS:æ‰§è¡Œé¢‘ç‡ï¼ˆ60*1000ï¼‰
            case CheckForWorkerTimeOut =>
                        // ç§»é™¤è¶…æ—¶çš„workers
                        timeOutDeadWorkers()
// æ‰€æœ‰æ³¨å†ŒæˆåŠŸçš„ worker åˆ›å»ºä¸€ä¸ª set é›†åˆï¼ˆååºè¿‡ç¨‹ä¸­ä¼šå†åˆ›å»ºä¸¤ä¸ª Map é›†åˆï¼‰
val workers = new HashSet[WorkerInfo]
    private val idToWorker = new HashMap[String, WorkerInfo]
    private val addressToWorker = new HashMap[RpcAddress, WorkerInfo]
// timeOutDeadWorkers() æ¯60ç§’åˆ¤æ–­ä¸€æ¬¡ work æ˜¯å¦è¶…æ—¶(å¤±è”60s), å¦‚æœè¶…æ—¶, åˆ™å¼‚å¸¸. é€šè¿‡ä¸Šæ¬¡çš„å¿ƒè·³æ—¶é—´æ¥ç¡®å®š
/*
æ€»ç»“: 
    åˆ›å»ºä¸€ä¸ªå•ä¾‹çº¿ç¨‹ï¼Œæ¯ 60 ç§’åˆ¤æ–­ä¸€æ¬¡ work æ˜¯å¦è¶…æ—¶(å¤±è”60s), å¦‚æœè¶…æ—¶, åˆ™å¼‚å¸¸ï¼ˆé€šè¿‡ä¸Šæ¬¡çš„å¿ƒè·³æ—¶é—´æ¥ç¡®å®šï¼‰ã€‚
*/
```

## 1.3 å¯åŠ¨ Worker

```scala
org.apache.spark.deploy.worker.Worke
// The life-cycle of an endpoint is:
// constructor -> onStart -> receive* -> onStop
// constructor -----------------------------------------------------------------
// å‰é¢æ­¥éª¤ä¸ Master åŸºæœ¬ä¸€è‡´ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯å¯åŠ¨å‚æ•°ä¸­åŒ…å«æ‰€æœ‰ Master URLï¼ˆé«˜å¯ç”¨ï¼‰ï¼Œä¼šå°†å…¶è§£æä¸ºä¸€ä¸ªå­—ç¬¦ä¸²æ•°ç»„ã€‚
// ä¼´ç”Ÿå¯¹è±¡ä¸­çš„ main æ ¹æ®é€šä¿¡å‚æ•°åˆ›å»º Worker ç«¯çš„ RpcEnv ç¯å¢ƒ, å¹¶å¯åŠ¨ RpcEnv

// onStart ----------------------------------------------------------------------
// å‘ master æ³¨å†Œ worker
registerWithMaster()
    // å°è¯•å»å‘ master æ³¨å†Œ, ä½†æ˜¯æœ‰å¯èƒ½æ³¨å†Œå¤±è´¥
    // æ¯”å¦‚ master è¿˜æ²¡æœ‰å¯åŠ¨æˆåŠŸ, æˆ–è€…ç½‘ç»œæœ‰é—®é¢˜
	// é¦–å…ˆå®šä¹‰æ³¨å†Œç­–ç•¥ï¼šå‘æ‰€æœ‰çš„ master æ³¨å†Œ
    registerMasterFutures = tryRegisterAllMasters()
// ------------tryRegisterAllMasters() ------------------------------------------
        masterRpcAddresses.map { masterAddress =>
            registerMasterThreadPool.submit(new Runnable {
                override def run(): Unit = {
                    try {
                        logInfo("Connecting to master " + masterAddress + "...")
    // æ ¹æ®æ„é€ å™¨ä¸­ Master çš„åœ°å€å¾—åˆ°ä¸€ä¸ª Master çš„ RpcEndpointRef, ç„¶åå°±å¯ä»¥å’Œ Master è¿›è¡Œé€šè®¯äº†.
                        val masterEndpoint: RpcEndpointRef = rpcEnv.setupEndpointRef(masterAddress, Master.ENDPOINT_NAME)
    // -------------------------- å‘ Master æ³¨å†Œ -------------------------------
             registerWithMaster(masterEndpoint)
            // -------------- registerWithMaster(masterEndpoint) --------------
                  // ç»™masterå‘é€ä¿¡æ¯:RegisterWorkerï¼ˆåŒ…å« work çš„ä¿¡æ¯ï¼‰---------
             masterEndpoint.ask[RegisterWorkerResponse](RegisterWorker(
             workerId, host, port, self, cores, memory, workerWebUiUrl))
                    } catch {
                        case ie: InterruptedException => // Cancelled
                        case NonFatal(e) => logWarning(s"Failed to connect to master $masterAddress", e)
                    }
                }
            })
    				// Master ç«¯æ”¶åˆ°æ³¨å†Œä¿¡æ¯åä¼šå¯¹å…¶è¿›è¡Œå¤„ç†-------------------
         case RegisterWorker(
        id, workerHost, workerPort, workerRef, cores, memory, workerWebUiUrl) =>
            logInfo("Registering worker %s:%d with %d cores, %s RAM".format(
                workerHost, workerPort, cores, Utils.megabytesToString(memory)))
            if (state == RecoveryState.STANDBY) { // state ä¸º Master å½“å‰çš„çŠ¶æ€
                // ç»™å‘é€è€…å›åº”æ¶ˆæ¯.  å¯¹æ–¹çš„ receive æ–¹æ³•ä¼šæ”¶åˆ°è¿™ä¸ªä¿¡æ¯
                context.reply(MasterInStandby)
            } else if (idToWorker.contains(id)) { // å¦‚æœè¦æ³¨å†Œçš„ Worker å·²ç»å­˜åœ¨
                context.reply(RegisterWorkerFailed("Duplicate worker ID"))
            } else {
                // æ ¹æ®ä¼ æ¥çš„ä¿¡æ¯å°è£… WorkerInfo
                val worker = new WorkerInfo(id, workerHost, workerPort, cores, memory,
                    workerRef, workerWebUiUrl)
                // è¿›è¡Œæ³¨å†Œï¼šregisterWorker(worker) æœ‰ 3 æ­¥ï¼šæ¸…ç† deadWorkerï¼›å°†æ–°æ³¨å†Œçš„ worker å­˜å…¥åˆ° workersï¼›æŠŠ id å’Œ worker çš„æ˜ å°„å­˜å…¥åˆ° HashMap ä¸­ã€‚
                if (registerWorker(worker)) { // æ³¨å†ŒæˆåŠŸ
                    persistenceEngine.addWorker(worker)
                    // å‘é€å“åº”ä¿¡æ¯ç»™ Workerï¼ŒåŒ…å«å½“å‰ master çš„ä¿¡æ¯
                    context.reply(RegisteredWorker(self, masterWebUiUrl))
                    schedule()
                } else {...}
            }
    // --------------Worker ç«¯æ”¶åˆ°æ³¨å†ŒæˆåŠŸä¿¡æ¯åä¼šå¯¹å…¶è¿›è¡Œå¤„ç†-------------------
   	 msg match {
            case RegisteredWorker(masterRef, masterWebUiUrl) =>
                logInfo("Successfully registered with master " + masterRef.address.toSparkURL)
                // æ›´æ–°æ³¨å†ŒçŠ¶æ€
                registered = true
                // æ›´æ–° Master
                changeMaster(masterRef, masterWebUiUrl)
                // èµ·ä¸€ä¸ªå•ä¾‹çº¿ç¨‹æ± ï¼Œç»™ Master å‘é€å¿ƒè·³ä¿¡æ¯ï¼Œé»˜è®¤ 1 åˆ†é’Ÿ 4 æ¬¡
                forwordMessageScheduler.scheduleAtFixedRate(new Runnable {
                    override def run(): Unit = Utils.tryLogNonFatalError {
                        // å…ˆæ˜¯è‡ªå·±ç»™è‡ªå·±å‘ï¼Œæ”¶åˆ°ä¿¡æ¯åå†ç»™ Master å‘é€å¿ƒè·³ä¿¡æ¯
                        self.send(SendHeartbeat)
                    }
                }, 0, HEARTBEAT_MILLIS, TimeUnit.MILLISECONDS)
         // ------------Master ç«¯æ”¶åˆ°æ³¨å†Œä¿¡æ¯åæ›´æ–°å¿ƒè·³æ—¶é—´-------------------
         case Heartbeat(workerId, worker) =>
            idToWorker.get(workerId) match {
                case Some(workerInfo) =>
                    // è®°å½•è¯¥ Worker çš„æœ€æ–°å¿ƒè·³   æ¯ 15ç§’æ›´æ–°ä¸€æ¬¡
                    workerInfo.lastHeartbeat = System.currentTimeMillis()
                
    // ä»¥ä¸Šåªæ˜¯ç­–ç•¥ï¼Œè¿˜æ²¡æœ‰å®é™…æ‰§è¡Œï¼ï¼ï¼
	// ä»çº¿ç¨‹æ± ä¸­å¯åŠ¨å•ä¾‹çº¿ç¨‹æ¥æ‰§è¡Œ Worker å‘ Master çš„æ³¨å†Œï¼Œè‹¥æ³¨å†Œå¤±è´¥åˆ™é‡‡ç”¨éšæœºçš„ç­–ç•¥å°è¯•æ³¨å†Œã€‚
            registrationRetryTimer = Some(forwordMessageScheduler.scheduleAtFixedRate(
                    new Runnable {
                        override def run(): Unit = Utils.tryLogNonFatalError {
                            Option(self).foreach(_.send(ReregisterWithMaster))
                        }
                    },
                    // [0.5, 1.5) * 10
                    INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS,
                    INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS,
                    TimeUnit.SECONDS))
        }
```

## 1.4 æ€»ç»“

![image-20200520205405410](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200520205405410.png)

```scala
1.	start-all.shè„šæœ¬ï¼Œå®é™…æ˜¯æ‰§è¡Œjava -cp Masterå’Œjava -cp Workerï¼›
2.	Masterå¯åŠ¨æ—¶é¦–å…ˆåˆ›å»ºä¸€ä¸ªRpcEnvå¯¹è±¡ï¼Œè´Ÿè´£ç®¡ç†æ‰€æœ‰é€šä¿¡é€»è¾‘ï¼›
3.	Masteré€šè¿‡RpcEnvå¯¹è±¡åˆ›å»ºä¸€ä¸ªEndpointï¼ŒMasterå°±æ˜¯ä¸€ä¸ªEndpointï¼ŒWorkerå¯ä»¥ä¸å…¶è¿›è¡Œé€šä¿¡ï¼›
4.	Workerå¯åŠ¨æ—¶ä¹Ÿæ˜¯åˆ›å»ºä¸€ä¸ªRpcEnvå¯¹è±¡ï¼›
5.	Workeré€šè¿‡RpcEnvå¯¹è±¡åˆ›å»ºä¸€ä¸ªEndpointï¼›
6.	Workeré€šè¿‡RpcEnvå¯¹è±¡å»ºç«‹åˆ°Masterçš„è¿æ¥ï¼Œè·å–åˆ°ä¸€ä¸ªRpcEndpointRefå¯¹è±¡ï¼Œé€šè¿‡è¯¥å¯¹è±¡å¯ä»¥ä¸Masteré€šä¿¡ï¼›
7.	Workerå‘Masteræ³¨å†Œï¼Œæ³¨å†Œå†…å®¹åŒ…æ‹¬ä¸»æœºåã€ç«¯å£ã€CPU Coreæ•°é‡ã€å†…å­˜æ•°é‡ï¼›
8.	Masteræ¥æ”¶åˆ°Workerçš„æ³¨å†Œï¼Œå°†æ³¨å†Œä¿¡æ¯ç»´æŠ¤åœ¨å†…å­˜ä¸­çš„Tableä¸­ï¼Œå…¶ä¸­è¿˜åŒ…å«äº†ä¸€ä¸ªåˆ°Workerçš„RpcEndpointRefå¯¹è±¡å¼•ç”¨ï¼›
9.	Masterå›å¤Workerå·²ç»æ¥æ”¶åˆ°æ³¨å†Œï¼Œå‘ŠçŸ¥Workerå·²ç»æ³¨å†ŒæˆåŠŸï¼›
10.	Workerç«¯æ”¶åˆ°æˆåŠŸæ³¨å†Œå“åº”åï¼Œå¼€å§‹å‘¨æœŸæ€§å‘Masterå‘é€å¿ƒè·³ã€‚
```

# 2. YARN æ¨¡å¼å¯åŠ¨æµç¨‹åˆ†æ

## 2.1  YARN Cluster æ¨¡å¼

YARN Cluster æ¨¡å¼ä¸‹æ€»å…±æœ‰ 3 ç§è¿›ç¨‹ï¼š

```
SparkSubmit
ApplicationMaster
CoarseGrainedExecutorBackend
```

### 2.1.1 spark-submit.sh

```bash
# åŒæ ·æ˜¯å…ˆé…ç½® ${SPARK_HOME} ç›¸å…³å˜é‡
# ä¹‹åè¿è¡Œ spark-class è„šæœ¬ï¼Œæ‰§è¡Œ org.apache.spark.deploy.SparkSubmit ç±»
exec "${SPARK_HOME}"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"
# spark-class -----------------------------------------------------------------
# æœ€ç»ˆæ‰§è¡Œçš„æ˜¯ä¸‹é¢è¿™è¡Œå‘½ä»¤ï¼š
/opt/module/jdk1.8.0_144/bin/java 
-cp /opt/module/spark-yarn/conf/:/opt/module/spark-yarn/jars/*:/opt/module/hadoop-2.7.2/etc/hadoop/ 
org.apache.spark.deploy.SparkSubmit 
--master yarn 
--deploy-mode cluster 
--class org.apache.spark.examples.SparkPi 
./examples/jars/spark-examples_2.11-2.1.1.jar 1000
```

### 2.1.2 SparkSubmit ç±»/ è¿›ç¨‹

```scala
// SparkSubmit Object çš„ main ä¸­å…ˆå®Œæˆå‚æ•°çš„è§£æï¼Œä¹‹åè¿åŒè§£æåçš„å‚æ•°æäº¤
appArgs.action match {
            // å¦‚æœæ²¡æœ‰æŒ‡å®š action, åˆ™ action çš„é»˜è®¤å€¼æ˜¯: Submit
            case SparkSubmitAction.SUBMIT => submit(appArgs)
            case SparkSubmitAction.KILL => kill(appArgs)
            case SparkSubmitAction.REQUEST_STATUS => requestStatus(appArgs)
        }
// submit()----------------------------------------------------------------------
// å‡†å¤‡æäº¤ç¯å¢ƒ
// å½“æ˜¯ yarn-cluster æ¨¡å¼: childMainClass = org.apache.spark.deploy.yarn.Client
// å½“æ˜¯ yarn-client æ¨¡å¼:  childMainClass = args.mainClass
	val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)
	// prepareSubmitEnvironment()-----------------------------------------------
	// é¦–å…ˆä¼šæ ¹æ®ä¼ å‚è¿›è¡Œå„ç§æ¨¡å¼åˆ¤æ–­ï¼šStandlone/ Yarn/ Mesosã€cluster/ clientã€‚æœŸé—´ä¼šå¯¹ç›¸å…³çš„æˆå‘˜å˜é‡è¿›è¡Œèµ‹å€¼ï¼Œå¹¶æ ¡éªŒå‡ºä¸åˆç†çš„ç»„åˆã€‚
	// ä¹‹åå¯¹å„ä¸ªè¿”å›å€¼è¿›è¡Œèµ‹å€¼ï¼Œé‡ç‚¹å…³æ³¨ childMainClass
	// childMainClass æœ€ç»ˆå€¼: org.apache.spark.deploy.yarn.Client
        childMainClass = args.mainClass   // ç¬¬ä¸€æ¬¡ï¼šç”¨æˆ·ä¸»ç±»
        // åœ¨ yarn é›†ç¾¤æ¨¡å¼ä¸‹, ä¼šè¿›è¡Œç¬¬äºŒæ¬¡èµ‹å€¼ï¼šyarn.Client
        childMainClass = "org.apache.spark.deploy.yarn.Client"
	// è¿”å›åæ‰§è¡Œ runMain å‡½æ•°
	runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)
	// runMain()----------------------------------------------------------------
	// ä½¿ç”¨åå°„çš„æ–¹å¼åŠ è½½ childMainClass = "org.apache.spark.deploy.yarn.Client"
	// ï¼ˆåœ¨ clientæ¨¡å¼ä¸‹, childMainClass ç›´æ¥æŒ‡å‘ç”¨æˆ·çš„ç±»ï¼‰
	mainClass = Utils.classForName(childMainClass)  
	// åå°„è·å– mainClass çš„ main æ–¹æ³•
    val mainMethod: Method = mainClass.getMethod("main", new Array[String](0).getClass)
	// æ‰§è¡Œ main æ–¹æ³•
	mainMethod.invoke(null, childArgs.toArray)
    // --------------------æ‰§è¡Œ Client ç±»çš„ main æ–¹æ³•----------------------------
		// Client ç±» main æ–¹æ³•ä¸­çš„æ ¸å¿ƒæ–¹æ³•ï¼šsubmitApplication()
		launcherBackend.connect()	// è¿æ¥åˆ° yarn.
		// åˆå§‹åŒ– yarn å®¢æˆ·ç«¯
		yarnClient.init(yarnConf)
         // å¯åŠ¨ yarn å®¢æˆ·ç«¯
         yarnClient.start()
		// ä» RM åˆ›å»ºä¸€ä¸ªåº”ç”¨ç¨‹åº
         val newApp = yarnClient.createApplication()
         val newAppResponse = newApp.getNewApplicationResponse()
		// è®¾ç½®æ­£ç¡®çš„ä¸Šä¸‹æ–‡å¯¹è±¡æ¥å¯åŠ¨ ApplicationMaster 
         val containerContext = createContainerLaunchContext(newAppResponse)
         // åˆ›å»ºåº”ç”¨ç¨‹åºæäº¤ä»»åŠ¡ä¸Šä¸‹æ–‡
         val appContext = createApplicationSubmissionContext(newApp, containerContext)
		// createApplicationSubmissionContextæ ¸å¿ƒä»£ç : ç¡®å®š ApplicationMaster ç±»
            val amClass =
            if (isClusterMode) { // å¦‚æœæ˜¯ cluster æ¨¡å¼
     Utils.classForName("org.apache.spark.deploy.yarn.ApplicationMaster").getName
            } else { // å¦‚æœæ˜¯ client æ¨¡å¼
     Utils.classForName("org.apache.spark.deploy.yarn.ExecutorLauncher").getName
            }
            // ç”Ÿæˆ ApplicationMaster/ ExecutorLauncher å¯åŠ¨å‘½ä»¤
            val commands = prefixEnv ++ Seq(
                YarnSparkHadoopUtil.expandEnvironment(Environment.JAVA_HOME) + "/bin/java", "-server"
            ) ++ javaOpts ++ amArgs ++
                Seq(
                    "1>", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stdout",
                    "2>", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stderr")
	// åˆ›å»ºåº”ç”¨ç¨‹åºæäº¤ä»»åŠ¡ä¸Šä¸‹æ–‡
	val appContext = createApplicationSubmissionContext(newApp, containerContext)
    // æäº¤åº”ç”¨ç»™ ResourceManager å¯åŠ¨ ApplicationMaster
    // org.apache.spark.deploy.yarn.ApplicationMaster
    yarnClient.submitApplication(appContext)
	// æœ€åè¿”å›ä» RM è·å–çš„ appId
```

### 2.1.3 ApplicationMaster  ç±»/ è¿›ç¨‹

```scala
// è¿›å…¥ ApplicationMaster çš„ main æ–¹æ³•ï¼Œå¯¹ä¼ è¿›å»çš„å‚æ•°è¿›è¡Œå°è£…åæ„å»º ApplicationMaster å®ä¾‹
SparkHadoopUtil.get.runAsSparkUser { () =>
    // æ„å»º ApplicationMaster å®ä¾‹,   ApplicationMaster éœ€è¦ä¸ RM é€šè®¯
    master = new ApplicationMaster(amArgs, new YarnRMClient)
    // è¿è¡Œ ApplicationMaster çš„ run æ–¹æ³•
    // run æ–¹æ³•ç»“æŸä¹‹å, ç»“æŸ ApplicationMaster è¿›ç¨‹
    System.exit(master.run())
    	// master.run() æ–¹æ³• --------------------------------------------------
        // å…³é”®æ ¸å¿ƒä»£ç 
        if (isClusterMode) {  // cluster æ¨¡å¼
            runDriver(securityMgr)
        } else { // client æ¨¡å¼
            runExecutorLauncher(securityMgr)
        }
    		// runDriver() æ–¹æ³•
    		// å¼€å§‹æ‰§è¡Œç”¨æˆ·ç±». å¯åŠ¨ä¸€ä¸ªå­çº¿ç¨‹æ¥æ‰§è¡Œç”¨æˆ·ç±»çš„ main æ–¹æ³•.  è¿”å›å€¼å°±æ˜¯è¿è¡Œç”¨æˆ·ç±»çš„å­çº¿ç¨‹.
			// çº¿ç¨‹åå°±å«"Driver"ï¼Œè¯´driveræ˜¯ä¸€ä¸ªè¿›ç¨‹(process)ä¹Ÿå¯¹, æŒ‡çš„æ˜¯ApplicationMasterè¿™ä¸ªè¿›ç¨‹
        	userClassThread = startUserApplication()
    		// startUserApplication() ä¸­å¾—åˆ°ç”¨æˆ·ç±»çš„ main æ–¹æ³• -----------------
        	val mainMethod: Method = userClassLoader.loadClass(args.userClass)
            .getMethod("main", classOf[Array[String]])
            // åœ¨å­çº¿ç¨‹å†…æ‰§è¡Œç”¨æˆ·ç±»çš„ main æ–¹æ³•
            val userThread: Thread = new Thread {
                override def run() {
                    try {
                        // è°ƒç”¨ç”¨æˆ·ç±»çš„ main æ–¹æ³•
                        mainMethod.invoke(null, userArgs.toArray)
                        finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS)
                        logDebug("Done running users class")
    
    	// åœ¨ main çº¿ç¨‹ä¸­æ³¨å†Œ ApplicationMaster , å…¶å®å°±æ˜¯è¯·æ±‚èµ„æº -------
        registerAM(sc.getConf, rpcEnv, driverRef, sc.ui.map(_.appUIAddress).getOrElse(""), securityMgr)
			// registerAM() ----------------------------------------------
             // å‘ RM æ³¨å†Œ, å¾—åˆ° YarnAllocator
                allocator = client.register(driverUrl,
                    driverRef,
                    yarnConf,
                    _sparkConf,
                    uiAddress,
                    historyAddress,
                    securityMgr,
                    localResources)
                // è¯·æ±‚åˆ†é…èµ„æºï¼ŒæœŸé—´æ¶‰åŠå‘ RM æå‡ºåˆ†é…è¯·æ±‚ã€ä» RM è·å¾—å®¹å™¨ã€å¤„ç†è·å¾—çš„å®¹å™¨ï¼ˆæ ¸å¿ƒä»£ç : è¿è¡Œåˆ†é…çš„å®¹å™¨ runAllocatedContainers(containersToUse)ï¼‰
                allocator.allocateResources()
                        // å…ˆå¯¹æ¯ä¸ªå®¹å™¨å¼€ä¸€ä¸ªçº¿ç¨‹ï¼Œåˆ›å»º nmClient é€šè®¯ï¼Œåœ¨é€šè®¯ä¸­ç”Ÿæˆ excutor å¯åŠ¨å‘½ä»¤ï¼Œå»å¯åŠ¨å®¹å™¨ä¸­çš„ excutor è¿›ç¨‹
                        handleAllocatedContainers(allocatedContainers.asScala)
                        runAllocatedContainers(containersToUse)
                        new ExecutorRunnable(
                                    Some(container),
                                    conf,
                                    sparkConf,
                                    driverUrl,
                                    executorId,
                                    executorHostname,
                                    executorMemory,
                                    executorCores,
                                    appAttemptId.getApplicationId.toString,
                                    securityMgr,
                                    localResources
                                ).run()
                             startContainer()
                             val commands = prepareCommand()
// excutor ç±»å org.apache.spark.executor.CoarseGrainedExecutorBackend

                        // nmå¯åŠ¨å®¹å™¨: å…¶å®å°±æ˜¯å¯åŠ¨ä¸€ä¸ªè¿›ç¨‹: CoarseGrainedExecutorBackend
                        nmClient.startContainer(container.get, ctx)
                // ----------------------------------------------------------
    	// çº¿ç¨‹ join: æŠŠ userClassThread çº¿ç¨‹æ‰§è¡Œå®Œæ¯•ä¹‹åå†ç»§ç»­æ‰§è¡Œå½“å‰çº¿ç¨‹.
            userClassThread.join()  // yield ç¤¼è®©ç»™ driver
}
```

### 2.1.4 CoarseGrainedExecutorBackend ç±»/ è¿›ç¨‹

```scala
// åœ¨ main æ–¹æ³•ä¸­å…ˆè¿è¡Œçš„æ˜¯ run æ–¹æ³•
run(driverUrl, executorId, hostname, cores, appId, workerUrl, userClassPath)
	// åˆ›å»º SparkEnv å¯¹è±¡ï¼Œå†…éƒ¨å°è£…äº†ä¸€ä¸ª EndPointEnv å¯¹è±¡ã€‚ä½œä¸º Spark å†…éƒ¨çš„ä¸€ä¸ªé€šè®¯ç¯å¢ƒï¼ŒDriver åœ¨ç”¨ï¼ŒExcutor ä¹Ÿç”¨ã€‚
    val env: SparkEnv = SparkEnv.createExecutorEnv(
        driverConf, executorId, hostname, port, cores, cfg.ioEncryptionKey, isLocal = false)
	// åœ¨ä¸Šè¿°ç¯å¢ƒä¸­è°ƒç”¨ CoarseGrainedExecutorBackend çš„æ„é€ å™¨åˆ›å»ºä¹‹
    new CoarseGrainedExecutorBackend(env.rpcEnv, driverUrl, executorId, hostname, cores, userClassPath, env
        // åœ¨ CoarseGrainedExecutorBackend.onStart() æ–¹æ³•ä¸­è·å¾— driver çš„ RPCrefï¼Œå¹¶è¿›è¡Œåå‘æ³¨å†Œã€‚
        ref.ask[Boolean](RegisterExecutor(executorId, self, hostname, cores, extractLogUrls))

    // æˆåŠŸä¹‹åï¼ŒDriver ç«¯çš„ CoarseGrainedSchedulerBackendï¼ˆä¼´éšç€ context å¯åŠ¨ï¼‰ å°† excutorData æ·»åŠ åˆ° hashMap ä¸­ï¼Œå’Œ standalone æ¨¡å¼ä¸‹çš„ workInfo é›†åˆç±»ä¼¼ã€‚
       val data = new ExecutorData(executorRef, executorRef.address, hostname,
                        cores, cores, logUrls)        
       // ç»™ Executor å‘é€æ³¨å†ŒæˆåŠŸçš„æ¶ˆæ¯
       executorRef.send(RegisteredExecutor)
    // Executor æ”¶åˆ°æ³¨å†ŒæˆåŠŸçš„æ¶ˆæ¯å new ä¸€ä¸ª Executor
    executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false)
```

### 2.1.5 æ€»ç»“

![image-20200521173128209](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200521173128209.png)

```scala
1.	æ‰§è¡Œè„šæœ¬æäº¤ä»»åŠ¡ï¼Œå®é™…æ˜¯å¯åŠ¨ä¸€ä¸ª SparkSubmit çš„ JVM è¿›ç¨‹ï¼›
2.	SparkSubmit ç±»ä¸­çš„ mainæ–¹æ³•åå°„è°ƒç”¨Clientçš„mainæ–¹æ³•ï¼›
3.	Clientåˆ›å»ºYarnå®¢æˆ·ç«¯ï¼Œç„¶åå‘Yarnå‘é€æ‰§è¡ŒæŒ‡ä»¤ï¼šbin/java ApplicationMasterï¼›
4.	Yarnæ¡†æ¶æ”¶åˆ°æŒ‡ä»¤åä¼šåœ¨æŒ‡å®šçš„NMä¸­å¯åŠ¨ApplicationMasterï¼›
5.	ApplicationMasterå¯åŠ¨Driverçº¿ç¨‹ï¼Œæ‰§è¡Œç”¨æˆ·çš„ä½œä¸šï¼›
6.	AMå‘RMæ³¨å†Œï¼Œç”³è¯·èµ„æºï¼›
7.	è·å–èµ„æºåAMå‘NMå‘é€æŒ‡ä»¤ï¼šbin/java CoarseGrainedExecutorBackenï¼›
8.	å¯åŠ¨ExecutorBackend, å¹¶å‘driveræ³¨å†Œ.
9.	æ³¨å†ŒæˆåŠŸå, ExecutorBackendä¼šåˆ›å»ºä¸€ä¸ªExecutorå¯¹è±¡.
10.	Driverä¼šç»™ExecutorBackendåˆ†é…ä»»åŠ¡, å¹¶ç›‘æ§ä»»åŠ¡çš„æ‰§è¡Œ.
æ³¨æ„:
â€¢	SparkSubmitã€ApplicationMasterå’ŒCoarseGrainedExecutorBackenæ˜¯ç‹¬ç«‹çš„è¿›ç¨‹ï¼›
â€¢	Clientå’ŒDriveræ˜¯ç‹¬ç«‹çš„çº¿ç¨‹ï¼›
â€¢	Executoræ˜¯ä¸€ä¸ªå¯¹è±¡ã€‚

```

## 2.2 Yarn Client æ¨¡å¼

YARN Client æ¨¡å¼ä¸‹æ€»å…±æœ‰ 3 ç§è¿›ç¨‹ï¼š

```
SparkSubmit    
ExecutorLauncher   (ApplicationMaster)
CoarseGrainedExecutorBackend  
```

### 2.2.1 SparkSubmit ç±»/è¿›ç¨‹

```scala
// å‰é¢çš„è¿‡ç¨‹ Client æ¨¡å¼ä¸ Cluster æ¨¡å¼å¹¶æ— äºŒè‡´ã€‚
// åˆ° runMain å‡½æ•°æ—¶ï¼Œåå°„å¹¶è¿è¡Œçš„å®é™…ä¸Šæ˜¯ç”¨æˆ·ä¸»ç±»çš„ main å‡½æ•°
    childMainClass = args.mainClass  // ç”¨æˆ·ä¸»ç±»

    mainMethod.invoke(null, childArgs.toArray)

// é‡ç‚¹ï¼šåˆ›å»º SparkContext ç±» -------------------------------------------------
SparkContext
	// é‡è¦å±æ€§
	// ç”¨æ¥ä¸å…¶ä»–ç»„ä»¶é€šè®¯ç”¨  *******
    private var _schedulerBackend: SchedulerBackend = _
    // ä»»åŠ¡è°ƒåº¦å™¨, æ˜¯è°ƒåº¦ç³»ç»Ÿä¸­çš„é‡è¦ç»„ä»¶ä¹‹ä¸€.
    // TaskScheduler æŒ‰ç…§è°ƒåº¦ç®—æ³•å¯¹é›†ç¾¤ç®¡ç†å™¨å·²ç»åˆ†é…ç»™åº”ç”¨ç¨‹åºçš„èµ„æºè¿›è¡ŒäºŒæ¬¡è°ƒåº¦ååˆ†é…ç»™ä»»åŠ¡
    // TaskScheduler è°ƒåº¦çš„ Task æ˜¯ç”± DAGScheduler åˆ›å»ºçš„, æ‰€ä»¥ DAGScheduler æ˜¯ TaskSchedulerçš„å‰ç½®è°ƒåº¦å™¨
    private var _taskScheduler: TaskScheduler = _
    // ç”¨æ¥æ¥æ”¶ Executor çš„ğŸ’›è·³ä¿¡æ¯
    private var _heartbeatReceiver: RpcEndpointRef = _
    // DAG è°ƒåº¦å™¨, æ˜¯è°ƒåº¦ç³»ç»Ÿçš„ä¸­çš„ä¸­çš„é‡è¦ç»„ä»¶ä¹‹ä¸€, è´Ÿè´£åˆ›å»º Job, å°† DAG ä¸­çš„ RDD åˆ’åˆ†åˆ°ä¸åŒçš„ Stage, æäº¤ Stage ç­‰.
    // SparkUI ä¸­æœ‰å…³ Job å’Œ Stage çš„ç›‘æ§æ•°æ®éƒ½æ¥è‡ª DAGScheduler
    @volatile private var _dagScheduler: DAGScheduler = _


	// æ„é€ å™¨ï¼Œé¦–å…ˆå¯¹è¯»å– conf ä¸­çš„å„é¡¹é…ç½®å¹¶æ ¡éªŒï¼Œä¾‹å¦‚åˆ¤æ–­ appNameã€master æ˜¯å¦è®¾ç½®
	// Executorå†…å­˜, é»˜è®¤ 1G
	// æ¥ä¸‹æ¥çš„é‡ç‚¹æ˜¯åˆ›å»º TaskScheduler
	val (sched, ts): (SchedulerBackend, TaskScheduler) = SparkContext.createTaskScheduler(this, master, deployMode)
	// å†…éƒ¨æ‰§è¡Œå¦‚ä¸‹ä»£ç --------------------------------------------
		// è·å–å¤–éƒ¨é›†ç¾¤ç®¡ç†å™¨
		val cm: ExternalClusterManager = getClusterManager(masterUrl) 
        try {
            // åˆ›å»º TaskScheduler. å®é™…ç±»å‹æ˜¯: YarnScheduler
            val scheduler: TaskScheduler = cm.createTaskScheduler(sc, masterUrl)
            // åˆ›å»º SchedulerBackend. å®é™…ç±»å‹æ˜¯: YarnClientSchedulerBackend
            val backend: SchedulerBackend = cm.createSchedulerBackend(sc, masterUrl, scheduler)
            // åˆå§‹åŒ– TaskScheduler å’Œ SchedulerBackend
            cm.initialize(scheduler, backend)
            // è¿”å› SchedulerBackend å’Œ TaskScheduler
            (backend, scheduler)

	_schedulerBackend = sched
    _taskScheduler = ts
    // åˆ›å»º DAGScheduler  stageçš„åˆ’åˆ†, ä»»åŠ¡çš„åˆ’åˆ†, æŠŠä»»åŠ¡å°è£…æˆ TaskSet äº¤ç»™TaskSchedulerè°ƒåº¦
    _dagScheduler = new DAGScheduler(this)
    _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)
    // å¯åŠ¨ TaskSheduler
    _taskScheduler.start()
    // å†…éƒ¨æ‰§è¡Œå¦‚ä¸‹ä»£ç --------------------------------------------
    	backend.start() // å®é™…è°ƒç”¨çš„æ˜¯ YarnClientSchedulerBackend.start()
            // æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤
            // å‘½ä»¤ä¸­çš„ client ç±»è¿˜æ˜¯ cluster æ¨¡å¼ä¸‹çš„ client ç±»
            bindToYarn(client.submitApplication(), None)
            // submitApplication() è¿˜æ˜¯ cluster æ¨¡å¼ä¸‹çš„ client ç±»
            	createContainerLaunchContext(newAppResponse) // -> æ ¸å¿ƒä»£ç 
            	// æ ¸å¿ƒä»£ç : ç¡®å®š ApplicationMaster ç±»
        		val amClass =
            	if (isClusterMode) { 
                    // å¦‚æœæ˜¯ cluster æ¨¡å¼
Utils.classForName("org.apache.spark.deploy.yarn.ApplicationMaster").getName
            	} else { // å¦‚æœæ˜¯ client æ¨¡å¼ï¼Œæ‰§è¡Œè¯¥æ¡æŒ‡ä»¤
Utils.classForName("org.apache.spark.deploy.yarn.ExecutorLauncher").getName
            	}
// æœ€åç”Ÿæˆ command æäº¤ç»™ RM
```

### 2.2.2 ExecutorLauncher ç±»/ è¿›ç¨‹

```scala
object ExecutorLauncher {
    // å…¶å®å°±æ˜¯ä¸€ä¸ªæŠ«ç€ ExecutorLauncher å¤–çš®çš„ ApplicationMaster
    def main(args: Array[String]): Unit = {
        ApplicationMaster.main(args)
    }
}
// æ¥ä¸‹æ¥çš„æ­¥éª¤ä¸ cluster æ¨¡å¼ä¸€è‡´ï¼Œç›´åˆ° run() æ–¹æ³•çš„ä»¥ä¸‹ä»£ç 
        // å…³é”®æ ¸å¿ƒä»£ç 
            if (isClusterMode) {  // cluster æ¨¡å¼
                runDriver(securityMgr)
            } else { // client æ¨¡å¼
                runExecutorLauncher(securityMgr)
            }
		// runExecutorLauncher å†…éƒ¨æ‰§è¡Œ registerAMï¼Œæ¥ä¸‹æ¥ä¸ cluster æ¨¡å¼å®Œå…¨ä¸€è‡´
			registerAM(sparkConf, rpcEnv, driverRef, sparkConf.get("spark.driver.appUIAddress", ""),
            securityMgr)
```

### 2.2.3 æ€»ç»“

![image-20200521203337361](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200521203337361.png)

```scala
1.	æ‰§è¡Œè„šæœ¬æäº¤ä»»åŠ¡ï¼Œå®é™…æ˜¯å¯åŠ¨ä¸€ä¸ªSparkSubmitçš„ JVM è¿›ç¨‹ï¼›
2.	SparkSubmitä¼´ç”Ÿå¯¹è±¡ä¸­çš„mainæ–¹æ³•åå°„è°ƒç”¨ç”¨æˆ·ä»£ç çš„mainæ–¹æ³•ï¼›
3.	å¯åŠ¨Driverï¼Œæ‰§è¡Œç”¨æˆ·çš„ä½œä¸šï¼Œå¹¶åˆ›å»ºScheduleBackendï¼›
4.	YarnClientSchedulerBackendå‘RMå‘é€æŒ‡ä»¤ï¼šbin/java ExecutorLauncherï¼›
5.	Yarnæ¡†æ¶æ”¶åˆ°æŒ‡ä»¤åä¼šåœ¨æŒ‡å®šçš„NMä¸­å¯åŠ¨ExecutorLauncherï¼ˆå®é™…ä¸Šè¿˜æ˜¯è°ƒç”¨ApplicationMasterçš„mainæ–¹æ³•ï¼‰ï¼›
object ExecutorLauncher {

  def main(args: Array[String]): Unit = {
    ApplicationMaster.main(args)
  }

}
6.	AMå‘RMæ³¨å†Œï¼Œç”³è¯·èµ„æºï¼›
7.	è·å–èµ„æºåAMå‘NMå‘é€æŒ‡ä»¤ï¼šbin/java CoarseGrainedExecutorBackenï¼›
8.	åé¢å’Œclusteræ¨¡å¼ä¸€è‡´
æ³¨æ„ï¼š
- SparkSubmitã€ExecutorLauncherå’ŒCoarseGrainedExecutorBackenæ˜¯ç‹¬ç«‹çš„è¿›ç¨‹ï¼›
- driverä¸æ˜¯ä¸€ä¸ªå­çº¿ç¨‹,è€Œæ˜¯ç›´æ¥è¿è¡Œåœ¨SparkSubmitè¿›ç¨‹çš„mainçº¿ç¨‹ä¸­, æ‰€ä»¥sparkSubmitè¿›ç¨‹ä¸èƒ½é€€å‡º.

```

# 3. ä»»åŠ¡è°ƒåº¦

## 3.1 Stage å±‚çº§çš„è°ƒåº¦

```scala
// ä»¥ collect() ç®—å­ä¸ºä¾‹
// Stage ç±»æœ‰ä¸¤ä¸ªå­ç±» â€”â€”â€”â€” ResultStage + ShuffleMapStage
def collect(): Array[T] = withScope {
        val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
        Array.concat(results: _*)
    }
// æœ€ç»ˆä¼šè°ƒç”¨ dagScheduler.runJob()
dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
    // runJob() ä¸­è¿è¡Œ submitJob
    val waiter: JobWaiter[U] = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
		// å‘å†…éƒ¨äº‹ä»¶å¾ªç¯å™¨å‘é€æ¶ˆæ¯ JobSubmitted
// eventProcessLoop: DAGSchedulerEventProcessLoop = new DAGSchedulerEventProcessLoop(this)
		
        eventProcessLoop.post(JobSubmitted(
            jobId, rdd, func2, partitions.toArray, callSite, waiter,
            SerializationUtils.clone(properties)))
				// post æ–¹æ³•å°±æ˜¯å°†ä¸Šè¿° JobSubmitted å¯¹è±¡æ”¾å…¥é˜Ÿåˆ—
				def post(event: E): Unit = {
   	 				eventQueue.put(event)
  				}
		// çˆ¶ç±»ä¸­è¿˜æœ‰ä¸€ä¸ª eventQueue.take(event) æ–¹æ³•ã€‚å¯¹è±¡åŠ è½½åˆ°æœ€åä¼šå¯åŠ¨ä¸€ä¸ªçº¿ç¨‹æ­»å¾ªç¯æ‰§è¡Œ eventQueue.take(event) æ–¹æ³•ã€‚
		// æ‹¿åˆ°æäº¤çš„ job åä¼šå¯¹å…¶è¿›è¡Œå¤„ç†
		case JobSubmitted(...) =>
            dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
		// å…ˆåˆ’åˆ† finalStage: ResultStage å¹¶æäº¤
		// createResultStage è¿‡ç¨‹ä¸­ä¼šè·å– shuffleMapStage
		// è¿™ä¸ªè¿‡ç¨‹é‡‡ç”¨é€’å½’çš„æ–¹å¼è¿›è¡Œæ·±åº¦ä¼˜å…ˆéå†ï¼Œè·å– shuffleMapStageï¼Œç›´åˆ°è·å–åˆ°ç¬¬ä¸€ä¸ª shuffleMapStage ä¹‹åæ‰ä¼šè¿”å› finalStageã€‚
	finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
	submitStage(finalStage)

        // submitStage(finalStage)-------------------------------------
		// æäº¤æ—¶è¿˜æ˜¯ä¼šé‡‡ç”¨æ·±åº¦ä¼˜å…ˆéå†çš„æ€æƒ³ï¼Œå…ˆæäº¤çˆ¶ Stageï¼Œå†æäº¤å½“å‰ Stage
        // 1. æ‰¾åˆ°æ²¡æœ‰æäº¤çš„çˆ¶ Stage
            val missing: List[Stage] = getMissingParentStages(stage).sortBy(_.id)
		   if (missing.isEmpty) { // 2. å¦‚æœä¸ºç©º, åˆ™ä»å½“å‰ stage ç›´æ¥æäº¤
               submitMissingTasks(stage, jobId.get)
            } else { // 3. å¦‚æœä¸ä¸ºç©º, åˆ™é€’å½’çš„å‘ä¸ŠæŸ¥æ‰¾
               for (parent <- missing) {
                  submitStage(parent)
               }
             // 4. å½“å‰ stage åŠ å…¥åˆ°ç­‰å¾…æäº¤çš„ stage å¯¹åˆ—ä¸­
             waitingStages += stage
```

## 3.2 task å±‚çº§çš„è°ƒåº¦

```scala
// Task ç±»æœ‰ä¸¤ä¸ªå­ç±» â€”â€”â€”â€” ResultTask + ShuffleMapTask
def submitMissingTasks(stage: Stage, jobId: Int){
    ...
    stage match {
            case s: ShuffleMapStage =>
                outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1)
            case s: ResultStage =>
                outputCommitCoordinator.stageStart(
                    stage = s.id, maxPartitionId = s.rdd.partitions.length - 1)
        }
    // å…ˆæ ¹æ® stage åˆ’åˆ† task
    val tasks: Seq[Task[_]] = try {
            stage match {
                case stage: ShuffleMapStage =>
                    partitionsToCompute.map { id =>
                        val locs = taskIdToLocations(id)
                        val part = stage.rdd.partitions(id)
                        // ShuffleMapTask
                        new ShuffleMapTask(stage.id, stage.latestInfo.attemptId,
                            taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, Option(jobId),
                            Option(sc.applicationId), sc.applicationAttemptId)
                    }
                
                case stage: ResultStage =>
                    partitionsToCompute.map { id =>
                        val p: Int = stage.partitions(id)
                        val part = stage.rdd.partitions(p)
                        val locs = taskIdToLocations(id)
                        // ResultTask
                        new ResultTask(stage.id, stage.latestInfo.attemptId,
                            taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics,
                            Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)
                    }
            }
        
     // æäº¤ task
     // æŠŠ tasks å°è£…åˆ° TaskSet ä¸­, ç„¶åäº¤ç»™ taskScheduler æ¥æäº¤
        // æ¯ä¸ª TaskSet æœ€åäº¤ç»™ä¸€ä¸ª TaskSetManager è¿›è¡Œè°ƒåº¦
	taskScheduler.submitTasks(new TaskSet(
                tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))
        // submitTasks ä¸­æŠŠä»»åŠ¡äº¤ç»™ä»»åŠ¡è°ƒåº¦æ± æ¥è°ƒåº¦ ------------------------
        // SchedulableBuilder æœ‰ä¸¤ä¸ªå­ç±» â€”â€”â€”â€” FIFO + Fair
            schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)
        // é€šçŸ¥ SchedulerBackend ç»™è‡ªå·±(Driver)å‘é€ä¿¡æ¯:ReviveOffers
        backend.reviveOffers()
        // è‡ªå·±(Driver)æ”¶åˆ°åç›¸åº”---------------------------------------------
        case ReviveOffers => makeOffers()
        	// makeOffers() ----------------------------------------------
        	private def makeOffers() {
            // Filter out executors under killing
            // è¿‡æ»¤å‡º Active çš„ Executor
            val activeExecutors: collection.Map[String, ExecutorData] = executorDataMap.filterKeys(executorIsAlive)
            // å°è£…èµ„æº
            val workOffers: immutable.IndexedSeq[WorkerOffer] = activeExecutors.map { case (id, executorData) =>
                new WorkerOffer(id, executorData.executorHost, executorData.freeCores)
            }.toIndexedSeq
            // å¯åŠ¨ä»»åŠ¡
            launchTasks(scheduler.resourceOffers(workOffers))
                // launchTasks() -------------------------------------------
                private def launchTasks(tasks: Seq[Seq[TaskDescription]]) {
            		for (task <- tasks.flatten) {
                	// åºåˆ—åŒ–ä»»åŠ¡
                	val serializedTask: ByteBuffer = ser.serialize(task)
                    // å‘é€ä»»åŠ¡åˆ° Executor.CoarseGrainedExecutorBackend
                    executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))
                // Executor æ¥æ”¶åˆ°ä»»åŠ¡åæ‰§è¡Œ ---------------------------------
            case LaunchTask(data) =>
            if (executor == null) {
                exitExecutor(1, "Received LaunchTask command but executor was null")
            } else {
                // å°è£…ä»»åŠ¡
                val taskDesc: TaskDescription = ser.deserialize[TaskDescription](data.value)
                logInfo("Got assigned task " + taskDesc.taskId)
                // å¯åŠ¨ä»»åŠ¡
                executor.launchTask(this, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,taskDesc.name, taskDesc.serializedTask)
                // launchTask() ----------------------------------------------
                def launchTask(
                      context: ExecutorBackend,
                      taskId: Long,
                      attemptNumber: Int,
                      taskName: String,
                      serializedTask: ByteBuffer): Unit = {
        		// Runnable æ¥å£çš„å¯¹è±¡.
       			val tr = new TaskRunner(context, taskId = taskId, attemptNumber = attemptNumber, taskName,serializedTask)
                 // runningTasksï¼šæ­£åœ¨è¿åŠ¨çš„ task é›†åˆ
                 runningTasks.put(taskId, tr)
                 // åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ taskï¼Œæ‰€ä»¥ task çš„æ‰§è¡Œçº§åˆ«æ˜¯çº¿ç¨‹çº§
                 threadPool.execute(tr)
                    // tr: TaskRunner çš„ run() æ–¹æ³•ï¼šæ›´æ–° task çŠ¶æ€ã€ååºåˆ—åŒ–ã€æ‰§è¡Œ
                    // æ ¸å¿ƒä»£ç 
                            val value = try {
                            // å¼€å§‹è¿è¡Œ taskï¼Œåœ¨ task.run() ä¸­æ‰§è¡Œé‚£äº›åŒ¿åå‡½æ•°
                            val res = task.run(
                                taskAttemptId = taskId,
                                attemptNumber = attemptNumber,
                                metricsSystem = env.metricsSystem)
                            threwException = false
                            res
                }
  			}
         }
     }
}

```









