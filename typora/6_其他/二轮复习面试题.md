# 一	Hive

## 1.1 Hive调优


### 1.1 存储优化

#### 1）列式存储

ORC 的压缩性能优于 parquet，但 parquet 的兼容性优于 ORC。
从 Spark 2.3 开始，Spark 支持 ORC。

#### 2）开启压缩

设置map端输出、中间结果压缩。（不完全是解决数据倾斜的问题，但是减少了IO读写和网络传输，能提高很多效率）

```sql
set hive.exec.compress.intermediate=true --启用中间数据压缩
set mapreduce.map.output.compress=true --启用最终数据压缩
set mapreduce.map.outout.compress.codec=…; --设置压缩方式
```

### 1.2 执行优化

#### 1）MapJoin

如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。

#### 2）行列过滤

列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。

行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤。 

#### 3）合理设置Map数

　　a) 假设input目录下有1个文件a，大小为780M，那么hadoop会将该文件a分隔成7个块（6个128M的块和1个12M的块），从而产生7个map书；

　　b) 假设input目录下有3个文件a,b,c，大小分别为10M，20M，130M，那么hadoop会分隔成4个块（10M，20M，128M，2M），从而产生4个map数；

　　注意：如果文件大于块大小（128M），那么会拆分，如果小于块大小，则把该文件当成一个块。

一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。那么，是不是保证每个map处理接近128M的文件块，就高枕无忧了？答案也是不一定。比如有一个127M的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。 

```sql
mapred.min.split.size: 指的是数据的最小分割单元大小；min的默认值是1B
mapred.max.split.size: 指的是数据的最大分割单元大小；max的默认值是256MB
通过调整max可以起到调整map数的作用，减小max可以增加map数，增大max可以减少map数。
需要提醒的是，直接调整mapred.map.tasks这个参数是没有效果的。
```

#### 4）合理设置reduce数

Reduce个数并不是越多越好

（1）过多的启动和初始化Reduce也会消耗时间和资源；

（2）另外，有多少个Reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；

在设置Reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的Reduce数；使单个Reduce任务处理数据量大小要合适；

#### 5）采用分区技术

Partition就是分区，分区通过在创建表时启动partition by实现，用来partition的维度并不是实现数据的某一列，具体分区的标志是插入内容时给定的。当要查询某一分区的内容时，可以采用where语句，类似 `where dt = 2020-06-24`

#### 6）并行执行

##### 为什么开启并行执行？

同步执行hive的多个阶段，hive在执行过程，将一个查询转化成一个或者多个阶段。某个特定的job可能包含众多的阶段，而这些阶段可能并非完全相互依赖的，也就是说可以并行执行的，这样可能使得整个job的执行时间缩短。

##### 基本使用

 hive执行开启

```sql
set hive.exec.parallel=true
```

### 1.3 小文件优化

#### 小文件如何产生的？

小文件如何产生的？

（1）动态分区插入数据，产生大量的小文件，从而导致map数量剧增；

（2）reduce数量越多，小文件也越多（reduce的个数和输出文件是对应的）；

（3）数据源本身就包含大量的小文件。

#### 如何优化？

##### 1）在数据源提前聚合

（1）在Map执行前合并小文件，减少Map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。

##### 2）merge输出合并小文件

```sql
SET hive.merge.mapfiles = true; -- 默认true，在map-only任务结束时合并小文件
SET hive.merge.mapredfiles = true; -- 默认false，在map-reduce任务结束时合并小文件
SET hive.merge.size.per.task = 268435456; -- 默认256M
SET hive.merge.smallfiles.avgsize  = 16777216; -- 当输出文件的平均大小小于16m该值时，启动一个独立的map-reduce任务进行文件merge
```

#### 3）JVM重用

##### 为什么要开启JVM重用？

JVM重用是hadoop调优参数的内容，对hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短。hadoop默认配置是使用派生JVM来执行map和reduce任务的，这是jvm的启动过程可能会造成相当大的开销，尤其是执行的job包含有成千上万个task任务的情况。

##### JVM重用基本使用

JVM重用可以使得JVM实例在同一个JOB中重新使用N次

N的值可以在Hadoop的`mapre-site.xml`文件中进行设置

```sql
mapred.job.reuse.jvm.num.tasks
```

也可在hive中设置

```sql
set mapred.job.reuse.jvm.num.tasks=10;
```

##### 开启JVM的弊端？

开启JVM重用将会一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡“的job中有几个reduce task 执行的时间要比其他reduce task消耗的时间多得多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。

##### 4）开启map端Combiner（不影响最终业务逻辑）

```sql
set hive.map.aggr=true；
```

### 1.4 调优效果

执行前和执行后提升了40%的效果。

## 1.2 Hive数据倾斜

### 1.2.1 数据倾斜的本质

如何将map端输出的数据均匀的分配到各个reduce中，就是解决数据倾斜的根本所在。

### 1.2.2 定位经常出现数据倾斜的算子

![image-20200712165245465](D:\ProgramFiles\Typora\图片备份\image-20200712165245465.png)



### 1.2.3 常见的出现数据倾斜的原因

1)、key分布不均匀

2)、业务数据本身的特性

3)、建表时考虑不周

4)、某些SQL语句本身就有数据倾斜

### 1.2.4 数据倾斜的表现

任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。

单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。 最长时长远大于平均时长。

## 1.3 说说对Hive桶表的理解？

桶表是对数据进行哈希取值，然后放到不同文件中存储。

数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。物理上，每个桶就是表(或分区）目录里的一个文件，一个作业产生的桶(输出文件)和reduce任务个数相同。

桶表专门用于抽样查询，是很专业性的，不是日常用来存储数据的表，需要抽样查询时，才创建和使用桶表。

![image-20200712233055480](D:\ProgramFiles\Typora\图片备份\image-20200712233055480.png)

## 1.4 hive和mysql的使用场景？

### 1.4.1 查询语言

hive的查询语言是类似SQL的hql语言

MySQL是SQL查询语言

### 1.4.2 数据存储位置

hive存储在HDFS上，MySQL存储在本地

### 1.4.3 数据更新

hive不支持插入和更新操作，MySQL支持

### 1.4.4 索引

hive不存在索引，MySQL有索引

### 1.4.5 执行引擎

hive大多数查询通过借助其他计算引擎比如tez、spark、MR，MySQL使用自己的执行引擎

### 1.4.6 执行延迟

hive没有索引，延迟较高，MySQL有索引查询较快

### 1.4.7 可扩展性

hive基于HDFS可以进行横向扩展，MySQL没有扩展性

### 1.4.8 数据规模

hive适合用于处理大数据场景的计算，MySQL相对较小



## 1.5 Hive与关系型数据库的关系？

没有关系，Hive是数据仓库工具，不能和数据库一样进行实时的CURD操作。

## 1.6 Hive分区表的使用场景，有什么优势？

可按照业务维度进一步细分：减少数据冗余、提高查询效率，也可以说是隔离数据、优化查询



# 二 Hadoop

## 2.1 联邦机制

### 2.1.1 什么是联邦机制？

Federation是指HDFS集群可使用多个独立的NameSpace(NameNode节点管理)来满足HDFS命名空间的水平扩展
这些NameNode分别管理一部分数据，且共享所有DataNode的存储资源。

NameSpace之间在逻辑上是完全相互独立的(即任意两个NameSpace可以有完全相同的文件名)。在物理上可以完全独立(每个NameNode节点管理不同的DataNode)也可以有联系(共享存储节点DataNode)。一个NameNode节点只能管理一个Namespace

Federation机制解决单NameNode存在的以下几个问题
（1）HDFS集群扩展性。每个NameNode分管一部分namespace，相当于namenode是一个分布式的。
（2）性能更高效。多个NameNode同时对外提供服务，提供更高的读写吞吐率。
（3）良好的隔离性。用户可根据需要将不同业务数据交由不同NameNode管理，这样不同业务之间影响很小。
（4）Federation良好的向后兼容性，已有的单Namenode的部署配置不需要任何改变就可以继续工作。

### 2.1.2 Federation是简单鲁棒的设计

鲁棒性（健壮和强壮）：在输入错误、磁盘故障、网络过载或有意攻击情况下，能否不死机、不崩溃

由于联盟中各个Namenode之间是相互独立的：Federation整个核心设计大部分改变是在Datanode、Config和Tools，而Namenode本身的改动非常少，这样Namenode原先的鲁棒性不会受到影响。比分布式的Namenode简单，虽然扩展性比真正的分布式的Namenode要小些，但是可以迅速满足需求。

另外一个原因是Federation良好的向后兼容性，可以无缝的支持目前单Namenode架构中的配置。已有的单Namenode的部署配置不需要任何改变就可以继续工作。

### 2.1.3 Federation不足之处

HDFS Federation并没有完全解决单点故障问题。虽然namenode/namespace存在多个，但是从单个namenode/namespace看，仍然存在单点故障。因此 Federation中每个namenode配置成HA高可用集群，以便主namenode挂掉一下，用于快速恢复服务。

### 2.1.4 Federation架构

Federation方案的基本思路就是使用多个独立的NameSpace(NameNode节点管理)来满足HDFS命名空间的水平扩展,NameSpace之间在逻辑上是完全相互独立的(即任意两个NameSpace可以有完全相同的文件名),而在物理上可以完全独立(NameNode节点管理不同的DataNode)也可以有联系(共享存储节点DataNode)很显然,任何一个NameNode节点只能管理一个Namespace.这种在逻辑上无法统一命名空间的设计对于初学者来说,可能会常常踩到文件名冲突或文件不存在的陷阱中.很显然,任何一个NameNode节点只能管理一个Namespace。

![image-20200709205308200](D:\ProgramFiles\Typora\图片备份\image-20200709205308200.png)

## 2.2 详解Hadoop的WordCount

Map阶段：

1）从HDFS读取元数据文件，默认的InputFormat格式是TextInputFormat按128MB对文件进行切片，直到当前片不大于128MB的1.1倍，对每一个切片都会启动一个MapTask进行处理。默认读取方式RecordRead是按行读取。

2）将读取到的每一行数据切分成一个一个的单词

3）对每一个单词进行格式转换，变成kv形式

4）在进入环形缓冲区之间会进去分区器，调用getPartitioner方法对每一个元素打上分区标签

5）将转换后的键值对格式发送给reduce

shuffle阶段：

1）对map阶段出来的键值对进行排序溢写

2）溢写时一个分区对应一个文件，当文件数超过一定范围进行一次归并排序

3）不影响业务逻辑的情况下，可以开启combineByKey预聚合

4）等待reduce阶段拉取

Reduce阶段：

1）拉取map阶段输出的单词键值对

2）将相同单词的键值对聚合成一组

3）对每一组的进行遍历，累加求和，得到每一个单词的总和

4）当得到(k,v)输出

## 2.3 HDFS的一致性是指什么？有几种语义？

**对于一致性，可以分为从客户端和服务端两个不同的视角。**

　　从**客户端**来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。

　　从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。

对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是**强一致性**。

如果能容忍后续的部分或者全部访问不到，则是**弱一致性**。

如果经过一段时间后要求能访问到更新后的数据，则是**最终一致性**

文件系统的一致模型描述了对文件读写的数据可见性。HDFS为性能牺牲了一些POSIX请求，因此一些操作可能比想像的困难。

在创建一个文件之后，在文件系统的命名空间中是可见的，如下所示：

但是，写入文件的内容并不保证能被看见，即使数据流已经被刷新。所以文件长度显示为0：

一旦写入的数据超过一个块的数据，新的读取者就能看见第一个块。对于之后的块也是这样。总之，它始终是当前正在被写入的块，其他读取者是看不见它的。

在HDFS中写入关闭一个文件其实还执行了一个隐含的sync()：

如果不调用sync()，那么一旦客户端或系统发生故障，就可能失去一个块的数据。对很多应用来说，这是不可接受的，所以我们应该在适当的地方调用sync()，例如在写入一定的记录或字节之后。尽管sync()操作被设计为尽量减少HDFS负载，但它仍然有开销，所以在数据健壮性和吞吐量之间就会有所取舍。应用依赖就比较能接受，通过不同的sync()频率来衡量应用程序，最终找到一个合适的平衡。

**联系HDFS读写流程的ACK_QUEUE**

## 2.4 HDFS租约机制

### 2.4.1 什么是HDFS租约？

在HDFS中，当每次客户端用户往某个文件中写入数据的时候，为了保持数据的一致性，此时其它客户端程序是不允许向此文件同时写入数据的。那么HDFS是如何做到这一点的呢？答案是租约（Lease）。换句话说，租约是HDFS给予客户端的一个写文件操作的临时许可证，无此证件者将不被允许操作此文件。

```java
  class Lease {
    // 租约持有者
    private final String holder;
    // 最近更新时间
    private long lastUpdate;
    // 当前租约持有者打开的文件
    private final HashSet<Long> files = new HashSet<>();
    ...
```



### 2.4.2 LeaseManager租约管理器

HDFS租约管理的操作集中在一个类上：LeaseManager。它与CacheManager（缓存管理类），SnapshotManager（快照管理类）类似，是一个中心管理类，运行在Active NameNode的服务中。租约类的定义就是在LeaseManager中的。在LeaseManager租约管理器中，它所做的事情主要归纳为两类。

第一个，维护HDFS内部当前所有的租约，并以多种映射关系进行保存。保存的映射关系分为如下3种：

- 租约持有者对租约的映射关系。
- 文件Id对租约的映射关系。
- 按照时间排序进行租约集合的保存，此关系并不是一种映射关系。

**在LeaseManager租约管理器中，还有一件重要的事情是定期释放过期的租约对象**。这个操作可以避免文件租约长期不释放导致其他客户端文件无法写文件的问题。

因为在某些异常情况下，客户端程序可能在写完文件后，没有正常关闭文件，导致文件始终处于正在写的状态中，此文件在对应的租约中没有被真正的移除掉。

LeaseManager中的解决办法是启动一个定时的监控线程，来释放过期的租约。

租约检测的操作可以归纳为如下步骤：

- 第1步，获取最老的已过期的租约。
- 第2步，得到此租约中保存的文件Id。
- 第3步，关闭这些文件Id对应的文件，并将这些文件Id从此租约中移除。
- 第4步，如果此租约中已经没有打开的文件Id，则将此租约从系统中进行移除。

## 2.5 HDFS写异常与恢复

可能的异常模式如下所列：

- Client 在写入过程中，Client 挂了
- Client 在写入过程中，DataNode 挂了
- Client 在写入过程中，NameNode 挂了

对于以上所列的异常模式，都有分别对应的恢复模式。

### 2.5.1 Client挂了

当 Client 在写入过程中，自己挂了。由于 Client 在写文件之前需要向 NameNode 申请该文件的租约（lease），只有持有租约才允许写入，而且租约需要定期续约。所以当 Client 挂了后租约会超时，HDFS 在超时后会释放该文件的租约并关闭该文件，避免文件一直被这个挂掉的 Client 独占导致其他人不能写入。这个过程称为 lease recovery。

在发起 lease recovery 时，若多个文件 block 副本在多个 DataNodes 上处于不一致的状态，首先需要将其恢复到一致长度的状态。这个过程称为 block recovery。 这个过程只能在 lease recovery 过程中发起。

### 2.5.2 DataNode挂了

当 Client 在写入过程中，有 DataNode 挂了。写入过程不会立刻终止（如果立刻终止，易用性和可用性都太不友好），取而代之 HDFS 尝试从流水线中摘除挂了的 DataNode 并恢复写入，这个过程称为 pipeline recovery。

### 2.5.3 NameNode挂了

当 Client 在写入过程中，NameNode 挂了。这里的前提是已经开始写入了，所以 NameNode 已经完成了对 DataNode 的分配，若一开始 NameNode 就挂了，整个 HDFS 是不可用的所以也无法开始写入。流水线写入过程中，当一个 block 写完后需向 NameNode 报告其状态，这时 NameNode 挂了，状态报告失败，但不影响 DataNode 的流线工作，数据先被保存下来，但最后一步 Client 写完向 NameNode 请求关闭文件时会出错，由于 NameNode 的单点特性，所以无法自动恢复，需人工介入恢复。

## 2.6 Hadoop和Spark的区别，Spark为什么快？Spark的特点是什么？

### 2.6.1 基于内存数据处理模式

spark是基于内存进行数据处理的，MapReduce是基于磁盘进行数据处理的

MapReduce的设计：中间结果保存在文件中，提高了可靠性，减少了内存占用。但是牺牲了性能，表现在Job和Job之间需要落盘，会造成大量的IO。
Spark的设计：数据在内存中进行交换，要快一些，所以性能方面比MapReduce要好。
DAG计算模型在迭代计算上还是比MapReduce的效率更高

### 2.6.2 DAG计算模型

spark中具有DAG有向无环图，DAG有向无环图在此过程中减少了shuffle以及落地磁盘的次数

Spark 计算比 MapReduce 快的根本原因在于 DAG 计算模型。一般而言，DAG 相比MapReduce 在大多数情况下可以减少 shuffle 次数。Spark 的 DAGScheduler 相当于一个改进版的 MapReduce，如果计算不涉及与其他节点进行数据交换，Spark 可以在内存中一次性完成这些操作，也就是中间结果无须落盘，减少了磁盘 IO 的操作。但是，如果计算过程中涉及数据交换，Spark 也是会把 shuffle 的数据写磁盘的！
有一个误区，Spark 是基于内存的计算，所以快，这不是主要原因，要对数据做计算，必然得加载到内存，Hadoop 也是如此，只不过 Spark 支持将需要反复用到的数据给 Cache 到内存中，减少数据加载耗时，所以 Spark 跑机器学习算法比较在行（需要对数据进行反复迭代）。Spark 基于磁盘的计算也是比 Hadoop 快。刚刚提到了 Spark 的 DAGScheduler 是个改进版的 MapReduce，所以 Spark天生适合做批处理的任务。Hadoop 的 MapReduce 虽然不如 spark 性能好，但是 HDFS 仍然是业界的大数据存储标准。

### 2.6.3 资源申请粒度

spark是粗粒度资源申请，也就是当提交spark application的时候，application会将所有的资源申请完毕，如果申请不到资源就等待，如果申请到资源才执行application，task在执行的时候就不需要自己去申请资源，task执行快，当最后一个task执行完之后task才会被释放。
**优点是执行速度快，缺点是不能使集群得到充分的利用**

MapReduce是细粒度资源申请，当提交application的时候，Task执行时，自己申请资源，自己释放资源，task执行完毕之后，资源立即会被释放，task执行的慢，application执行的相对比较慢。

优点是集群资源得到充分利用，缺点是application执行的相对比较慢。

**Spark是基于内存的，而MapReduce是基于磁盘的迭代**



# 三	Flink

## 3.1 出现的问题及解决方案

#### 问题一 

#### 问题描述

在进行双流Join时，出现OOM。

#### 定位原因

从Flink Web UI的反压监控面板和Task Metric查看

> 自定义监控指标会在Flink WebUI显示出来，如需在外部系统显示，需要自定义reporter类。

Metric这个接口有四个直接子类，分别是

```
Gauge —— 最简单的度量指标，只是简单的返回一个值，比如当前实时读取kafka数据的条数 
Counter —— 计数器，在一些情况下，会比Gauge高效，比如通过一个AtomicLong变量来统计一个队列的长度； 
Meter —— 吞吐量的度量，也就是一系列事件发生的速率，例如TPS； 
Histogram —— 度量值的统计结果，如最大值、最小值、平均值，以及分布情况等。
```

当时从Flink Web UI界面并没有看出来明显的端倪，后面通过系统的Metric库在自己的代码中进行map打点通过实现RichFunction函数收集metrics数据，通过Flink提供的外部接口reporter接口将这些metrics数据转义成JSON导出到influxDB时序性数据库：

```scala
public class MyMapper extends RichMapFunction<String, String> {
    //初始化counter对象
  private transient Counter counter;
 
  @Override
  public void open(Configuration config) {
    this.counter = getRuntimeContext()
      .getMetricGroup()
      .counter("myCounter");
  }
 
  @Override
  public String map(String value) throws Exception {
      //每进来一个事件就触发一次metrics
    this.counter.inc();
    return value;
  }
}
```

在Grafana可视化界面看到两条流，一条流的数据量特别大出现了反压的情况，因为为了保证 EOS（Exactly-Once-Semantics，准确一次），对于有两个以上输入管道的 Operator，checkpoint barrier 需要对齐（Alignment），接受到较快的输入管道的 barrier 后，它后面数据会被缓存起来但不处理，直到较慢的输入管道的 barrier 也到达，这些被缓存的数据会被放到state 里面，导致 checkpoint 变大。





另外一条流的因为barrier对齐的原因导致event产生积压，不断的累计状态。

#### 分析原因

![image-20200710000131237](D:\ProgramFiles\Typora\图片备份\image-20200710000131237.png)

在可视化界面看到两条流的QPS在纵向维度存在明显差异，V1流的起始速率远远大于V2流的起始速率，对V按照时间积分得到数据积压量等于(V2-V1)*T，由于V2-V1大于0，所以在绝对的时间域内，当系统内的状态累计到某一个临界值时，触发了OOM。

#### 解决方案

调整数据源的QPS大小。

## 3.2 流计算框架Flink和Strom的性能对比

从状态管理、窗口的支持、一致性、容错机制4个点回答。

#### 3.2.1 基于流的世界观对比

|          | Storm                                                        | Flink                                                        |
| :------- | :----------------------------------------------------------- | ------------------------------------------------------------ |
| 状态管理 | 无状态，需用户自行进行状态管理                               | 有状态                                                       |
| 窗口支持 | 对事件窗口支持较弱，缓存整个窗口的所有数据，窗口结束时一起计算 | 窗口支持较为完善，自带一些窗口聚合方法，并且会自动管理窗口状态。 |
| 一致性   | At Most Once At Least Once                                   | At Most Once At Least Once **Exactly Once**                  |
| 容错方式 | **[ACK机制](http://storm.apache.org/releases/1.1.0/Guaranteeing-message-processing.html)** ：对每个消息进行全链路跟踪，失败或超时进行重发。 | **[检查点机制](https://ci.apache.org/projects/flink/flink-docs-master/internals/stream_checkpointing.html#checkpointing)** ：通过分布式一致性快照机制，对数据流和算子状态进行保存。在发生错误时，使系统能够进行回滚。 |
| 应用现状 | 在美团点评实时计算业务中已有较为成熟的运用，有管理平台、常用 API 和相应的文档，大量实时作业基于 Storm 构建。 | 在美团点评实时计算业务中已有一定应用，但是管理平台、API 及文档等仍需进一步完善。 |

最重要的两点：状态的区别，内部对事务的维护机制。

#### 3.2.2 生产压测对比

集群参数：

8核、16GB、500G硬盘

并行度16

#### 3.2.3 结论

##### 框架本身性能

- Storm 单线程吞吐约为 8.7 万条/秒，Flink 单线程吞吐可达 35 万条/秒。Flink 吞吐约为 Storm 的 3-5 倍。
- Storm QPS 接近吞吐时延迟（含 Kafka 读写时间）中位数约 100 毫秒，99 线约 700 毫秒，Flink 中位数约 50 毫秒，99 线约 300 毫秒。Flink 在满吞吐时的延迟约为 Storm 的一半，且随着 QPS 逐渐增大，Flink 在延迟上的优势开始体现出来。
- 综上可得，**Flink 框架本身性能优于 Storm。**

#### 3.2.4 推荐使用Flink的场景

综合上述测试结果，以下实时计算场景建议考虑使用 Flink 框架进行计算： + 要求消息投递语义为 **Exactly Once** 的场景； + 数据量较大，要求**高吞吐低延迟**的场景； + 需要进行**状态管理**或**窗口统计**的场景。

## 3.3 如何监控反压？

反压还会影响到两项指标: **checkpoint 时长和 state 大小。**

Web UI  +  Metric

**反压并不会直接影响作业的可用性，它表明作业处于亚健康的状态，有潜在的性能瓶颈并可能导致更大的数据处理延迟**。通常来说，对于一些对延迟要求不太高或者数据量比较小的应用来说，反压的影响可能并不明显，然而对于规模比较大的 Flink 作业来说反压可能会导致严重的问题。

这是因为 Flink 的 checkpoint 机制，反压还会影响到两项指标: **checkpoint 时长和 state 大小。**

- 前者是因为 checkpoint barrier 是不会越过普通数据的，数据处理被阻塞也会导致 checkpoint barrier 流经整个数据管道的时长变长，因而 checkpoint 总体时间（End to End Duration）变长。
- 后者是因为为保证 EOS（Exactly-Once-Semantics，准确一次），对于有两个以上输入管道的 Operator，checkpoint barrier 需要对齐（Alignment），接受到较快的输入管道的 barrier 后，它后面数据会被缓存起来但不处理，直到较慢的输入管道的 barrier 也到达，这些被缓存的数据会被放到state 里面，导致 checkpoint 变大。

这两个影响对于生产环境的作业来说是十分危险的，因为 checkpoint 是保证数据一致性的关键，checkpoint 时间变长有可能导致 checkpoint 超时失败，而 state 大小同样可能拖慢 checkpoint 甚至导致 OOM （使用 Heap-based StateBackend）或者物理内存使用超出容器资源（使用 RocksDBStateBackend）的稳定性问题。

因此，我们在生产中要尽量避免出现反压的情况。

## 3.4 Flink统计过哪些指标？

恶意刷单，恶意点击，PV，UV

### 3.4.1 实时热门商品TopN统计

#### 需求

- 每隔5分钟输出最近一小时内点击量最多的前N个商品
- 点击量用浏览次数(“pv”)来衡量

#### 解决思路

在所有用户行为数据中，过滤出浏览(“pv”)行为进行统计 . 构建滑动窗口，窗口长度为1小时，滑动距离为5分钟 . 窗口计算使用增量聚合函数和全窗口聚合函数相结合的方法 . 使用窗口结束时间作为key，对DataStream进行keyBy()操作 . 将KeyedStream中的元素存储到ListState中，当水位线超过窗口结束时间时，排序输出。

#### 实现思路

```scala
val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setParallelism(1)
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)

    //source
    val sourceStream = env.readTextFile("D:\\源笔记资料\\hadoop生态圈\\17_Flink\\UserBehavior.csv")
    //transform
    val itemViewStream = sourceStream
      .map(line => {
        val arr = line.split(",")
        UserBehavior(arr(0).toLong, arr(1).toLong, arr(2).toLong, arr(3), arr(4).toLong * 1000)
      })

      /**
       * 基本需求
       * - 每隔5分钟输出最近一小时内点击量最多的前N个商品
       * - 点击量用浏览次数(“pv”)来衡量
       */
      .assignAscendingTimestamps(_.timestamp)
      .filter(_.behavior == "pv")
      //按用户分流
      .keyBy(_.itemId)
      .timeWindow(Time.hours(1), Time.minutes(5))
      //对一小时窗口的数据进行增量和全量聚合获取到最近一小时的总点击次数
      //窗口增量聚合函数的目的是为了累计状态
      //窗口全量聚合函数的目的是为了获取到窗口关闭时间，利用窗口关闭时间来区分每个窗口
      .aggregate(new MyAgg, new MyProcessWindow)

    //来的是以一小时作为窗口长度的流，每五分钟滑动一次。
    itemViewStream
      .keyBy(_.windowEnd)
      .process(new MyKeyedProcess(3))
      .print


    env.execute()
```

1）创建StreamExecutionEnvironment；

2）设定Time类型为EventTime；

3）不配置并行度，在提交Job时手动设置各个算子的并行度；

4）通过执行环境摄入数据源，获取到DataStream；

5）通过filter + map算子把数据ETL成UserBehavior类型

6）使用filter算子过滤出流中的点击事件

7）对流中的事件生成watermarker

8）keyBy算子按照商品ID分流

9）timeWindow开一个1小时的窗口，滑动距离为5分钟

10）窗口内使用窗口增量聚合 + 全窗口聚合函数

> 增量聚合：目的是累加状态
>
> ```scala
>  //<IN, ACC, OUT>
>   class MyAgg extends AggregateFunction[UserBehavior, Long, Long] {
>     //初始化累加器
>     override def createAccumulator(): Long = 0L
> 
>     //累加器逻辑
>     override def add(value: UserBehavior, accumulator: Long): Long = accumulator + 1
> 
>     //返回值
>     override def getResult(accumulator: Long): Long = accumulator
> 
>     //累加器合并
>     override def merge(a: Long, b: Long): Long = a + b
>   }
> ```
>
> 
>
> 全窗口函数：获取窗口关闭时间，利用关闭时间区分每个窗口
>
> ```scala
>   // ProcessWindowFunction[IN, OUT, KEY, W <: Window]
>   class MyProcessWindow extends ProcessWindowFunction[Long, ItemViewCount, Long, TimeWindow] {
> 
>     override def process(key: Long, context: Context, elements: Iterable[Long], out: Collector[ItemViewCount]): Unit = {
>         //获取窗口结束时间，封装到样例类中
>       out.collect(ItemViewCount(key.toLong, context.window.getEnd, elements.head))
>     }
>   }
> ```
>
> 

11）对全窗口函数封装的样例类中的窗口结束时间进行keyBy分流

12）对keyBy后的流使用KeyedProcessFunction遍历每一个传入的事件

> ```scala
> public abstract class KeyedProcessFunction<K, I, O> ...
> ```

```scala
//1.创建状态保存进来的每一个事件

//2.processElement方法中获取一个样例类当前窗口结束时间后的定时器
	//-目的是为了确保当前时间范围内keyBy后的所有事件已经执行完毕
//3.在定时器
```

#### 需求亮点总结

![image-20200711195112817](D:\ProgramFiles\Typora\图片备份\image-20200711195112817.png)

### 3.4.2 使用布隆过滤器实时统计UV

#### 需求

统计一小时内的UV数。

#### 解决思路

在所有用户行为数据中，过滤出浏览(“uv”)行为进行统计 。 构建窗口，窗口长度为1小时，定义窗口trigger，每进来一个事件就触发一次窗口函数，在窗口关闭时的回调函数中调用redis获取一小时内的count数量对接外部数据源。

#### 实现思路

1）创建StreamExecutionEnvironment；

2）设定Time类型为EventTime；

3）不配置并行度，在提交Job时手动设置各个算子的并行度；

4）通过执行环境摄入数据源，获取到DataStream；

5）通过filter + map算子把数据ETL成UV样例类类型；

6）使用filter算子过滤出流中的UV事件；

7）对流中的事件生成watermarker；

8）为了使用keyedStream的窗口函数对流进行map后分流；

```scala
      	DataStream
		.map(r => ("key", r.userId))
      	.keyBy(_._1)
```



9）timeWindow开一个1小时的滚动窗口；

10）自定义一个Trigger，定义触发WindowProcessFunction的规则

11）在WindowProcessFunction中借助Redis布隆过滤器实现UV去重

12）在Trigger的回调方法onEventTime中连接Redis取出本次窗口中累计的UV

#### 需求亮点总结

##### 布隆过滤器哈希

```scala
  class Bloom(size: Long) extends Serializable {
    private val cap = size

    def hash(value: String, seed: Int): Long = {
      var result = 0
      for (i <- 0 until value.length) {
        result = result * seed + value.charAt(i)
      }
      (cap - 1) & result
    }
  }
```

##### WindowProcessFunction

```scala
//以当前窗口结束作为map的主键
	val storeKey = context.window.getEnd.toString
	var count = 0L
//判断当前窗口是否是第一次滚动
	if (jedis.hget("UvCountHashTable", storeKey) != null) {
   	 count = jedis.hget("UvCountHashTable", storeKey).toLong
	}
      val userId = vals.last._2
//获取本次事件用户的布隆标志位
      val offset = bloom.hash(userId, 61)
//判断该标志位上是否被命中过的flag
      val isExist = jedis.getbit(storeKey, offset)
//如果该标志位上没有被命中则count+1存入Redis
      if (!isExist) {
        jedis.setbit(storeKey, offset, true)
        jedis.hset("UvCountHashTable", storeKey, (count + 1).toString)
      }

```

##### TriggerFunction

```scala
 class MyTrigger123 extends Trigger[(String, String), TimeWindow] {
    //使用事件时间时，窗口关闭后的回调函数
    override def onEventTime(time: Long,
                             window: TimeWindow,
                             ctx: TriggerContext): TriggerResult = {
      if (ctx.getCurrentWatermark >= window.getEnd) {
        val jedis = new Jedis("localhost", 6379)
        val key = window.getEnd.toString
        TriggerResult.FIRE_AND_PURGE
        println(key, jedis.hget("UvCountHashTable", key))
      }
      TriggerResult.CONTINUE
    }
	//使用处理事件时，窗口关闭后的回调函数
    override def onProcessingTime(
      time: Long,
      window: TimeWindow,
      ctx: TriggerContext
    ): TriggerResult = {
      TriggerResult.CONTINUE
    }

    override def clear(
      window: TimeWindow,
      ctx: Trigger.TriggerContext
    ): Unit = {}
	//进来的每一个事件
    override def onElement(element: (String, String),
                           timestamp: Long,
                           window: TimeWindow,
                           ctx: TriggerContext): TriggerResult = {
        //触发processWindowFunction清空当前窗口
      TriggerResult.FIRE_AND_PURGE
    }
  }
```

### 3.4.3 恶意登录实现

#### 需求

实时检测用户在5秒内连续登录两次失败，匹配到事件后要求用户输入验证码。

#### 解决思路

在所有用户行为数据中，过滤出登录(“login”)行为进行统计，keyBy分流，自定义processFunction定义键控状态进行判断。

#### 实现思路

##### 底层API实现

ProcessElement方法逻辑：

```scala
//定义login-fail状态 loginState
//定义定时器状态 timestamp

//判断传入的每一个事件
     if (value.loginStatus == "fail") {
         //将该次事件添加到ListState状态
        loginState.add(value)
        if (!timestamp.value()) {
            //判断当前定时器状态是否定义
          timestamp.update(value.ts.toLong * 1000 + 5000L)
            //注册一个5秒钟后的定时器
          ctx
            .timerService()
            .registerEventTimeTimer(value.ts.toLong * 1000 + 5000L)
        }
      }

//如果本次事件时登录成功事件，清空ListState状态，删除定时器状态
      if (value.loginStatus == "success") {
        loginState.clear()
        ctx
          .timerService()
          .deleteEventTimeTimer(timestamp.value())
      }
```

定时器逻辑：

```scala
	  val allLogins = ListBuffer[LoginEvent]()
      import scala.collection.JavaConversions._
      for (login <- loginState.get) {
        allLogins += login
      }
//清空状态
      loginState.clear()
//判断5秒内的登录次数
      if (allLogins.length > 1) {
        out.collect("5s以内连续两次登陆失败")
      }
```

##### CEP实现

```scala
package com.atguigu.need4

import java.util

import org.apache.flink.cep.PatternSelectFunction
import org.apache.flink.cep.pattern.conditions.IterativeCondition
import org.apache.flink.cep.scala.CEP
import org.apache.flink.cep.scala.pattern.Pattern
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.scala._


/**
 * @Classname CEPLogin
 * @Description TODO
 *              Date ${Date} 22:16
 * @Create by childwen
 */
object CEPLogin {
  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    env.setParallelism(1)

    val inputStream = env
      .fromElements(
        LoginEvent("1", "0.0.0.0", "fail", "1"),
        LoginEvent("1", "0.0.0.0", "success", "2"),
        LoginEvent("1", "0.0.0.0", "fail", "3"),
        LoginEvent("1", "0.0.0.0", "fail", "4")
      )
      .assignAscendingTimestamps(_.ts.toLong * 1000)


    val pattern = Pattern
      .begin[LoginEvent]("one")
      .where(new MyCondition)
      .times(2)
      .consecutive()


    val patternStream = CEP.pattern(inputStream.keyBy(_.userId), pattern)

    val resStream = patternStream.select(new MySelectFunction)

    resStream.print()

    env.execute()
  }


}

case class LoginEvent(userId: String, ip: String, state: String, ts: String)

class MyCondition extends IterativeCondition[LoginEvent] {
  override def filter(value: LoginEvent, ctx: IterativeCondition.Context[LoginEvent]): Boolean = {
    value.state == "fail"
  }

}

class MySelectFunction extends PatternSelectFunction[LoginEvent, String] {
  override def select(pattern: util.Map[String, util.List[LoginEvent]]): String = {
    import scala.collection.JavaConversions._
    val event = pattern.get("one").head
    s"${event.userId}号用户连续登录失败超过两次以上！！！请输入验证码"
  }
}
```

### 3.4.4 订单支付实时监控

#### 需求

- 用户下单之后，应设置订单失效时间，以提高用户支付的意愿，并降低系统风险
- 用户下单后15分钟未支付，则输出监控信息

> 在电商平台中，最终创造收入和利润的是用户下单购买的环节；更具体一点，是用户真正完成支付动作的时候。用户下单的行为可以表明用户对商品的需求，但在现实中，并不是每次下单都会被用户立刻支付。当拖延一段时间后，用户支付的意愿会降低。所以为了让用户更有紧迫感从而提高支付转化率，同时也为了防范订单支付环节的安全风险，电商网站往往会对订单状态进行监控，设置一个失效时间（比如15分钟），如果下单后一段时间仍未支付，订单就会被取消。

#### 解决思路

利用CEP从订单事实中匹配出对应的create时间和pay时间，严格近邻。同时匹配出超时事件，输出到下游。

利用底层API实现，分流后在process函数内定义键控状态，定义键控状态，注册定时器，在定时器内检测状态是否超时。

#### 实现思路

##### CEP实现

###### 创建pattern

```scala
    val pattern = Pattern
      .begin[OrderEvent]("begin").where(_.eventType == "create")
      .next("next").where(_.eventType == "pay")
      .within(Time.seconds(5))
```

###### 定义超时标签

```scala
  //定义超时标签
    val orderTimeOut = OutputTag[String]("orderTimeOut")
```

###### 匹配模板流

```scala
 //匹配模板流
    val patternStream = CEP.pattern(orderEventStream.keyBy(_.orderId), pattern)
```

###### 从模板流中获取我们关心的流如

​	获取在定义的时间窗口内hit的事件

```scala
    val payStream: DataStream[String] = patternStream
      .select(orderTimeOut, new MyTimeOutOrderPattern, new MyTimeOutSelectPattern)
```

​	获取在定义的时间窗口内超时事件

```scala
    val timeOutStream: DataStream[String] = payStream
      .getSideOutput(new OutputTag[String]("orderTimeOut"))
```

###### 定义PatternSelectFunction

```scala
  /**
   * 匹配正确事件
   */
  class MyTimeOutSelectPattern extends PatternSelectFunction[OrderEvent, String] {
    override def select(pattern: util.Map[String, util.List[OrderEvent]]): String = {
      import scala.collection.JavaConversions._
      val head = pattern.get("next").head
      s"${head.orderId}号客户已完成支付~"
    }
  }
```

###### 定义PatternTimeoutFunction

```scala
  /**
   * 匹配超时事件
   */
  class MyTimeOutOrderPattern extends PatternTimeoutFunction[OrderEvent, String] {
    override def timeout(pattern: util.Map[String, util.List[OrderEvent]], timeoutTimestamp: Long): String = {
      import scala.collection.JavaConversions._
      val head = pattern.get("begin").head
      println(timeoutTimestamp)
      s"${head.orderId}号客户支付超时~"
    }
  }
```

### 3.4.5 实时对象：订单事实双流Join

#### 亮点

利用键控状态的特性，在相同的key流中匹配状态，触发定时器。

触发定时器逻辑：先查找支付或订单状态是否存在，如果存在则匹配取消定时器。

如果不存在支付或订单状态，则注册一个定时器。

## 3.5 Flink状态有哪些？

### 3.5.1 键控状态

![image-20200618082311845](D:\ProgramFiles\Typora\图片备份\image-20200618082311845.png)

1）键控状态是根据输入数据中定义的键key来维护和访问的。

2）Flink 为每个 key 维护一个状态实例，并将具有相同键的所有数据，都分配到同一个算子任务中，这个任务会维护和处理这个 key 对应的状态。

3）**当任务处理一条数据时，它会自动将状态的访问范围限定为当前数据的 key**。

### 3.5.2 键控状态分类

1）值状态（Value state）
将状态表示为单个的值

2）列表状态（List state）
将状态表示为一组数据的列表

3）映射状态（Map state）
将状态表示为一组 Key-Value 对

4）聚合状态（Reducing state & Aggregating State）
将状态表示为一个用于聚合操作的列表

### 3.5.3 算子状态基本介绍

![image-20200616214356132](D:\ProgramFiles\Typora\图片备份\image-20200616214356132.png)

1）算子状态的作用范围限定为算子任务，由**同一并行任务所输入的所有数据都可以访问到相同的状态**
2）状态对于同一任务而言是共享的
3）算子状态不能由相同或不同算子的另一个任务访问

4）在代码中使用算子状态需要实现ListCheckPoint实现快照保存和故障恢复。

### 3.5.4 算子状态分类

1）列表状态（List state）
将状态表示为一组数据的列表

2）联合列表状态（Union list state）
也将状态表示为数据的列表。它与常规列表状态的区别在于，在发生故障时，或者从保存点（savepoint）启动应用程序时如何恢复

3）广播状态（Broadcast state）
如果一个算子有多项任务，而它的每项任务状态又都相同，那么这种特殊情况最适合应用广播状态。

## 3.6 Spark Streaming和Flink区别？

**处理模型**：

Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。  重点！！！
**架构模型**：

Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、Executor。 

Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。
**时间机制**：

时间机制SparkStreaming支持的时间机制有限，只支持处理时间。 Flink支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持 watermark 机制来处理滞后数据。
**容错机制**：

Spark的CheckPoint只能保证数据不丢失，但是无法保证不重复，Flink使用两阶段提交来处理这个问题。

## 3.7 你们的Flink怎么提交的？使用的per-job模式吗？为什么使用Yarn-Session的模式？有什么好处？

第一种yarn seesion(Start a long-running Flink cluster on YARN)这种方式需要先启动集群，然后在提交作业，接着会向yarn申请一块空间后，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn中的其中一个作业执行完成后，释放了资源，那下一个作业才会正常提交.这种方式资源被限制在session中，不能超过，比较适合特定的运行环境或者测试环境。
第二种Flink run直接在YARN上提交运行Flink作业(Run a Flink job on YARN)，这种方式的好处是一个任务会对应一个job,即每提交一个作业会根据自身的情况，向yarn申请资源，直到作业执行完成，并不会影响下一个作业的正常运行，除非是yarn上面没有任何资源的情况下。一般生产环境是采用此种方式运行。这种方式就需要确保集群资源足够。

## 3.8 Flink实时计算 维表Join

### 3.8.1 什么是维表？

维表作为 SQL 任务中一种常见表的类型，其本质就是关联表数据的额外数据属性，通常在 Join 语句中进行使用。比如源数据有人的身份证号，人名，你现在想要得到人的家庭地址，那么可以通过身份证号去关联人的身份证信息，就可以得到更全的数据。

![image-20200713203831175](D:\ProgramFiles\Typora\图片备份\image-20200713203831175.png)

维表可以是静态的数据，也可以是动态的数据(比如定时更新的数据)，一般会通过特定的主键来进行关联。它可以在 **Mysql** 中进行存储，也可以在 **Nosql** 数据库中进行存储，比如 **HBase**等。

## 3.9 Flink-perJob和Flink-Session的区别？

![image-20200714212847688](D:\ProgramFiles\Typora\图片备份\image-20200714212847688.png)

yarn-session会将所有的job放在一个容器里执行。

yarn-session -d 会在容器里将任务和任务之间相互隔开，互不影响。

## 3.10 flink和spark的区别，使用场景

2模2机，内存管理

- Spark: 由java管理到tungsten来管理内存
- Flink: 由Flink自身管理内存

![image-20200716212401455](D:\ProgramFiles\Typora\图片备份\image-20200716212401455.png)



#  四 HBase

## 4.1 LSM树

### 4.1.1 LSM树基本介绍

LSM树，即日志结构合并树(Log-Structured Merge-Tree)。其实它并不属于一个具体的数据结构，它更多是一种数据结构的设计思想。大多NoSQL数据库核心思想都是基于LSM来做的，只是具体的实现不同。

## 4.2 什么是NoSQL？

### 4.2.1 关系型数据库的查询瓶颈

当用户表的数据达到几千万甚至几亿级别的时候，对单条数据的检索将花费数秒甚至达到分钟级别。实际情况更复杂，查询的操作速度将会受到以下两个因素的影响：

① 高并发的更新（插入、修改、删除）操作。大中型网站的并发操作一般能达到几十甚至几百并发，此时单条数据查询的延时将轻而易举地达到分钟级别。

② 多表关联后的复杂查询，以及频繁的group by或者order by操作，此时性能下降较为明显。

### 4.2.2 CAP定理

![image-20200711085916127](D:\ProgramFiles\Typora\图片备份\image-20200711085916127.png)

Consistency 强一致性：数据更新操作的一致性，所有数据变动都是同步的。

Availability 高可用性：良好的响应性能。

Partition tolerance 高分区容错性：可靠性。

### 4.2.3 NoSQL

有些数据库在实现性能的同时会牺牲一部分一致性，即数据在更新时，不会立刻同步，而是经过了一段时间才达到一致性。这个特性也称之为最终一致性！

## 4.3 Hbase基本介绍

### 4.3.1 HBase来源

2006年Google技术人员发布了一篇文章Bigtable，文章向世人介绍了一种分布式的数据库，这种数据库可以在局部几台服务器崩溃的情况下继续提供高性能的服务。

2007年Powerset公司的工作人员基于此文研发了BigTable的Java开源版本，即HBase。刚开始它只是Hadoop的一部分。

2008年HBase成为了Apache的顶级项目。HBase几乎实现了BigTable的所有特性。它被称为一个开源的非关系型分布式数据库。

### 4.3.2 HBase定义

HBase是一种分布式、可扩展、支持海量数据库的NoSQL数据库。

HBase面向列存储，构建于Hadoop之上，类似于Google的BigTable，提供对**10亿级别表数据的快速随机实时读写！**

### 4.3.3 HBase的特点

#### 1）海量存储

HBase适合存储PB级别的海量数据，在PB级别的数据以及采用廉价PC存储的情况下，能在几十到百毫秒内返回数据。这与HBase的极易扩展性息息相关。正是因为HBase良好的扩展性，才为海量数据的存储提供了便利。

#### 2）列式存储

这里的列式存储其实说的是列族存储，HBase是根据列族来存储数据的。列族下面可以有非常多的列，列族在创建表的时候就必须指定。

#### 3）极易扩展

HBase的扩展性主要体现在两个方面，一个是基于上层处理能力（RegionServer）的扩展，一个是基于存储的扩展（HDFS）。

通过横向添加RegionSever的机器，进行水平扩展，提升HBase上层的处理能力，提升Hbsae服务更多Region的能力。

#### 4）高并发

由于目前大部分使用HBase的架构，都是采用的廉价PC，因此单个IO的延迟其实并不小，一般在几十到上百ms之间。这里说的高并发，主要是在并发的情况下，HBase的单个IO延迟下降并不多。能获得高并发、低延迟的服务。

#### 5）稀疏

稀疏主要是针对HBase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的。

### 4.3.4 HBase的优点

①HDFS有高容错，高扩展的特点，而Hbase基于HDFS实现数据的存储，因此Hbase拥有与生俱来的超强的扩展性和吞吐量。

②HBase采用的是Key/Value的存储方式，这意味着，即便面临海量数据的增长，也几乎不会导致查询性能下降。

③HBase是一个列式数据库，相对于于传统的行式数据库而言。当你的单张表字段很多的时候，可以将相同的列(以regin为单位)存在到不同的服务实例上，分散负载压力。

### 4.3.5 HBase的缺点

①架构设计复杂，且使用HDFS作为分布式存储，因此只是存储少量数据，它也不会很快。在大数据量时，它慢的不会很明显！

②Hbase不支持表的关联操作，因此数据分析是HBase的弱项。常见的 group by或order by只能通过编写MapReduce来实现！

③Hbase部分支持了ACID

### 4.3.6 HBase的总结

适合场景：单表超千万，上亿，且高并发！

不适合场景：主要需求是数据分析，比如做报表。数据量规模不大，对实时性要求高！

### 4.3.7 数据模型

逻辑上，HBase的数据模型同关系型数据库很类似，数据存储在一张表中，有行有列。但从HBase的底层物理存储结构（K-V）来看，HBase更像是一个multi-dimensional map。

### 4.3.8 逻辑结构

![image-20200711101249569](D:\ProgramFiles\Typora\图片备份\image-20200711101249569.png)

### 4.3.9 HBase物理存储结构

![image-20200711101405855](D:\ProgramFiles\Typora\图片备份\image-20200711101405855.png)

### 4.3.10 基本架构

#### Region Server

RegionServer是一个服务，负责多个Region的管理。其实现类为HRegionServer，主要作用如下:

对于数据的操作：get, put, delete；

对于Region的操作：splitRegion、compactRegion。

客户端从ZooKeeper获取RegionServer的地址，从而调用相应的服务，获取数据。

#### Master

Master是所有Region Server的管理者，其实现类为HMaster，主要作用如下：

对于表的操作：create, delete, alter，这些操作可能需要跨多个ReginServer，因此需要Master来进行协调！

对于RegionServer的操作：分配regions到每个RegionServer，监控每个RegionServer的状态，负载均衡和故障转移。

即使Master进程宕机，集群依然可以执行数据的读写，只是不能进行表的创建和修改等操作！当然Master也不能宕机太久，有很多必要的操作，比如创建表、修改列族配置，以及更重要的分割和合并都需要它的操作。

#### Zookeeper

RegionServer非常依赖ZooKeeper服务，ZooKeeper管理了HBase所有RegionServer的信息，包括具体的数据段存放在哪个RegionServer上。

客户端每次与HBase连接，其实都是先与ZooKeeper通信，查询出哪个RegionServer需要连接，然后再连接RegionServer。Zookeeper中记录了读取数据所需要的元数据表

hbase:meata,因此关闭Zookeeper后，客户端是无法实现读操作的！

HBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。

#### HDFS

HDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用的支持。

## 4.4 RegionServer架构

![image-20200711141459581](D:\ProgramFiles\Typora\图片备份\image-20200711141459581.png)

### 4.4.1 StoreFile

保存实际数据的物理文件，StoreFile以Hfile的形式存储在HDFS上。每个Store会有一个或多个StoreFile（HFile），数据在每个StoreFile中都是有序的。

### 4.4.2 MemStore

写缓存，由于HFile中的数据要求是有序的，所以数据是先存储在MemStore中，排好序后，等到达刷写时机才会刷写到HFile，每次刷写都会形成一个新的HFile。

### 4.4.3 WAL

由于数据要经MemStore排序后才能刷写到HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入MemStore中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。

每间隔hbase.regionserver.optionallogflushinterval(默认1s)， HBase会把操作从内存写入WAL。 

一个RegionServer上的所有Region共享一个WAL实例。

WAL的检查间隔由hbase.regionserver.logroll.period定义，默认值为1小时。检查的内容是把当前WAL中的操作跟实际持久化到HDFS上的操作比较，看哪些操作已经被持久化了，被持久化的操作就会被移动到.oldlogs文件夹内（这个文件夹也是在HDFS上的）。一个WAL实例包含有多个WAL文件。WAL文件的最大数量通过hbase.regionserver.maxlogs（默认是32）参数来定义。

### 4.4.4 BlockCache

读缓存，每次查询出的数据会缓存在BlockCache中，方便下次查询。

## 4.5 HBase读流程

![image-20200711142423792](D:\ProgramFiles\Typora\图片备份\image-20200711142423792.png)

1）Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。

2）访问对应的Region Server，获取hbase:meta表，根据读请求的namespace:table/rowkey，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。

3）与目标Region Server进行通讯；

4）分别在Block Cache（读缓存），MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。

5）将查询到的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。

6）将合并后的最终结果返回给客户端。

## 4.6 MemStore Flush

![image-20200711143230775](D:\ProgramFiles\Typora\图片备份\image-20200711143230775.png)

在讨论MemStore Flush之前，我们应该明确的是各组件之间的层级关系：

### HMaster

一个HBase集群中，有一个Active HMaster，是所有Region Server的管理者，**主要作用是对于表的操作，因为表的操作经常会涉及到跨RegionServer**，因此需要HMaster来进行协调。

所以即使，HMaster进程宕机，集群依然可以进行数据的读写，只是不能进行表的create和修改等操作，当然Master也不能宕机太久，因为有很多必要的操作，比如创建表，修改列族配置，**以及更要的分割合并都需要它的操作**。

### RegionServer

RegionServer是一个服务，负责多个Region的管理，主要作用是：

对数据的操作，如：get、put、delete；

对Region的操作，如：splitRegion、compactRegion

客户端从Zookeeper获取RegionServer的地址，从而调用响应的服务，获取数据。

### Region

![image-20200711144555951](D:\ProgramFiles\Typora\图片备份\image-20200711144555951.png)

![image-20200711144746844](D:\ProgramFiles\Typora\图片备份\image-20200711144746844.png)

**站在表的角度：**

每个Region由`rowKey + 多个列族`组成，多个列族之间的类型可以不同。

一个列族有多个列，一个列族中的所有列的字段类型必须一致。

**站在物理存储的角度：**

每个Region由MemStore + StoreFile组成。

- MemStore为写缓存，目的是为了解决StoreFile中的数据有序问题，每次操作数据都会先存储在MemStore中排序后，等到刷写时机刷写成物理文件。

- StoreFile是保存实际数据的物理文件，以HFile的形式存储在HDFS上， 所以每次刷写都会生成一些新的StoreFile。

综上，MemStore的意义就是在写入HDFS前，将其中的数据整理有序等待刷写。

### MemStore刷写时机

#### 手动刷写

```
flush `表名`
flush `region`
```

> flush之后的文件，是HFile文件类型，不是一个文件文件，无法直接查看。借助Hbase提供的工具查看

```
hbase org.apache.hadoop.hbase.io.hfile.HFile -e -p -f 文件路径
```



#### 自动刷写

##### 基于MemStore的刷写

​		单个memstore的大小，超过

```
hbase.hregion.memstore.flush.size(默认128M)
```

​		当前memstore所在region的所有的列族都会执行刷写！



一旦单个memstore使用的空间，超过

```
hbase.hregion.memstore.flush.size（默认值128M）
hbase.hregion.memstore.block.multiplier（默认值4)
```

,此时会阻塞客户端继续向此memstore写入！

##### 基于RegionServer的刷写

一个regionserver负责多个region，每个region又包含多个store对象，每个store对象都有一个memstore!

regionserver和memstore是1对N的关系！

当一个regionserver所负责的所有的store对象的memstore总大小超过java_heapsize的默认百分比大小就会进行刷写。

```
hbase.regionserver.global.memstore.size（默认值0.4）
hbase.regionserver.global.memstore.size.lower.limit（默认值0.95）
```

将当前regionserver所负责的所有的memstore按照大小排序！从大到小依次刷写，直到regionserver所负责的所有的store对象的memstore总大小低于以上阈值！

如果regionserver所负责的所有的store对象的memstore总大小已经超过

```
hbase.regionserver.global.memstore.size(默认值0.4）
```

此时，会阻塞客户端向当前regionserver的所有region的写入！

##### 基于时间的刷写

默认每间隔**hbase.regionserver.optionalcacheflushinterval（默认1小时）**，自动刷写

##### 基于WAL处理文件的刷写

如果正在使用的WAL文件的数量超过32，此时会根据WAL文件的生成时间，从早到晚依次刷写！

###  4.7 StoreFile Compaction

由于Hbase依赖HDFS存储，HDFS只支持追加写。所以，当新增一个单元格的时候，HBase在HDFS上新增一条数据。当修改一个单元格的时候，HBase在HDFS又新增一条数据，只是版本号比之前那个大（或者自定义）。当删除一个单元格的时候，HBase还是新增一条数据！只是这条数据没有value，类型为DELETE，也称为墓碑标记（Tombstone）

HBase每间隔一段时间都会进行一次合并（Compaction），合并的对象为HFile文件。合并分为两种

minor compaction和major compaction。

在HBase进行major compaction的时候，它会把多个HFile合并成1个HFile，在这个过程中，一旦检测到有被打上墓碑标记的记录，在合并的过程中就忽略这条记录。这样在新产生的HFile中，就没有这条记录了，自然也就相当于被真正地删除了

由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的HFile中，因此查询时需要遍历所有的HFile。为了减少HFile的个数，以及清理掉过期和删除的数据，会进行StoreFile Compaction。

Compaction分为两种，分别是Minor Compaction和Major Compaction。Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，但不会清理过期和删除的数据。Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉过期和删除的数据。

![image-20200711155533260](D:\ProgramFiles\Typora\图片备份\image-20200711155533260.png)

## 4.7 Region Split

### 4.7.1 Region Split基本介绍

默认情况下，每个Table起初只有一个Region，随着数据的不断写入，Region会自动进行拆分。刚拆分时，两个子Region都位于当前的Region Server，但处于负载均衡的考虑，HMaster有可能会将某个Region转移给其他的Region Server。

### 4.7.1 Region Split切分时机

#### 0.94版本之前

当1个region中的某个Store下所有StoreFile的总大小超过hbase.hregion.max.filesize，该Region就会进行拆分（0.94版本之前）。

#### 0.94版本之后

默认使用IncreasingToUpperBoundRegionSplitPolicy策略切分region, **getSizeToCheck()是检查region的大小以判断是否满足切割切割条件**。

**tableRegionsCount：为当前Region Server中属于该Table的region的个数。**

**getDesiredMaxFileSize() 这个值是hbase.hregion.max.filesize参数值，默认为10GB。**

具体的切分策略为tableRegionsCount在0和100之间，则为

initialSize（默认为2*128） * tableRegionsCount^3,例如：

第一次split：1^3 * 256 = 256MB 

第二次split：2^3 * 256 = 2048MB 

第三次split：3^3 * 256 = 6912MB 

第四次split：4^3 * 256 = 16384MB > 10GB，因此取较小的值10GB 

后面每次split的size都是10GB了。

tableRegionsCount超过100个，则超过10GB才会切分region。

## 4.8 HBase优化

### 4.8.1 高可用

创建backup-master文件

在文件中配置HMaster节点

将conf目录发送到其他节点

### 4.8.2 预分区

每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。

#### 手动设置预分区

```
HBase> create 'staff1','info','partition1',SPLITS => ['1000','2000','3000','4000']
```

#### 生成16进制序列预分区

```
create 'staff2','info','partition2',{NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}
```

#### 按照文件设置的规则预分区

创建splits.txt文件内容如下：

```
aaaa
bbbb
cccc
dddd
```

然后执行：

```
create 'staff3','partition3',SPLITS_FILE => 'splits.txt'
```

#### 使用JavaAPI创建预分区

```java
//自定义算法，产生一系列Hash散列值存储在二维数组中
byte[][] splitKeys = 某个散列值函数
//创建HBaseAdmin实例
HBaseAdmin hAdmin = new HBaseAdmin(HBaseConfiguration.create());
//创建HTableDescriptor实例
HTableDescriptor tableDesc = new HTableDescriptor(tableName);
//通过HTableDescriptor实例和散列值二维数组创建带有预分区的HBase表
hAdmin.createTable(tableDesc, splitKeys);
```

### 4.8.3 RowKey的设计

一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的 ，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈rowkey常用的设计方案。

#### 生成随机数、hash、散列值

```
原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7
原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd
原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913
```

**在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。**

#### 字符串反转

```
20170524000001转成10000042507102
20170524000002转成20000042507102
```

这样也可以在一定程度上散列逐步put进来的数据。

#### 字符串拼接

```
20170524000001_a12e
20170524000001_93i7
```

### 4.8.4 内存优化

HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是**不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了**，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。

###  4.8.5 基础优化

#### 允许在HDFS的文件追加内容

开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。

#### 优化DN允许的最大文件打开数

hdfs-site.xml

属性：dfs.datanode.max.transfer.threads

解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096

#### 优化延迟高的数据操作的等待时间

hdfs-site.xml

属性：dfs.image.transfer.timeout

解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。

#### 优化数据的写入效率（开启压缩）

mapred-site.xml

属性：

mapreduce.map.output.compress

mapreduce.map.output.compress.codec

解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。

#### 设置RPC监听数量

HBase-site.xml

属性：HBase.regionserver.handler.count

解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。

#### 优化HStore文件大小

HBase-site.xml

属性：HBase.hregion.max.filesize

解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。

#### 优化HBase客户端缓存

HBase-site.xml

属性：HBase.client.write.buffer

解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。

#### 指定scan.next扫描HBase所获取的行数

HBase-site.xml

属性：HBase.client.scanner.caching

解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。

#### Flush（刷写）、compact（合并）、split（切分）机制

当MemStore达到阈值，将Memstore中的数据Flush进Storefile；

compact机制则是把flush出来的小文件合并成大的Storefile文件；

split则是当Region达到阈值，会把过大的Region一分为二。

**涉及属性：**

128M就是Memstore的默认阈值

HBase.hregion.memstore.flush.size：134217728

这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。

HBase.regionserver.global.memstore.upperLimit：0.4

HBase.regionserver.global.memstore.lowerLimit：0.38

当MemStore使用内存总量达到HBase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit

## 4.9 在商业项目中的能力

**每天：**

1) 消息量：发送和接收的消息数超过60亿

2) 将近1000亿条数据的读写

3) 高峰期每秒150万左右操作

4) 整体读取数据占有约55%，写入占有45%

5) 超过2PB的数据，涉及冗余共6PB数据

6) 数据每月大概增长300千兆字节。

## 4.10 RowKey在Region中的作用

在 HBase 中，Region 相当于一个数据的分片，每个 Region 都有`StartRowKey`和`StopRowKey`，这是表示 Region 存储的 RowKey 的范围，HBase 表的数据是按照 RowKey 来分散到不同的 Region，要想将数据记录均衡的分散到不同的Region中去，因此需要 RowKey 满足这种散列的特点。此外，在数据读写过程中也是与RowKey 密切相关，RowKey在读写过程中的作用：

1. 读写数据时通过 RowKey 找到对应的 Region；
2. MemStore 中的数据是按照 RowKey 的字典序排序；
3. HFile 中的数据是按照 RowKey 的字典序排序。

## 4.11 Row设计原则

**长度原则**

RowKey是一个二进制码流，可以是任意字符串，最大长度为64kb，实际应用中一般为10-100byte，以byte[]形式保存，一般设计成定长。建议越短越好，不要超过16个字节，原因如下：

1. 数据的持久化文件HFile中时按照Key-Value存储的，如果RowKey过长，例如超过100byte，那么1000w行的记录，仅RowKey就需占用近1GB的空间。这样会极大影响HFile的存储效率。
2. MemStore会缓存部分数据到内存中，若RowKey字段过长，内存的有效利用率就会降低，就不能缓存更多的数据，从而降低检索效率。
3. 目前操作系统都是64位系统，内存8字节对齐，控制在16字节，8字节的整数倍利用了操作系统的最佳特性。

**唯一原则**

必须在设计上保证RowKey的唯一性。由于在HBase中数据存储是Key-Value形式，若向HBase中同一张表插入相同RowKey的数据，则原先存在的数据会被新的数据覆盖。

**散列原则**

设计的RowKey应均匀的分布在各个HBase节点上。

## 4.12 HBase Region的自动切分策略

### 4.12.1 Region自动切分

HBase 中，表会被划分为1...n 个 Region，被托管在 RegionServer 中。

Region 二个重要的属性：StartKey 与 EndKey 表示这个 Region 维护的 RowKey 范围，当读/写数据时，如果 RowKey 落在某个 start-end key 范围内，那么就会定位到目标region并且读/写到相关的数据。

默认，HBase 在创建表的时候，会自动为表分配一个 Region，正处于混沌时期，start-end key 无边界，所有 RowKey 都往这个 Region里分配。

当数据越来越多，Region 的 size 越来越大时，达到默认的阈值时（根据不同的拆分策略有不同的阈值），HBase 中该 Region 将会进行 split，会找到一个 MidKey 将 Region 一分为二，成为 2 个 Region。而 MidKey 则为这二个 Region 的临界，左为 N 无下界，右为 M 无上界。< MidKey 被分配到 N 区，> MidKey 则会被分配到 M 区。

随着数据量进一步扩大，分裂的两个 Region 达到临界后将重复前面的过程，分裂出更多的 Region。

### 4.12.2 Region自动切分过程

Region 的分割操作是不可见的，Master 不会参与其中。RegionServer 拆分 Region的步骤是：先将该 Region 下线，然后拆分，将其子 Region 加入到 META 元信息中，再将他们加入到原本的 RegionServer 中，最后汇报 Master。

*执行 split 的线程是 CompactSplitThread。*

### 4.12.3 Region自动切分策略的配置

在 2.0.5 版本中，HBase 提供了 7 种自动拆分策略：

![image-20200712111957231](D:\ProgramFiles\Typora\图片备份\image-20200712111957231.png)

#### 三种配置方法

- 在 hbase-site.xml 中配置，例如：

```xml
<property> 
  <name>hbase.regionserver.region.split.policy</name> 
  <value>org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy</value> 
</property>
```

- 在 HBase Configuration中配置：

```properties
private static Configuration conf = HBaseConfiguration.create();
conf.set("hbase.regionserver.region.split.policy", "org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy");
```

- 在创建表的时候配置：Region 的拆分策略需要根据表的属性来合理的配置， 所以在建表的时候不建议用前两种方式配置，而是针对不同的表设置不同的策略，每种策略在建表时具体使用在解释每种策略的时候说明。

#### 根据文件大小决定何时切

##### ConstantSizeRegionSplitPolicy（常数切分策略）

0.94.0 之前的默认拆分策略，这种策略非常简单，只要 **Region 中的任何一个 StoreFile 的大小达到了`hbase.hregion.max.filesize` 所定义的大小**，就进行拆分。

**1）相关参数：**

**hbase.hregion.max.filesize**

- default: 10737418240 (10GB)
- description: 当一个 Region 的任何一个 StoreFile 容量达到这个配置定义的大小后,就会拆分 Region

**2）拆分效果：**

经过这种策略的拆分后，Region 的大小是均匀的，例如一个 10G 的Region，拆分为两个 Region 后，这两个新的 Region 的大小是相差不大的，理想状态是每个都是5G。

**ConstantSizeRegionSplitPolicy **切分策略对于大表和小表没有明显的区分，阈值（hbase.hregion.max.filesize）：

- 设置较大对大表比较友好，但是小表就有可能不会触发分裂，极端情况下可能就1个，这对业务来说并不是什么好事；
- 设置较小则对小表友好，但大表就会在整个集群产生大量的 Region，这对于集群的管理、资源使用、failover 来说都不是一件好事。

##### IncreasingToUpperBoundRegionSplitPolicy（分片判断因子切分策略）

该策略继承自 ConstantSizeRegionSplitPolicy，是 0.94.0  到 2.0.0 版本的默认策略，其**优化了原来 ConstantSizeRegionSplitPolicy 只是单一按照 Region 文件大小的拆分策略，增加了对当前表的分片数作为判断因子**。当Region中某个 Store Size  达到 sizeToCheck 阀值时进行拆分。

**1）相关参数：**

**hbase.hregion.max.filesize**

- default: 10737418240 (10GB)
- description: 当一个 Region 的任何一个 StoreFile 容量达到这个配置定义的大小后，就会拆分 Region，该策略下并不会一开始就以该值作为拆分阈值

**hbase.increasing.policy.initial.size**

- default: none
- description: IncreasingToUpperBoundRegionSplitPolicy 拆分策略下用于计算 Region 阈值的一个初始值

**hbase.hregion.memstore.flush.size**

- default: 134217728 (128MB)
- description: 如果 Memstore 的大小超过这个字节数，它将被刷新到磁盘



在默认情况，使用IncreasingToUpperBoundRegionSplitPolicy 策略拆分 Region 的过程是:

- 某张表刚开始只有一个Region， 当 Region 达到 256M 的时候开始拆分 ：

- 拆分后这张表有两个 Region，当 Region 大小达到 2GB 时开始拆分：

- 以此类推，当表有3个 Region 的时候，Region 的最大容量为 6.75G
- 当表有4个Region的时候，计算出来的结果大于10GB，所以使用10GB 作为以后的拆分上限

**2）拆分效果：**

和 ConstantSizeRegionSplitPolicy  一样，也是均匀拆分。

不同的是，**IncreasingToUpperBoundRegionSplitPolicy** 切分策略弥补了ConstantSizeRegionSplitPolicy 的短板，能够自适应大表和小表，并且在大集群条件下对于很多大表来说表现很优秀。

但并不完美，这种策略下很多小表会在大集群中产生大量小 Region，分散在整个集群中。而且在发生 Region 迁移时也可能会触发 Region 分裂。

##### SteppingSplitPolicy

2.0 版本默认切分策略。SteppingSplitPolicy 是IncreasingToUpperBoundRegionSplitPolicy 的子类，其对 Region 拆分文件大小做了优化，如果只有1个 Region 的情况下，那第1次的拆分就是 256M，后续则按配置的拆分文件大小（10G）做为拆分标准。

**1）相关参数：**

同 IncreasingToUpperBoundRegionSplitPolicy 。

**2）拆分效果：**

在 IncreasingToUpperBoundRegionSplitPolicy 策略中，针对大表的拆分表现很不错，但是针对小表会产生过多的 Region，SteppingSplitPolicy 则将小表的 Region 控制在一个合理的范围，对大表的拆分也不影响。

#### 触发切分后，可以使用RowKey定义如何切

##### KeyPrefixRegionSplitPolicy（定义拆分点）

KeyPrefixRegionSplitPolicy 是 IncreasingToUpperBoundRegionSplitPolicy 的子类，该策略除了具备其父类自动调整 Region 拆分阈值大小、适应大小表的特点外，增加了对拆分点(splitPoint，拆分点就是 Region 被拆分处的 RowKey)的定义，可以保证有相同前缀的 RowKey不会被拆分到两个不同的 Region 里面。

**1）相关参数：**

在 IncreasingToUpperBoundRegionSplitPolicy 的配置之上增加了一个参数。

**KeyPrefixRegionSplitPolicy.prefix_length**

- default: 0，会尝试去使用已经废弃的参数 prefix_split_key_policy.prefix_length 值
- description: 指定了在 RowKey 中，取前几个字符作为前缀，例如这个设置这个值为5，那么在 RowKey中，如果前5个字符是相同的，拆分后也一定会在一个Region中

**2）拆分效果**

- 普通拆分

  ![image-20200712113400329](D:\ProgramFiles\Typora\图片备份\image-20200712113400329.png)

- 按照rowKey前缀拆分

  ![image-20200712113420598](D:\ProgramFiles\Typora\图片备份\image-20200712113420598.png)

*KeyPrefixRegionSplitPolicy （SteppingSplitPolicy、DelimitedKeyPrefixRegionSplitPolicy、BusyRegionSplitPolicy (HBase-2.x Only)）按照 RowKey 的前缀去拆分 Region，但是什么时候拆分，原 Region 容量的最大值是多少还是需要使用 IncreasingToUpperBoundRegionSplitPolicy 的方法去计算*。

如果所有数据都只有一两个前缀，那么采用默认的策略较好。  如果前缀划分的比较细，查询就比较容易发生跨 Region 查询的情况，此时采用KeyPrefixRegionSplitPolicy 较好。

所以这个策略适用的场景是：

- 数据有多种前缀
- 查询多是针对前缀，较少跨越多个前缀来查询数据

##### DelimitedKeyPrefixRegionSplitPolicy

继承自 IncreasingToUpperBoundRegionSplitPolicy，也是根据 RowKey 前缀来进行拆分的。不同就是：KeyPrefixRegionSplitPolicy 是根据 RowKey 的固定前几位字符来进行判断，而 DelimitedKeyPrefixRegionSplitPolicy 是根据分隔符来判断的。

**1）相关参数：**

在 IncreasingToUpperBoundRegionSplitPolicy 的配置之上增加了一个参数。

**DelimitedKeyPrefixRegionSplitPolicy.delimiter**

- default: none
- description: 使用该参数定义的分隔符分隔 RowKey ，分隔后的前部分相同的 RowKey 拆分后一定会在一个 Region 中

**2）拆分效果：**

DelimitedKeyPrefixRegionSplitPolicy  根据 RowKey 中指定分隔字符做为拆分，显得更加灵活，如 RowKey 的值为“userid_eventtype_eventid”，userId 不是定长的，则 DelimitedKeyPrefixRegionSplitPolicy 可以取 RowKey 值中从左往右且第一个分隔字符串之前的字符做为拆分串，在该示例中就是“userid”。

![image-20200712113558861](D:\ProgramFiles\Typora\图片备份\image-20200712113558861.png)

#### 根据热点决定如何切

##### BusyRegionSplitPolicy

之前的策略都未考虑 Region 热点问题，考虑某些 Region 可能被频繁访问，负荷很大，BusyRegionSplitPolicy 策略同样继承自 IncreasingToUpperBoundRegionSplitPolicy，但主要针对 Region 问题，是在 2.x 中新增加的拆分策略。

**1）相关参数：**

在 IncreasingToUpperBoundRegionSplitPolicy 的配置之上增加了如下参数：

**hbase.busy.policy.blockedRequests**

- default: 0.2f
- description: 请求阻塞率，即请求被阻塞的严重程度。取值范围是[0.0, 1.0]，默认是0.2，即20%的请求被阻塞的意思。

**hbase.busy.policy.minAge**

- default: 600000 (10min)
- description: 拆分最小年龄。当 Region 的年龄比这个小的时候不拆分，这是为了防止在判断是否要拆分的时候出现了短时间的访问频率波峰，结果没必要拆分的 Region 被拆分了，因为短时间的波峰会很快地降回到正常水平。单位毫秒，默认值是 600000，即10分钟。

**hbase.busy.policy.aggWindow**

- default: 300000 (5min)
- description: 计算是否繁忙的时间窗口，单位毫秒，默认值是300000，即5分钟。用以控制计算的频率。

**2）拆分效果：**

如果系统常常会出现热点 Region，又对性能有很高的追求，那么这种策略可能会比较适合。

它会通过拆分热点 Region 来缓解热点 Region 的压力，但是根据热点来拆分Region 也会带来很多不确定性因素，因为不能确定下一个被拆分的 Region 是哪个。

#### 禁用切分

##### DisabledRegionSplitPolicy

DisabledRegionSplitPolicy 就是不使用 Region 拆分策略，将所有的数据都写到同一个 Region 中。

**2）拆分效果：**

这个策略极少使用。

即使在建表的时候合理的进行了预拆分，还没有写入的数据的时候就已经手动分好了 Region，但是随着数据的持续写入，我预先分好的 Region 的大小也会达到阈值，那时候还是要依靠 HBase 的自动拆分策略去拆分 Region。

但这种策略也有它的用途：

假如有一批静态数据，一次存入以后不会再加入新数据，且这批数据主要是用于查询，为了性能好一些，可以先进行预分区后，各个 Region 数据量相差不多，然后设置拆分策略为禁止拆分，最后导入数据即可。



## 4.13 HBase Region预分区

### 4.13.1 为什么需要预分区？

已经有自动分区了，为什么还需要预分区？

HBase 在创建表的时候，会自动为表分配一个Region，当一个 Region 达到拆分条件时（shouldSplit 为 true），HBase 中该 Region 将会进行 split，分裂为2个 Region，以此类推。表在进行 split 的时候，会耗费很多的资源，有大量的 io 操作，频繁的分区对  HBase 的性能有很大的影响。

所以，HBase 提供了预分区功能，让用户可以在创建表的时候对表按照一定的规则分区。

假设初始 10 个 Region，那么导入大量数据的时候，就会均衡到 10 个 Region 里面，显然比初始 1 个 Region 要好很多，**合理的预分区可以减少 Region 热点问题，提升写数据的性能和速度，而且也能减少后续的 split 操作**。

### 4.13.2 如何预分区？

首先要明白数据的 RowKey 是如何分布的，然后根据 RowKey 的特点规划要分成多少 Region，每个 Region 的 startKey 和 endKey 是多少，接着就可以预分区了。

在建表语句中执行SPLITS

```
create 'test', {NAME => 'cf', VERSIONS => 3, BLOCKCACHE => false}, SPLITS => ['10','20','30']
```

或者指定SPLIT_FILE的文件，从文件中读取分区值。

```
create 'test', {NAME =>'cf', COMPRESSION => 'SNAPPY'}, {SPLITS_FILE =>'region_split_info.txt'}
```



在HBase Web UI可以观察：

![image-20200712114514006](D:\ProgramFiles\Typora\图片备份\image-20200712114514006.png)

### 4.13.3 随机散列与预分区

为防止热点问题，同时避免 Region Split 后，部分 Region 不再写数据或者很少写数据。也为了得到更好的并行性，希望有好的 load blance，让每个节点提供的请求处理都是均等的，并且 Region 不要经常 split，因为 split 会使 server 有一段时间的停顿，随机散列加上预分区是比较好的解决方式。

预分区一开始就预建好了一部分 Region，这些 Region 都维护着自已的 start-end keys，再配合上随机散列，写数据能均等地命中这些预建的 Region，就能通过良好的负载，提升并行，大大地提高了性能。

**hash + 预分区**

在 RowKey 的前面拼接通过 hash 生成的随机字符串，可以生成范围比较随机的 RowKey，可以比较均衡分散到不同的 Region 中，那么就可以解决写热点问题。

假设 RowKey 原本是自增长的 long 型，可以将 RowKey 先进行 hash，加上本身 id  ，组成rowkey，这样就生成比较随机的 RowKey 。

```java
long currentId = 1L;
byte [] rowkey = Bytes.add(MD5Hash.getMD5AsHex(Bytes.toBytes(currentId)).substring(0, 8).getBytes(),
Bytes.toBytes(currentId));
```

那么对于这种方式的 RowKey 设计，如何去进行预分区？

1. 先取样，先随机生成一定数量的 RowKey ，将取样数据按升序排序放到一个集合里；
2. 根据预分区的 Region 个数，对整个集合平均分割，得到相关的 splitKeys；
3. 再创建表时将步骤2得到的 splitKeys 传入即可。

**partition + 预分区**

partition 顾名思义就是分区式，这种分区有点类似于 mapreduce 中的 partitioner，将区域用长整数作为分区号，每个 Region 管理着相应的区域数据，在 RowKey 生成时，将 id 取模后，然后拼上 id 整体作为 RowKey 。

```java
long currentId = 1L;
long partitionId = currentId % partition;
byte[] rowkey = Bytes.add(Bytes.toBytes(partitionId),
                    Bytes.toBytes(currentId));
```

## 4.14 HBase如何批量put数据进HBase里？

数据写入Hbase这里采用Table.put(putList) 的方式 ，数据的格式为Put类型，put方法传参既可以是一个Put对象也可以是一个List<Put>集合，集合情况下为批量入库

入库机制：类似于关系型数据库，如果是一条一条入库，则region每次都会进行一次提交;

批量入库，则在数据量达到一定阈值时提交一次，在Table关闭时还会提交一次，批量入库时也可以类似与关系型数据库那样支持手动提交。

## 4.15 HBase使用场景

- 写密集型应用，每天写入量巨大，而相对读数量较小的应用

- 单条记录或者小范围的查询是可以接受的，大范围的查询由于分布式的原因，可能在性能上有点影响
-  对性能和可靠性要求非常高的应用，由于HBase本身没有单点故障，可用性非常高。
- 数据分析需求并不多



# 五 Spark

## 5.1 集群资源充足，多个任务集中在一个NM上运行

![image-20200712092501689](D:\ProgramFiles\Typora\图片备份\image-20200712092501689.png)

## 5.2 hadoop和spark的都是并行计算，那么他们有什么相同和区别

spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错。

hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。

## 5.3 Spark在Client与在集群运行的区别？

Spark可以使得多个Tasks在同一个容器(container)里面运行。注意这里和Hadoop的MapReduce作业不一样，MapReduce作业为每个Task开启不同的JVM来运行。

Client模式下Driver端运行在本地，Cluster模式下Driver运行在Executor上。

Client模式下driver不是一个子线程,而是直接运行在SparkSubmit进程的main线程中, 所以sparkSubmit进程不能退出.

Cluster模式下，由AM启动Driver在Executor上执行。

## 5.4 Spark为什么比MR快？

1、MR缓冲区

2、

​	Spark的一个Executor中可以运行多个Task，减少了重启开关容器的时间。

​	MR的一个Task需要开启一个JVM。

## 5.5 Spark性能调优做过哪些？



动态分区-非严格模式
被压
最大消费速率
优雅关闭
首次消费earliest
手动维护偏移量

### 5.5.1 最优资源配置

```xml
spark-submit
	--master yarn
	--driver-core 2
	--driver-memory 8g
	--executor-cores 4
	--num-executor 10
	--executor-memory 8g
	--class PackageName.ClassName XXXX.jar
	--name "Spark Job Name"
	--InputPath
	--OutPutPath
```

官方建议每个Executor核心数为3-5个

### 5.5.2 Shuffle参数优化

map端、shuffle端、reduce端优化3个，reduce拉取次数和等待时长2个。共5个

调节map端溢写磁盘的缓冲区大小，默认32K：spark.shuffle.file.buffer

sortShuffle阈值，默认200M：spark.shuffle.sort.bypassMergeThreshold

reduce拉取map端缓冲区大小，默认48M：spark.reducer.maxSizeInFlight
reduce拉取重试次数，默认3：spark.shuffle.io.maxRetries
reduce拉取失败等待时长，默认5S：spark.shuffle.io.retryWait

### 5.5.3 RDD优化

1）对重复使用的RDD做Cache

2）尽可能早的做过滤操作

### 5.5.4 并行度调节

首先要明白在Spark中：

​	**并行度 = Min（分区数，Cores）**

理想的并行度设置，应该是让并行度与资源相匹配，简单来说就是在资源允许的前提下，并行度要设置的尽可能大，达到可以充分利用集群资源。合理的设置并行度，可以提升整个 Spark 作业的性能和运行速度。

### 5.5.5 广播变量

### 5.5.6 Kryo序列化

### 5.5.7 调节进程本地化等待时间

Spark 作业运行过程中，Driver 会对每一个 stage 的 task 进行分配。根据 Spark 的 task 分配算法，Spark希望task能够运行在它要计算的数据所在的节点（数据本地化思想），这样就可以避免数据的网络传输。

通常来说，task可能不会被分配到它处理的数据所在的节点，因为这些节点可用的资源可能已经用尽，此时，Spark会等待一段时间，默认3s，如果等待指定时间后仍然无法在指定节点运行，那么会自动降级，尝试将task分配到比较差的本地化级别所对应的节点上，比如将task分配到离它要计算的数据比较近的一个节点，然后进行计算，如果当前级别仍然不行，那么继续降级。

本地化级别：

| ***\*名称\****          | ***\*解析\****                                               |
| ----------------------- | ------------------------------------------------------------ |
| ***\*PROCESS_LOCAL\**** | 进程本地化，task和数据在同一个Executor中，性能最好。         |
| ***\*NODE_LOCAL\****    | 节点本地化，task和数据在同一个节点中，但是task和数据不在同一个Executor中，数据需要在进程间进行传输。 |
| ***\*RACK_LOCAL\****    | 机架本地化，task和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输。 |
| ***\*NO_PREF\****       | 对于task来说，从哪里获取都一样，没有好坏之分。               |
| ***\*ANY\****           | task和数据可以在集群的任何地方，而且不在一个机架中，性能最差。 |

调优目的：尽可能的让Task在数据本地执行，减少网络间的IO传输。默认等待3S

**重点：**

在Spark项目开发阶段，可以使用client模式对程序进行测试，此时，可以在本地看到比较全的日志信息，日志信息中有明确的task数据本地化的级别，如果大部分都是PROCESS_LOCAL，那么就无需进行调节，但是如果发现很多的级别都是NODE_LOCAL、ANY，那么需要对本地化的等待时长进行调节，通过延长本地化等待时长，看看task的本地化级别有没有提升，并观察Spark作业的运行时间有没有缩短。 注意，过犹不及，不要将本地化等待时长延长地过长，导致因为大量的等待时长，使得Spark作业的运行时间反而增加了。

### 5.5.8 算子优化

#### 尽量分区算子优化

尽量使用分区算子，比如mapPartition，foreachPartition等。

#### repartition解决sparkSQL低并行度问题

Spark SQL的并行度不允许用户自己指定，Spark SQL自己会默认根据 hive 表对应的 HDFS 文件的 split 个数自动设置 Spark SQL 所在的那个 stage 的并行度，用户自己通spark.default.parallelism参数指定的并行度，只会在没Spark SQL的stage中生效。

由于Spark SQL所在stage的并行度无法手动设置，如果数据量较大，并且此stage中后续的transformation操作有着复杂的业务逻辑，而Spark SQL自动设置的task数量很少，这就意味着每个task要处理为数不少的数据量，然后还要执行非常复杂的处理逻辑，这就可能表现为第一个有 Spark SQL 的 stage 速度很慢，而后续的没有 Spark SQL 的 stage 运行速度非常快。

为了解决Spark SQL无法设置并行度和 task 数量的问题，我们可以使用repartition算子。

![image-20200712202801015](D:\ProgramFiles\Typora\图片备份\image-20200712202801015.png)

### 5.5.9 JVM调优

#### 调节Executor堆外内存

默认情况下，Executor 堆外内存上限大概为300多MB，在实际的生产环境下，对海量数据进行处理的时候，这里都会出现问题，导致Spark作业反复崩溃，无法运行，此时就会去调节这个参数，到至少1G，甚至于2G、4G。

Executor堆外内存的配置需要在spark-submit脚本里配置，

```
--conf spark.executor.memoryOverhead=2048
```

以上参数配置完成后，会避免掉某些JVM OOM的异常问题，同时，可以提升整体 Spark 作业的性能。

#### 调节拉取资源连接等待时长

在Spark作业过程中，Executor优先从自己本地关联的BlockManager中获取某份数据，如果本地的BlockManager没有的话就会从远程的Executor中获取数据，如果Task在运行过程中创建大量对象占用大量的内存，就会频繁的导致垃圾回收，垃圾回收会导致工作线程全部停止，也就是说垃圾回收一旦执行SparkExecutor就会停止工作，无法提供相应的连接此时，由于没有响应，无法建立网络连接，会导致网络连接超时。

在生产环境下，有时会遇到file not found、file lost这类错误，在这种情况下，很有可能是Executor的BlockManager在拉取数据的时候，无法建立连接，然后超过默认的连接等待时长120s后，宣告数据拉取失败，如果反复尝试都拉取不到数据，可能会导致 Spark 作业的崩溃。这种情况也可能会导致 DAGScheduler 反复提交几次 stage，TaskScheduler 返回提交几次 task，大大延长了我们的 Spark 作业的运行时间。此时，可以考虑调节连接的超时时长，连接等待时长需要在spark-submit脚本中进行设置

```
--conf spark.core.connection.ack.wait.timeout=300
```

调节连接等待时长后，通常可以避免部分的XX文件拉取失败、XX文件lost等报错。

## 5.6 Spark提交Job流程

### 5.6.1 YARN Cluster提交模式

![image-20200712171809053](D:\ProgramFiles\Typora\图片备份\image-20200712171809053.png)



执行脚本提交任务，实际是启动一个 SparkSubmit 的 JVM 进程；

SparkSubmit 类中的 main方法反射调用Client的main方法；

Client创建Yarn客户端，然后向Yarn发送执行指令：bin/java ApplicationMaster；

Yarn框架收到指令后会在指定的NM中启动ApplicationMaster；

ApplicationMaster启动Driver线程，执行用户的作业；

AM向RM注册，申请资源；

获取资源后AM向NM发送指令：bin/java CoarseGrainedExecutorBacken；

启动ExecutorBackend, 并向driver注册；

注册成功后, ExecutorBackend会创建一个Executor对象；

Driver会给ExecutorBackend分配任务, 并监控任务的执行；

注意：

- SparkSubmit、ApplicationMaster和CoarseGrainedExecutorBacken是独立的进程；

- Client和Driver是独立的线程；

> Client的作用就是创建Yarn客户端，向Yarn客户端发送指令启动AM
>
> AM启动以后在NM中启动Driver进程执行用户作业，计算资源生成有向无环图等。。。

- Executor是一个对象。

### 5.6.2 YARN Client提交模式

![image-20200712172755288](D:\ProgramFiles\Typora\图片备份\image-20200712172755288.png)

执行脚本提交任务，实际是启动一个SparkSubmit的 JVM 进程；

SparkSubmit伴生对象中的main方法反射调用用户代码的main方法；

启动Driver线程，执行用户的作业，并创建ScheduleBackend；

YarnClientSchedulerBackend向RM发送指令：bin/java ExecutorLauncher；

Yarn框架收到指令后会在指定的NM中启动ExecutorLauncher（实际上还是调用ApplicationMaster的main方法）；

AM向RM注册，申请资源；

获取资源后AM向NM发送指令：bin/java CoarseGrainedExecutorBacken；

后面和cluster模式一致

注意：

- SparkSubmit、ExecutorLauncher和CoarseGrainedExecutorBacken是独立的进程；

- driver不是一个子线程,而是直接运行在SparkSubmit进程的main线程中, 所以sparkSubmit进程不能退出.

## 5.7 Spark通用运行流程

![image-20200712184140673](D:\ProgramFiles\Typora\图片备份\image-20200712184140673.png)

上图为 Spark 通用运行流程，不论 Spark 以何种模式进行部署，都是以如下核心步骤进行工作的：

任务提交后，都会先启动 Driver 程序；

随后 Driver 向集群管理器注册应用程序；

之后集群管理器根据此任务的配置文件分配 Executor 并启动该应用程序；

当 Driver 所需的资源全部满足后，Driver 开始执行 main 函数，Spark 转换为懒执行，当执行到 Action 算子时开始反向推算，根据宽依赖进行 Stage 的划分，随后每一个 Stage 对应一个 Taskset，Taskset 中有多个Task；

根据本地化原则，Task 会被分发到指定的 Executor 去执行，在任务执行的过程中，Executor 也会不断与 Driver 进行通信，报告任务运行情况。

## 5.8 Executor怎么获取到Task？

根据本地化原则，由Driver将Task分发到指定的Executor去执行，在执行过程中Executor会不断与Driver进行通信，报告任务运行情况。

## 5.9 Spark内存管理

Spark 与 Hadoop 的重要区别之一就在于对内存的使用。

Hadoop只是将内存作为计算资源，Spark除了将内存作为计算资源以外，还将内存的一部分纳入到存储体系中，Spark使用**MemoryManager**对存储体系和计算使用的内存进行管理。

### 5.9.1 堆内内存和堆外内存规划

![image-20200712194609027](D:\ProgramFiles\Typora\图片备份\image-20200712194609027.png)

枚举类型**MemoryMode**中定义了堆内存和堆外内存:

```scala
@Private
public enum MemoryMode {
  ON_HEAP,   // 堆内内存
  OFF_HEAP   // 堆外内存
}
```

说明：

- 这里的堆内内存不能与 JVM 中的 Java 堆直接画等号, 它只是 JVM 堆内存的一部分，由 JVM 统一管理。
- 堆外内存则是 Spark 使用 sun.misc.Unsafe的 API 直接在工作节点的系统内存中开辟的空间。

### 5.9.2 堆内内存

堆内内存的大小由 Spark 应用程序启动时的-executor-memory 或 spark.executor.memory 参数配置。

Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存。

### 5.9.3 堆外内存

为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。

堆外内存意味着把内存对象分配在 Java 虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。

利用 JDK Unsafe API，Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。

堆外内存可以被精确地申请和释放（堆外内存之所以能够被精确的申请和释放，是由于内存的申请和释放不再通过JVM机制，而是直接向操作系统申请，JVM对于内存的清理是无法准确指定时间点的，因此无法实现精确的释放），而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。

在默认情况下堆外内存并不启用，可通过配置 spark.memory.offHeap.enabled 参数启用，并由 spark.memory.offHeap.size 参数设定堆外空间的大小。

除了没有 other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。

## 5.10 Driver负责哪些工作？

1）初始化SparkContext对象

2）准备运行时上下文

3）保持和AM的RPC连接，通过AM申请资源

> 当RM向AM返回容器资源时，AM就尝试在对应的Container上启动ExecutorBackend进程，进程起来后会向Driver反向注册，注册成功后保持与Driver的心跳，同时等待Driver分配任务，当任务执行完毕将任务状态上报给Driver

4）根据生成的有向无环图，将任务发送到空闲的Executor上

5）保持监听Executor的心跳

![image-20200712194222766](D:\ProgramFiles\Typora\图片备份\image-20200712194222766.png)

## 5.11 Spark做缓存时缓存在哪里？

默认是持久化到内存，通常只持久化到内存。

在哪个分区执行的RDD，就缓存到哪个分区所对应的节点内存中。

如果持久化到磁盘，位置我们无法控制，Spark会放入到一个临时目录中。

对于shuffle算子来说，Spark会自动给shuffle后的RDD添加缓存。

实际生产中，当RDD不在使用时需要手动清除缓存，释放内存。

## 5.12 Spark中distinct算子的底层本质是什么？

distinct底层本质还是使用reduceByKey来去重的。

```scala
def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    map(x => (x, null)).reduceByKey((x, y) => x, numPartitions).map(_._1)
}
```

## 5.13 Spark广播出去的信息可以修改吗？

spark 广播变量可以通过unpersist方法删除，然后重新广播。

## 5.14 Spark执行内存如何分配，执行任务时给多大内存？

执行任务时，启动10个executor，分配4个核心，每个executor分配8g内存

## 5.15 Spark被压机制具体实现原理

<img src="D:\ProgramFiles\Typora\图片备份\image-20200517211731236.png" alt="image-20200517211731236" style="zoom:67%;" />

接收器：Spark 1.5以前版本，用户如果要限制 Receiver 的数据接收速率，可以通过设置静态配制参数**spark.streaming.receiver.maxRate**的值来实现，此举虽然可以通过限制接收速率，来适配当前的处理能力，防止内存溢出，但也会引入其它问题

1、数据量很大集群处理不过来，接收器OOM

2、数据量很小集群资源浪费

最理想的情况是数据来了，当发现集群处理不过来时，接收器控制数据量来的慢一点达到平衡，这种平衡属于动态平衡，需要对速度做一个动态的调整。

> 到kafka的时候没有接收器，会有一种直连模式。

为了更好的协调数据接收速率与资源处理能力，1.5版本开始 Spark Streaming 可以动态控制数据接收速率来适配集群数据处理能力。

背压机制（即Spark Streaming Backpressure）: 根据 JobScheduler 反馈作业的执行信息来动态调整 Receiver 数据接收率。

通过属性**spark.streaming.backpressure.enabled**来控制是否启用backpressure机制，默认值**false**，即不启用。

## 5.16 Spark Streaming统计过哪些指标？如何实现的？

UV，PV，恶意登录，恶意薅羊毛，商品热度TopN，订单支付状态监控，双流Join

## 5.17 Spark BlockManager是什么？

BlockTransferService、本地数据读写解密、远程数据读写

blockmanager 用于缓存/ 存储每个 executor 上用到数据（广播变量、DFS 数据缓存）。

![image-20200713211612956](D:\ProgramFiles\Typora\图片备份\image-20200713211612956.png)

spark Broadcast 广播变量存储在哪？存储在block manager上

driver端：blockmanager master
executor端：blockmanager slave

block manager 还存储了什么数据？
 RDDBlockId（rdd 缓存，cache presist）、ShuffleBlockId、ShuffleDataBlockId、ShuffleIndexBlockId、BroadcastBlockId、TaskResultBlockId、TempLocalBlockId、TempShuffleBlockId 8 个，spark streaming 一个 ReceiverInputDStream 接受到的数据也是先放在 BlockManager 中， 然后封装为一个 BlockRdd 进行下一步运算的。

![image-20200713211804398](D:\ProgramFiles\Typora\图片备份\image-20200713211804398.png)

**block manager 结构**
 1、MemoryStore  内存写入操作
 2、DiskStore 磁盘写入操作

## 5.18 Spark block读取流程

优先从本地获取，根据StorageLevel（存储级别）设置，如果是存储在内存中，则从本地的MemoryStore中查询，存在则读取并返回；如果是存储在磁盘上，则从本地的DiskStore中查询，存在则读取并返回。

> 设计到进程本地化和节点本地化，如果当前Task要运行的executor资源已满，当前Task可能会被分配到别的节点上运行，默认等待时间3秒。

本地不存在， 则会从远程的Executor读取，向远程的Driver上的BlockManagerMasterEndpoint查询对应Block ID，有哪些Executor已经保存了该Block数据，Dirver返回一个包含了该Block数据的Location列表，如果对应的Location信息与当前ShuffleMapTask执行所在Executor在同一台节点上，则会优先使用该Location，因为同一节点上的多个Executor之间传输Block数据效率更高
 **注意：广播变量的 block 不会汇报给 Master**

## 5.19 大数据性能调优的本质

### 5.19.1 优化的本质是什么？

编程的时候发现一个惊人的规律，软件是不存在的！所有编程高手级别的人无论做什么类型的编程，最终思考的都是硬件方面的问题！最终思考都是在一秒、一毫秒、甚至一纳秒到底是如何运行的，并且基于此进行算法实现和性能调优，最后都是回到了硬件！

在大数据性能的调优，它的本质是硬件的调优！即基于 **CPU(计算)、Memory(存储)、IO-Disk/ Network(数据交互)** 基础上构建算法和性能调优！我们在计算的时候，数据肯定是存储在内存中的。磁盘IO怎么去处理和网络IO怎么去优化。

### 5.19.2 Spark性能调优要点分析

```properties
num-executors 10 
executor-memory 8g
executor-cores 4
driver-memory 8g
spark.default.parallelizm 120
spark.storage.memoryFraction 0.6
spark.shuffle.memoryFraction 0.2
```

- num-executors：该参数一定会被设置，Yarn 会按照 Driver 的申请去最终为当前的 Application 生产指定个数的 Executors，实*际生产环境下应该分配80个左右 Executors 会比较合适。*
- executor-memory：这个定义了每个 Executor 的内存，它与 JVM OOM 紧密相关，很多时候甚至决定了 Spark 运行的性能。实际生产环境下建义是 8G 左右，很多时候 Spark 运行在 Yarn 上，内存占用量不要超过 Yarn 的内存资源的 50%。
- executor-cores：决定了在 Executors 中能够并行执行的 Tasks 的个数。*实际生产环境下应该分配4个左右，一般情况下不要超过 Yarn 队列中 Cores 总数量的 50％。*
- driver-memory：默应是 1G
- spark.default.parallelizm：并行度问题，如果不设置这个参数，Spark 会跟据 HDFS 中 Block 的个数去设置这一个数量，原理是默应每个 Block 会对应一个 Task，默应*情况下，如果数据量不是太多就不可以充份利用 executor 设置的资源，就会浪费了资源。*建义设置为 100个，最好 700个左右。**Spark官方的建义是每一个 Core 负责 2-3 个 Task**。 
- spark.storage.memoryFraction：默应占用 60％，如果计算比较依赖于历史数据则可以调高该参数，当如果计算比较依赖 Shuffle 的话则需要降低该比例。
- spark.shuffle.memoryFraction：默应占用 20％，如果计算比较依赖 Shuffle 的话则需要调高该比例。

## 5.20 Spark出现OOM时如何分析？

### 5.21 Spark内存模型

Spark框架主要有三处消耗heap的地方, Spark内部将其分成2个区: Storage和Execution(Execution部分主要用于ShuffleRead和ShuffleWrite) 

1.Storage: 主要存RDD, Broadcast等. 涉及的Spark操作: persist/cache/sc.broadcast等

2.Execution: 主要用于Shuffle阶段, read shuffle/write shuffle阶段需要开buffer来做一些merge操作或者防止shuffle数据放内存原地爆炸. 一般涉及的操作: XXXXByKey(reduceByKey,combineByKey等)/coGroup/join类等。 

3.一个 Task 运行的时候，也需要占用一部分内存。

### 5.22 Driver heap OOM

- 用户在Driver端生成大对象

  > 比如创建了一个大的集合数据结构；
  >
  > 解决思路：
  >
  > 1）考虑将该大对象转化成Executor端加载. 例如调用sc.textFile/sc.hadoopFile等
  >
  > 2）如若无法避免, 自我评估该大对象占用的内存, 相应增加driver-memory的值

- 从Executor端收集数据回Driver端

  > 比如Collect. 某个Stage中Executor端发回的所有数据量不能超过spark.driver.maxResultSize，默认1g. 如果用户增加该值, 请对应增加2delta increase到Driver Memory, resultSize该值只是数据序列化之后的Size, 如果是Collect的操作会将这些数据反序列化收集, 此时真正所需内存需要膨胀2-5倍, 甚至10倍. 
  >
  > 解决思路:
  >
  > 1）本身不建议将大的数据从Executor端, collect回来. 建议将Driver端对collect回来的数据所做的操作, 转化成Executor端RDD操作
  >
  > 2）如若无法避免, 自我评估collect需要的内存, 相应增加driver-memory的值

- Spark本身框架的数据消耗

  > 现在在Spark1.6版本之后主要由Spark UI数据消耗, 取决于作业的累计Task个数
  >
  > 解决思路：
  >
  > 1）考虑缩小大numPartitions的Stage的partition个数, 例如从HDFS load的partitions一般自动计算, 但是后续**用户的操作中做了过滤等操作已经大大减少数据量, 此时可以缩小Parititions。**
  >
  > 2）通过参数spark.ui.retainedStages(默认1000)/spark.ui.retainedJobs(默认1000)控制
  >
  > 3）实在没法避免, 相应增加内存

### 5.23 Executor head OOM

- 数据相关

  > 例如用户单key对应的Values过多, 调用groupByKey/对RDD[K, V: List/Array]做集合操作
  >
  > 解决思路：
  >
  > 1）控制单key的Value的个数, 做个数截断或者做过滤. 很多情况是用户自身有异常数据导致。
  >
  > 2）考虑对业务逻辑的RDD操作, 考虑其他方式的RDD实现, 避免统一处理所有的Values. 比如	  对Key做且分,类似keyA_1, keyA_2操作。
  >
  > 3）降低spark.memory.fraction的值, 以此提高用户可用的内存空间. 注意spark.memory.fraction的至少保证在0.1. 降低该值会影响Spark的执行效率, 酌情减少。
  >
  > 4）增加 Exeutor-memory
  >
  
- 用户代码

  > 在RDD操作里创建了不容易释放的大对象, 例如集合操作中产生不易释放的对象。
  >
  > 解决思路：
  >
  > 1）优化逻辑. 避免在一个RDD操作中实现大量集合操作, 可以尝试转化成多个RDD操作。
  >
  > 2）降低spark.memory.fraction的值, 以此提高用户可用的内存空间. 注意spark.memory.fraction的至少保证在0.1, 降低该值会影响Spark的执行效率, 酌情减少。
  >
  > 3）增加Executor-memory

# 六 ES

## 6.1 大规模数据如何检索？

### 6.1.1 需要解决的问题

当系统数据量上了10亿，100亿条时，做系统架构需要从哪些角度考虑问题？

- 用什么数据库好？(关系型数据库：MySQL 非关系型数据库：HBase、redis)
- 如何解决单点故障？
- 如何保证数据安全性
- 如何解决索引难题
- 如何解决统计分析问题

### 6.1.2 传统数据库应对解决方案

对于关系型数据，我们通常采用以下或类似架构去解决查询瓶颈和写入瓶颈： 
解决要点： 
1）通过主从备份解决数据安全性问题； 
2）通过数据库代理中间件心跳监测，解决单点故障问题； 
3）通过代理中间件将查询语句分发到各个slave节点进行查询，并汇总结果 

### 6.1.3 非关系型数据库解决方案

1）通过副本备份保证数据安全性； 
2）通过节点竞选机制解决单点问题； 
3）先从配置库检索分片信息，然后将请求分发到各个节点，最后由路由节点合并汇总结果 

## 6.2 ES核心概念

### 1）Cluster：集群

ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。

### 2）Node：节点

形成集群的每个服务器称为节点。

### 3）Shard：分片

当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。 
当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。

### 4）Replia：副本

为提高查询吞吐量或实现高可用性，可以使用分片副本。 
副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。 
当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。

### 5）全文检索

全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。 
全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。

## 6.3 ELK是什么？

ELK=elasticsearch+Logstash+kibana 
elasticsearch：后台分布式存储以及全文检索 
logstash: 日志加工、“搬运工” 
kibana：数据可视化展示。 
ELK架构为数据分布式存储、可视化查询和日志解析创建了一个功能强大的管理链。 三者相互配合，取长补短，共同完成分布式大数据处理工作。



# 七 数仓

## 7.1 数仓质量监控

### Apache Griffin(Ebay开源数据质量监控平台)

![image-20200712161955535](D:\ProgramFiles\Typora\图片备份\image-20200712161955535.png)

(1)从准确性、完整性、时效性、唯一性等多个维度进行监控

(2)计算结果存储至ES、HDFS

(3)计算结果metrics展示

(4)支持实时和离线

(5)优势：开源

### metrics展示

![image-20200712162016133](D:\ProgramFiles\Typora\图片备份\image-20200712162016133.png)

## 7.2 业务需求要增加一个维度，需要做什么工作？

 数据仓库最常碰到的扩展是给一个已经存在的维度表和事实表添加列。本篇先讨论如果需要增加列，模式会发生怎样的变化。

### 7.2.1 修改数据库模式

使用SQL脚本修改维度表和事实表，增加新列。

```sql
USE dw;

ALTER TABLE customer_dim
  ADD shipping_address VARCHAR (50) AFTER customer_state
, ADD shipping_zip_code INT (5) AFTER shipping_address
, ADD shipping_city VARCHAR (30) AFTER shipping_zip_code
...
```

### 7.2.2 修改定期装载脚本

修改数据库模式后，还要修改已经使用的定期装载脚本。

## 7.3 数仓的发展

### 阶段一：以解决数据孤岛的需求

1）传统数据库太分散，久而久之随着业务数据越来越多形成了一个个的数据孤岛，于是有了数据仓库

### 阶段二：更好的管理和架构数据存储

1）由于数仓中内容越发庞杂，以此为背景逐步发展出许多数仓架构的模型如：雪花、星型、星座等
2）基于这些模型理论，数仓中的一些行为又被梳理成对应的概念：维度表，事实表，指标等

### 阶段三：更好的数据处理流程

1）在对数据的存储架构进行了梳理后，一个新的问题出现了，随着分析的进行产生了太多的中间结果表、最终结果表、临时表、转换表等流程产物。
2）这些东西挤在数据库中也很多很杂，基于这样的背景，数仓分层的架构被提出。

分层架构指出，要基于业务的特性和数据处理流程的特点，针对性的对数仓进行分层处理，根据层级来确定数据处理的方向。这样方便:
清哳数据结构: .
每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。
数据血缘追踪:
简单来说，我们最终给业务呈现的是一个能直接使用的业务表，但是它的来源有很多，如果有一张来源表出问题了 ，我们希望能够快速准确地定位到问题，并清楚它的危害范围。
减少重复开发: 
规范数据分层，开发一些通用的中间层数据，能够极大的减少重复计算。
把复杂问题简单化:
将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。
屏蔽原始数据的异常：
屏蔽业务的影响，不必改一次业务就需要重新接入数据。

![image-20200712211328244](D:\ProgramFiles\Typora\图片备份\image-20200712211328244.png)

## 7.4 数仓版本选型

![image-20200712213556685](D:\ProgramFiles\Typora\图片备份\image-20200712213556685.png)

## 7.5 你们公司日活、月活、留存多少？

2019年时公司**最高留存已经达到了55%-32%-16%（次日，7日，30日）**，日活这方面在采用了一些推广手段后以每日6000-10000的规模增长（离开公司之前累计用户已达100万，日活23万，月活100W），今年因为疫情原因每日流失的用户也在这个规模。

每天日活跃用户23万，每人每天平均100条，23万 × 100条 = 2300万条

每条日志1.5KB左右，每天680万条，一亿/1024/1024 = 约30G

## 7.6 数仓分层架构存储策略

1）数仓ODS层

采用LZO压缩存储：30G数据可以压缩到 6G左右

2）数仓DWD层

采用LZO + parquet存储：6G左右

3）数仓DWS层

轻度聚合存储（为了快速运算，不压缩）：15G左右

4）数仓DWT层

聚合宽表，方便ADS层的指标统计，数据量很小

5）数仓ADS层

对各个主题的指标进行分析的结果，数据量很小

# 八 Scala

## 8.1 说说Scala伴生对象

当一个单例对象和某个类共享一个名称时，这个单例对象称为***伴生对象***。同理，这个类被称为是这个单例对象的伴生类。类和它的伴生对象可以互相访问其私有成员。

> 通过伴生类和伴生对象实现静态工厂类，使用该工厂类去创建对象，屏蔽了创建对象的方法。
>
> 通过构造器私有化，就可以避免外界创建对象

## 8.2 说说Scala模式匹配

模式匹配中采用match关键字声明，每个分支采用case关键字声明，需要匹配时从第一个cese分支开始，匹配成功则执行对应的代码逻辑，匹配不成功则继续执行下一个分支继续判断，如果所有的case都不匹配，会执行case_分支，类似于Java中的default语句。

### 类型匹配

```scala
  def main(args: Array[String]): Unit = {
    //类型匹配
    val arr: Any = Array(1.3, 2, 3)
    val list: Any = List(10, 1)
    val b: Any = false
    list match {
      case a: Array[Int] =>
        println("Array[int]")
      case a: Array[Double] =>
      // 只有数据可以匹配模式判断类型，因为scala中数组底层是java的数组
      //创建java数组 new int[]()：在创建时类型是确定的
      case a: List[Double] =>
        println("list[_]")
      case a: Boolean =>
        println("Boolean")
    }
  }
```



### 内容匹配

```scala
    //内容匹配 匹配嵌套元组
    val t1 = (1, (2, (3, (5, (5, 6)))))
    t1 match {
      case (_, (_, (a, (_, (c, _))))) => println(s"a=$a,c=$c")
    }
```

### 对象匹配

```scala
 object sqrt {
    //对象匹配时，在case语句中会自动调用对象的unapply方法
    //如果返回值为Some类型则执行
    //如果返回值为None则跳过此case匹配下一个case
    //对相匹配模式的返回值类型必须为Option
    //这个i是match传进来的参数
    def unapply(i: Double): Option[Double] = if (i > 0) Some(math.sqrt(i)) else None
  }

  def main(args: Array[String]): Unit = {
    val n:Double = -9
    //需求：通过模式匹配计算出9的平方根
    n match {
        //这个n是unapply方法的返回值
      case sqrt(n) => println(n)
      case _=>println("参数异常")
    }
  }

```

### 常量（变量）匹配

```scala
//首字母小写表示变量
case a =>
//首字母大写表示常量
case A =>
```

## 8.3 说说Scala隐式转换

一个从类型 S 到类型 T 的隐式转换由一个函数类型 S => T 的隐式值来定义，或者由一个可转换成所需值的隐式方法来定义。

当我们需要某个类中的一个方法，但是这个类没有提供这样的一个方法，就会找隐式转换看看能不能解决，如果还不能就会报错。

隐式类（其实是对隐式转换函数的一种简化）在scala2.1.x才出现

使用implicit声明隐式转换函数

## 8.4 case class和class的区别

样例类仍然是类，和普通类相比，只是其自动生成了伴生对象，并且伴生对象中自动提供了一些常用的方法，如***apply***、***unapply***、***toString***、equals、hashCode和copy。



# 九 Git

## 9.1 Git常用命令

**git init：** 

 新建一个本地仓库，初始化本地库也就是一个文件夹

**git status：**

查看本地库的状态(git status -s 简化输出结果)

**git add [file]：**  

多功能命令: 1. 开始跟踪新文件 2. 把已跟踪的文件添加到暂存区 3. 合并时把有冲突的文件标记为已解决状态，add后不指定文件默认将所有文件添加到暂存区

**git commit –m “xxx” [file]：**

将暂存区的文件提交到本地库,-m 后面为修改的说明，commit后不加文件名默认将暂存区的文件全部提交

**git reflog：**

查看所有操作的历史记录，显示所有的记录包含提交记录和版本回退，前进记录。

**git reset --hard [具体版本号，例如：1f9a527等]：**

 回到（回退和前进都行）指定版本号的版本

## 9.2 Git如何切换分支？

git checkout [分支名]：

切换分支

git branch -v：

查看分支

git branch [分支名]：

创建分支

# 十 数据结构与算法

## 10.1 简单描述一下快速排序的思想

**1）什么是快速排序？**      

 快速排序算法是对冒泡排序的一种改进，通过多次比较和交换来实现排序。     

**2）快速排序的核心思想是什么？**      

通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以用递归进行。     **3）快速排序的基本思路**      

① 首先设定一个分界值，通过该分界值将数组分成左右两个部分。       

② 将大于或等于分界值的数据集中到数组右边，小于分界值的数据集中到数组的左边。此时，左边部分中各元素都小于或等于分界值，而右边部分中各元素都大于或等于分界值。     

③ 然后左边和右边的数据可以再进行独立排序，这里使用到了递归思想。对于左侧的数据，又可以取一个分界值，将该部分数据分成左右两部分，同理在左边放置较小值，右边放置较大值。以此类推右侧的数据也可以做同样的处理。       

④重复上述过程，可以看出这是一个递归定义。通过递归将左侧部分排好序后，再递归排好右侧部分的顺序。当左、由两个部分各数据排序完成后整个数组的排序也就完成了。    

# 十一 Java

## 11.1 JVM三大性能调优参数

-Xss规定了每个线程堆栈的大小。一般情况下256K是足够了。影响了此进程中并发线程数大小。

-Xms初始的Heap的大小。

-Xmx最大Heap的大小。

在很多情况下，-Xms和-Xmx设置成一样的。这么设置，是因为当Heap不够用时，会发生内存抖动，影响程序运行稳定性。

## 11.2 String类可以被继承吗？为什么？

![image-20200713105716048](D:\ProgramFiles\Typora\图片备份\image-20200713105716048.png)

当用final修饰一个类时，表明这个类不能被继承。final类中的成员变量可以根据需要设为final，但是要注意final类中的所有成员方法都会被隐式地指定为final方法。

## 11.3 使用线程池有什么好处？

- **降低资源消耗**：通过重复利用已创建的线程降低线程创建和销毁造成的消耗。
- **提高响应速度**：当任务到达时，可以不需要等待线程创建就能立即执行。
- **提高线程的可管理性**：线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，监控和调优。

## 11.4 Java集合的框架体系

### 11.4.1 单列集合

#### 1）单列集合基本介绍

![image-20200715171401247](D:\ProgramFiles\Typora\图片备份\image-20200715171401247.png)

1）单列集合，可以理解成就是直接放入单个数据，比如说ArrayList

2）单列集合的根接口是Collection

3）Collection接口没有直接的实现子类，而是由两个子接口List和Set

4）List子接口的常用实现类有：LinkedList，ArrayList（线程不安全的），Vector(线程安全的)

5）Set子接口常用实现类有：HashSet，TreeSet

#### 2）单列集合遍历方式

##### 使用Iterator迭代器遍历

##### 使用for循环增强遍历

#### 3）单列集合之List基本介绍

List集合类中元素有序(即添加顺序和取出顺序一致)、且可重复

List集合中的每个元素都有其对应的顺序索引，即支持索引。

List容器中的元素都对应一个整数型的序号记载其在容器中的位置，可以根据序号存取容器中的元素。

List接口的实现类有：常用的有：ArrayList、LinkedList和Vector。

#### 4）ArrayList底层操作机制源码

- ArrayList中维护了一个Object类型的数组elementData

  >```java
  >    public ArrayList(int initialCapacity) {
  >        if (initialCapacity > 0) {
  >            this.elementData = new Object[initialCapacity];
  >        } else if (initialCapacity == 0) {
  >            this.elementData = EMPTY_ELEMENTDATA;
  >        } else {
  >            throw new IllegalArgumentException("Illegal Capacity: "+
  >                                               initialCapacity);
  >        }
  >    }
  >```

- 当创建对象时，如果使用的是无参构造器，则初始elementData容量为0

- 如果使用的是指定容量capacity的构造器，则初始elementData容量为capacity

- 当添加元素时：先判断是否需要扩容，如果需要扩容，则调用grow方法，否则直接添加元素到合适位置

- 如果使用的是无参构造器，如果第一次添加，需要扩容的话，则扩容elementData为10，如果需要再次扩容的话，则扩容elementData为1.5倍。

- 如果使用的是指定容量capacity的构造器，如果需要扩容，则直接扩容elementData为1.5倍。

#### 5）Vector和ArrayList的比较

![image-20200715201709138](D:\ProgramFiles\Typora\图片备份\image-20200715201709138.png)

#### 6）ArrayList和LinkedList的比较

![image-20200715201843931](D:\ProgramFiles\Typora\图片备份\image-20200715201843931.png)

1) 如果我们改查的操作多，选择ArrayList

2) 如果我们增删的操作多，选择LinkedList 

3) 一般来说，在程序中，80%-90%都是查询，因此大部分情况下会选择ArrayList

4) 在一个项目中，根据业务灵活选择，也可能这样，一个模块使用的是ArrayList,另外一个模块是LinkedList.

#### 7）HashSet添加元素底层实现

![image-20200715202256960](D:\ProgramFiles\Typora\图片备份\image-20200715202256960.png)

##### 注意事项

- 如果添加的对象(元素)相同，则hashCode一定相同

- hashCode相同，对象(元素)不一定相同 概率很小

- 判断对象(元素)是否相同，使用的是equals方法。如果我们希望只要对象的属性值相同，就认为是同一个对象，那么需要重写hashCode和equals方法。

- 当添加不同的对象，进行hasCode后，得到hash值，理论上说，不同的对象对应不同的hash值。

- 不同的hash值，求索引后，索引是有可能相同的。

- HashSet本质是一个HashMap

  ```java
      public HashSet() {
          map = new HashMap<>();
      }
  ```

  

#### 8）TreeSet的基本介绍

- TreeSet的本质是一个TreeMap

```java
TreeMap的底层是红黑树，红黑树在添加元素时，会根据 Comparable 接口 的 CompareTo 方法来决定 比较大小    public TreeSet() {
        this(new TreeMap<E,Object>());
    }
```

- TreeMap 底层是A Red-Black tree(红黑树(**二叉树一种**))的实现 , 只要底层是树结构，就是有序
- TreeSet元素默认是**自然排序**，也可以通过 **Comparator** 来指定排序规则
- TreeSet不能添加null元素，否则抛出NullPointExeception异常。
- 添加到TreeSet的对象，需要实现 **Comparable** 接口，原因是TreeSet的底层是TreeMap
- TreeMap的底层是红黑树，红黑树在添加元素时，会根据 Comparable 接口 的 CompareTo 方法来决定 比较大小。
- 自然排序就是传入TreeSet的对象自带比较功能，定制排序时在创建TreeSet时在构造器中传入Comparator实现类对象来实现定制排序，定制排序的优先级比自然排序的优先级高。

#### 9）LinkedHashSet基本介绍

- LinkedHashSet 是 **HashSet** 的子类
- LinkedHashSet 根据元素的 **hashCode 值来决定元素的存储位置**，但它同时使用链表维护元素的次序，这使得元素看起来是以插入顺序保存的。
- 底层维护了一个hash 表+ 双向链表

### 11.4.2 双列集合

#### 1）双列集合基本介绍

![image-20200715171716453](D:\ProgramFiles\Typora\图片备份\image-20200715171716453.png)

1）Map是存放双列集合的，比如key-value

2）Map有直接的实现子类，常见的有：HashMap，TreeMap，HashTable，Properties

3）Map 中的key用Set来存放，不允许重复， value 可以重复。

4）Map 的key 可以为 null, value 也可以为null 

#### 2）Map体系继承图

![image-20200715205632979](D:\ProgramFiles\Typora\图片备份\image-20200715205632979.png)

1) HashMap和Hashtable 实行了Map接口,使用虚线表示

2) Properties 是Hashtable的子类

3) LinkedHashMap 是HashMap的子类

4) SortedMap是一个Map子接口

5) TreeMap 是SortedMap的实现子类，是可以进行排序处理.

#### 3）HashMap底层结构和扩容机制

- HashMap底层维护了Node类型的数组table，默认为null
- 当创建对象时，将加载因子(loadfactor)初始化为0.75
- 当添加元素时（调用putVal方法），需要通过该元素的哈希值获取在table中的索引。然后判断该索引处是否有元素，如果没有元素直接添加。如果该索引处有元素，需要继续判断是否相等(equals)，如果相等，则直接覆盖；如果不相等需要判断是树结构还是链表结构，做出相应处理。如果添加时发现容量不够，则需要扩容。
- 如果第一次添加，则需要扩容table容量为16，临界值(threshold)为12.[即使用到12个空间，就要扩容, 公式=table容量* loadfactor = 16 * 0.75 = 12 ]
- 如果其他次添加，则需要扩容table容量为原来的2倍，临界值为原来的2倍[公式=table容量* loadfactor = 16 * 0.75 = 12]

#### 4）TreeMap的基本介绍

- TreeSet 的底层其实就是TreeMap, 因此TreeMap的使用和特点基本和TreeSet一样的
- 底层结构：红黑树，里面可以实现对键进行自然排序或定制排序。
- 使用方式1-自然排序，将key的元素类型实现Comparable接口，并且里面实现compareTo方法。
- 使用方式2-定制排序，创建TreeMap对象时，通过构造器传入一个比较器Comparator接口，实现compare方法。
- 对于TreeMap, 如果key元素实现了 Comparable接口, 同时TreeMap创建时，又通过构造器传入一个比较器Comparator接口，那么Comparator 的优先级高。

#### 5）HashTable和HashMap的对比

![image-20200715214403988](D:\ProgramFiles\Typora\图片备份\image-20200715214403988.png)



### 11.4.3 开发中如何选择集合实现类

![image-20200715214513933](D:\ProgramFiles\Typora\图片备份\image-20200715214513933.png)







## 11.5 HashMap和HashTable的区别？

### Map的实现类

#### HashMap

是Map的主要实现类，HashMap是线程不安全的，Key和Value可以是null

##### LinkedHashMap

LinkedHashMap是HashMap的子类。

可以按照元素添加的顺序进行遍历，因为底层维护了一张链表用来记录元素添加的顺序。

LinkedHashMap底层实现和HashMap底层实现一样。

#### HashTable

是一个古老的实现类，HashTable是线程安全的，HashTable的key和value不可以是null。

##### Properties

Properties的key和value都是String，经常用来读取配置文件中的内容。

#### TreeMap

TreeMap可以对key中指定的属性进行排序，注意：只能对key排序。





# 十二 Kafka

## 12.1 kafka 有几个 broker？几个 topic？几个分区？几个副本？

**broker：**

3个，kafka集群数量

**topic：**

​	离线阶段

​	用户行为日志一个主题，启动日志一个主题，	

​	实时分析

​	canal监控业务数据，增量日志一个主题

一共三个主题，如果涉及到实时数仓的话还需要创建ODS层主题和DWD层主题。

**分区：**

3个，分区代表着消费能力，通过分区可以动态增加消费速率

**副本：**

2个，容灾备份







# 十三 Redis

## 13.1 缓存雪崩

### 什么是缓存雪崩？

数据未加载到缓存中，或者缓存同一时间大面积的失效，从而导致所有请求都去查数据库，导致数据库CPU和内存负载过高，甚至宕机。

我们都知道Redis不可能把所有的数据都缓存起来(内存昂贵且有限)，**所以Redis需要对数据设置过期时间**，并采用的是**惰性删除+定期删除**两种策略对过期键删除。Redis对过期键的策略+持久化

如果**缓存数据设置的过期时间是相同的**，并且Redis恰好将这部分数据全部删光了。这就会导致在这段时间内，这些缓存**同时失效**，全部请求到数据库中。

**这就是缓存雪崩：**
Redis挂掉了，请求全部走数据库。
对缓存数据设置相同的过期时间，导致某段时间内缓存失效，请求全部走数据库。
缓存雪崩如果发生了，很可能就把我们的数据库搞垮，导致整个服务瘫痪！

**比如一个雪崩的简单过程：**

1、redis集群大面积故障

2、缓存失效，但依然大量请求访问缓存服务redis

3、redis大量失效后，大量请求转向到mysql数据库

4、mysql的调用量暴增，很快就扛不住了，甚至直接宕机

5、由于大量的应用服务依赖mysql和redis的服务，这个时候很快会演变成各服务器集群的雪崩，最后网站彻底崩溃。
![image-20200712225858798](D:\ProgramFiles\Typora\图片备份\image-20200712225858798.png)

### 如何预防缓存雪崩？

#### 1.缓存的高可用性

缓存层设计成高可用，防止缓存大面积故障。即使个别节点、个别机器、甚至是机房宕掉，依然可以提供服务，例如 Redis Sentinel 和 Redis Cluster 都实现了高可用。

(**主从架构+Sentinel（哨兵） 或者Redis Cluster（集群）)，尽量避免Redis挂掉这种情况发生**。

#### 2.缓存降级（关闭一部分非必要应用）

可以利用ehcache等本地缓存(暂时支持)，但主要还是对源服务访问进行限流、资源隔离（熔断）、降级等。

当访问量剧增、服务出现问题仍然需要保证服务还是可用的。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级，这里会涉及到运维的配合。

降级的最终目的是保证核心服务可用，即使是有损的。

比如推荐服务中，很多都是个性化的需求，假如个性化需求不能提供服务了，可以降级补充热点数据，不至于造成前端页面是个大空白。

在进行降级之前要对系统进行梳理，比如：哪些业务是核心(必须保证)，哪些业务可以容许暂时不提供服务(利用静态页面替换)等，以及配合服务器核心指标，来后设置整体预案，比如：

（1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；

（2）警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警；

（3）错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；

（4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。

设置**本地缓存**(ehcache)+**限流**(hystrix)，尽量避免我们的数据库被干掉(起码能保证我们的服务还是能正常工作的)

#### 3.Redis备份和快速预热

1)Redis数据备份和恢复

> **redis持久化，重启后自动从磁盘上加载数据，快速恢复缓存数据**。

2)快速缓存预热

#### 4.给过期时间加随机值

在缓存的时候给**过期时间加上一个随机值**，这样就会大幅度的减少缓存在同一时间过期。

#### 5.提前演练

## 13.2 缓存穿透

### 什么是缓存穿透？

缓存穿透是指查询一个一不存在的数据。例如：从缓存redis没有命中，需要从mysql数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。

### 解决思路

如果查询数据库也为空，直接设置一个默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库。设置一个过期时间或者当有值的时候将缓存中的值替换掉即可。

可以给key设置一些格式规则，然后查询之前先过滤掉不符合规则的Key。

## 13.3 缓存并发

### 什么是缓存并发？

这里的并发指的是多个redis的client同时set key引起的并发问题。其实redis自身就是单线程操作，多个client并发操作，按照先到先执行的原则，先到的先执行，其余的阻塞。当然，另外的解决方案是把redis.set操作放在队列中使其串行化，必须的一个一个执行。

将删除缓存、修改数据库、读取缓存等的操作积压到队列里边，实现串行化。

## 13.4 缓存预热

### 什么是缓存预热？

缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。

这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！

解决思路：

1、直接写个缓存刷新页面，上线时手工操作下；

2、数据量不大，可以在项目启动的时候自动进行加载；

目的就是在系统上线前，将数据加载到缓存中。

## 13.5 redis单线程为什么效率很高

### 13.5.1 完全基于内存

绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)；

### 13.5.2 单线程设计

采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；

### 13.5.3 使用多路I/O复用模型，非阻塞IO

**这里“多路”指的是多个网络连接，“复用”指的是复用同一个线程。**采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络 IO 的时间消耗），且 Redis 在内存中操作数据的速度非常快，也就是说内存内的操作不会成为影响Redis性能的瓶颈，主要由以上几点造就了 Redis 具有很高的吞吐量。




# 十四 扩充

## 14.1 机械硬盘每秒读取次数多少？

读取数据的时候寻址不超过10ms，1s/10ms差不多100多次。

固态硬盘采用闪存作为存储介质，bai读取速度相对机械硬盘du更快。固态硬盘不用磁头zhi，寻道时间几乎为0。持续写入的速度非常惊人，现在的固态硬盘厂商大多会宣称自家的固态硬盘持续读写速度超过了500MB/s云云，这相对机械硬盘的100MB/s的速度着实是相当可观的。
　　固态硬盘的快绝不仅仅体现在持续读写上，随机读写速度快才是固态硬盘的终极奥义，这最直接体现在绝大部分的日常操作中。与之相关的还有极低的存取时间，目前机械硬盘最快也要14毫秒左右，而固态硬盘可以轻易达到0.1毫秒甚至更低。

## 14.4 讲一讲什么是CAP法则？Zookeeper符合了这个法则的哪两个？

CAP法则：强一致性、高可用性、分区容错性；

Zookeeper符合强一致性、高可用性！



## 14.5 MySQL索引使用什么实现的？

### 14.5.1 什么是MySQL的索引？

索引（Index）是帮助 MySQL 高效获取数据的数据结构。

>  MySQL索引采用的数据结构有哪些？
>
> Hash表 平衡二叉树 B树 B+树

### 14.2 Hash表

哈希表（Hash table，也叫散列表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做**散列函数**，存放记录的数组叫做散列表。

优点: 查找可以直接根据key访问
缺点: 不能进行范围查找
index=Hash(key)

> 比如 通过name 去做hash值
>
> 3 = Hash("jack")
>
> 4= Hash("Mack")
>
> 存放的是存放到数组中
>
> 然后 查询时候
>
> select * from table where name = 'jack'; 先计算出hash值 然后根据下标的位置就可以找到了
>
> 效率高，但是不能做范围查询 （ hash是个散列的 不能有范围！ 计算出hash值去比较的 如果不是name 是数字的话 hash值散列 没法获取正确的范围）

### 14.3 平衡二叉树

平衡二叉查找树，又称 AVL树。 它除了具备二叉查找树的基本特征之外，还具有一个非常重要的特点：它 的左子树和右子树都是平衡二叉树，且左子树和右子树的深度之差的绝对值（平衡因子 ） 不超过1。 也就是说AVL树每个节点的平衡因子只可能是-1、0和1（左子树高度减去右子树高度）。

![image-20200715222804453](D:\ProgramFiles\Typora\图片备份\image-20200715222804453.png)

 优点: 平衡二叉树算法基本与二叉树查询相同，效率比较高
 缺点: 插入操作需要旋转，支持范围查询

 通过平衡二叉树建立id的索引，

 平衡二叉树，会取一个中间值，中间值左边成为左子树，中间值右边称为右子树

 左子树值<中间值<右子树值

 如果利用平衡二叉树建立索引的话会生成一个文件

 如果有一百万条数据 ，则会有一百万个节点。

 查询时候，索引文件加载到内存，然后进行比较。这样容易内存溢出

 一般在做的时候：

![image-20200715225911865](D:\ProgramFiles\Typora\图片备份\image-20200715225911865.png)

**假设查询10 （需要经历4次IO操作）**

**1次 从硬盘中读取4 （内存），判断下10>4，取右指针**

**2次 从硬盘中读取8 （内存），判断下10>8，取右指针**

**3次 从硬盘中读取9 （内存），判断下10>，取右指针**

**4次 从硬盘中读取10 （内存），判断下10=10，定位到数据**

查询四次！

**平衡二叉树 查询效率还可以，缺点：虽然支持范围查询，但是回旋查询效率低。**

规律：如果树的高度越高，那么查询IO次数会越多。

### 14.4 B树

维基百科对B树的定义为“在计算机科学中，B树（B-tree）是一种树状数据结构，它能够存储数据、对其进行排序并允许以O(log n)的时间复杂度运行进行查找、顺序读取、插入和删除的数据结构。B树，概括来说是一个节点可以拥有多于2个子节点的二叉查找树。与自平衡二叉查找树不同，B-树为系统最优化大块数据的读和写操作。B-tree算法减少定位记录时所经历的中间过程，从而加快存取速度。普遍运用在数据库和文件系统。”
因为B树节点元素比平衡二叉树要多，所以B树数据结构相比平衡二叉树数据结构实现减少磁盘IO的操作。

B树对于平衡二叉树做改进

平衡二叉树高度越高 查询IO次数越多

规律： 如果树的高度越高，查询次数越多。如何减少IO次数？

B树在平衡二叉树中，减少树的高度减少IO次数，明显高度低了

![image-20200715230705238](D:\ProgramFiles\Typora\图片备份\image-20200715230705238.png)

### 14.5 B+ 树

B+树相比B树，继承了B数的特征。**新增叶子节点与非叶子节点关系**，叶子节点中包含了key和value，非叶子节点中只是包含了key，不包含value（等价于非叶子节点中不存放数据）。
所有相邻的叶子节点，**使用链表进行结合**，有一定顺序排序，**从而范围查询效率非常高**。

![image-20200715231059328](D:\ProgramFiles\Typora\图片备份\image-20200715231059328.png)

通过非叶子节点查询叶子节点获取对应的value。范围查询的效率提高！

缺点： 因为有冗余的节点数据，会比较占硬盘。

结合索引：

key就是建立索引的值

value是地址 

B+树 解决范围查询问题、减少IO查询的操作

### 14.6 数据库之B树和B+树

 MyISAM和InnoDB对B-Tree索引不同的实现方式 主键索引： MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。

下图是MyISAM主键索引的

这里设表一共有三列，假设我们以Col1为主键，图myisam1是一个MyISAM表的主索引（Primary key）示意。可以看出：**叶子节点的key就是主键id  value定位到数据库中哪一行的数据的地址**

![image-20200715231525668](D:\ProgramFiles\Typora\图片备份\image-20200715231525668.png)

**区别：** 

 InnoDB直接存放的数据不是地址是数据，它的value直接就是那一行的数据（但是有大小限制的不可能存放无限制的）！

 而MyISam是地址，再通过地址查询行数。

MyISAM和InnoDB对B-Tree索引不同的实现方式，InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。
MyISAM**索引文件和数据文件是分离的**，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。

![image-20200715231853177](D:\ProgramFiles\Typora\图片备份\image-20200715231853177.png)

**索引文件最终是存放在硬盘上的，不是内存。有IO操作的**

 B+树相比B树，新增叶子节点与非叶子节点关系，叶子节点中包含了key和value，非叶子节点中只是包含了key，不包含value。

所有相邻的叶子节点，使用链表进行结合，有一定顺序排序，从而范围查询效率非常高

注意：MyISAM和InnoDB对B-Tree索引不同的实现方式
**MyISAM底层使用B+树 叶子节点的value对应存放行数的地址**，在通过行数定位到数据。
**InnoDB底层使用B+树，叶子节点的value对应存放是行的data数据**，相比MyISAM效率要高一些，但是比较占硬盘内存大小。

最终形成了一个索引文件，是存放在硬盘的。每次查询时候都是在读硬盘。

选择B+树的原因:

B+树索引具有范围查找和前缀查找的能力，相当于二分查找。

Hash索引只能支持等于查询，无法支持范围查询

## 14.6 Mysql索引失效

### 14.6.1 索引为什么会失效？注意那些事项？

- 索引无法存储null值，where num is null 全表扫描
- 如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)
  要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
- 对于多列索引，不是使用的第一部分，则不会使用索引
- like查询以%开头
- 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引
- 如果mysql估计使用全表扫描要比使用索引快,则不使用索引
- 避免where字句中 ！= 或 <> 
- in 和 not in 也要慎用 

## 14.7 MySQL为什么用B+树而不是B树？

### 14.7.1 B+树的IO次数少

B+树的数据都集中在叶子节点。分支节点只负责索引。B树的分支节点也有数据。B+树的层高会小于B树，平均的Io次数会远小于B树

### 14.7.2 B+树双向链表更适合范围查询

**B+树更擅长范围查询。存储在叶子节点中的数据是按顺序放置的双向链表。而B树范围查询只能中序遍历**。

### 14.7.3 索引节点没有数据 占用空间比较小

B+树的中间节点没有数据，同样大小的磁盘也可以容纳更多的节点元素。

同时数据量相同的情况下，B+树的结构会更加矮胖，查询时IO次数也会更少。

### 14.7.4 B+树更稳定

B+树的查询必须查找到最终的叶子节点，而B树只要找到匹配元素即可，无论匹配元素处于中间节点还是叶子节点。

因此B树的查找性能并不稳定，最好情况是只查根节点，最坏情况是查到叶子节点。

而B+树的每一次查找都是稳定的。

## 14.8 MySQL查询优化

开启慢查询日志，可以让MySQL记录下查询超过指定时间的语句，通过定位分析性能的瓶颈，才能更好的优化数据库系统的性能。
先捕获低效SQL→慢查询优化方案→慢查询优化原则 

## 14.9 分库分表为什么能够提高数据查询效率？

因为会将一张表的数据拆分成多个n张表进行存放，然后在使用第三方中间件（MyCat或者Sharding-JDBC）并行同时查询，然后在交给第三方中间进行组合返回给客户端。

同时操作，就跟多线程一样。

## 14.10 复合索引

### 14.10.1 基本介绍

select * from users where area=’beijing’ and age=22; 

在area和age上分别创建单个索引的话，由于mysql查询每次只能使用一个索引，这样已经相对不做索引时全表扫描提高了效率。但是如果在area、age两列上创建复合索引的话将带来更高的效率。如果我们创建了(area, age, salary)的复合索引，那么其实相当于创建了(area,age,salary)、(area,age)、(area)三个索引，这被称为**最佳左前缀特性**。因此我们在创建复合索引时应该将最常用作限制条件的列放在最左边，依次递减。

注1：在mysql中执行查询时，只能使用一个索引，如果我们在lname,fname,age上分别建索引,执行查询时，只能使用一个索引，mysql会选择一个最严格(获得结果集记录数最少)的索引。

注2：在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。

### 14.10.2 MySQL 最左前缀匹配原则（即创建联合索引的使用）

在mysql建立联合索引时会遵循最左前缀匹配的原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。

- 选择性高的放在最左边
- 把长度小的列放在左侧(单页可以容纳的数据更多，减少io)
- **使用最频繁的放在左侧**

使用联合索引的好处

- **减少开销**。每多一个索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开销！
- **效率高**。索引列越多，通过索引筛选出的数据越少。

## 14.11MySQL索引分类

```
1.普通索引index :加速查找
2.唯一索引
    主键索引：primary key ：加速查找+约束（不为空且唯一）
    唯一索引：unique：加速查找+约束 （唯一）
3.联合索引
    -primary key(id,name):联合主键索引
    -unique(id,name):联合唯一索引
    -index(id,name):联合普通索引
4.全文索引fulltext :用于搜索很长一篇文章的时候，效果最好。
5.空间索引spatial :了解就好，几乎不用
```

## 14.12 Zookeeper的三种典型应用场景

### 14.12.1 数据发布 / 订阅

**（1）数据发布/订阅系统即所谓的配置中心，也就是发布者将数据发布到ZooKeeper的一个节点或者一系列节点上，提供订阅者进行数据订阅，从而实现动态更新数据的目的，实现配置信息的集中式管理和数据的动态更新。**

采用推拉相结合的方式：客户端向服务器注册自己需要关注的节点，一旦该节点的数据发生改变，服务端就会向相应的客户端发送观察者Watcher事件，客户端收到消息通知后主动去服务端获取最新的变化。

（2）实际系统开发过程中：**我们可以将初始化配置信息放到节点上集中管理，应用在启动时都会主动到ZooKeeper服务端进行一次配置读取，同时在指定节点注册Watcher监听，主要配置信息一旦变更，订阅者就可以获取读取最新的配置信息。**通常系统中需要使用一些通用的配置信息，比如机器列表信息、运行时的开关配置、数据库配置信息等全局配置信息，这些都会有以下3点特性：

　　1） 数据量通常比较小（通常是一些配置文件）

　　2） 数据内容在运行时会经常发生动态变化（比如数据库的临时切换等）

　　3） 集群中各机器共享，配置一致（比如数据库配置共享）。

（3）利用的ZooKeeper特性是：**ZooKeeper对任何节点（包括子节点）的变更，只要注册Wacther事件（使用Curator等客户端工具已经被封装好）都可以被其它客户端监听**

### 14.12.2 Master选举

　　（1）在一些读写分离的应用场景中，客户端写请求往往是由Master处理的，而另一些场景中，Master则常常负责处理一些复杂的逻辑，并将处理结果同步给集群中其它系统单元。比如一个广告投放系统后台与ZooKeeper交互，广告ID通常都是经过一系列海量数据处理中计算得到（非常消耗I/O和CPU资源的过程），那就可以只让集群中一台机器处理数据得到计算结果，之后就可以共享给整个集群中的其它所有客户端机器。

　　（2）利用ZooKeeper的特性：**利用ZooKeeper的强一致性，即能够很好地保证分布式高并发情况下节点的创建一定能够保证全局唯一性，ZooKeeper将会保证客户端无法重复创建一个已经存在的数据节点，也就是说如果多个客户端请求创建同一个节点，那么最终一定只有一个客户端请求能够创建成功，这个客户端就是Master，而其它客户端注在该节点上注册子节点Wacther，用于监控当前Master是否存活，如果当前Master挂了，那么其余客户端立马重新进行Master选举。**

　　（3）竞争成为Master角色之后，创建的子节点都是临时顺序节点，比如：_c_862cf0ce-6712-4aef-a91d-fc4c1044d104-lock-0000000001，并且序号是递增的。**需要注意的是这里有"lock"单词，这说明ZooKeeper这一特性，也可以运用于分布式锁。**

### 14.12.3 分布式锁

　　（1）对于排他锁：**ZooKeeper通过数据节点表示一个锁，例如/exclusive_lock/lock节点就可以定义一个锁，所有客户端都会调用create()接口，试图在/exclusive_lock下创建lock子节点，但是ZooKeeper的强一致性会保证所有客户端最终只有一个客户创建成功。也就可以认为获得了锁，其它线程Watcher监听子节点变化（等待释放锁，竞争获取资源）。**

　　（2）对于共享锁：ZooKeeper同样可以通过数据节点表示一个锁，类似于/shared_lock/[Hostname]-请求类型（读/写）-序号的临时节点，比如/shared_lock/192.168.0.1-R-0000000000

## 14.13 关系型数据库和非关系型数据库的区别？

### 14.13.1 关系型数据库

#### 基本介绍

 关系型数据库是指采用了关系模型来组织数据的数据库。简单来说，关系模式就是二维表格模型。

主要代表：SQL Server，Oracle,Mysql

#### 优点

- 容易理解，二维表的结构非常贴近现实世界。
- 使用方便，通用的SQL语句使得操作关系型数据库非常方便。
- 易于维护，数据库的ACID属性，大大降低了数据冗余和数据不一致的概率。

#### 瓶颈

- 海量数据的读写效率

  网站的并发量高，往往达到每秒上万次的请求，对于传统关系型数据库来说，硬盘I/O是一个很大的挑战。

- 高扩展性的可用性

  在基于web的结构中，数据库是最难以横向拓展的，当一个应用系统的用户量和访问量与日俱增的时候，数据库没有办法像web Server那样简单的通过添加更多的硬件和服务节点来拓展性能和负载能力。

### 14.13.2 从关系型到非关系型的转变

关系型数据库的最大优点就是事务的一致性，这个特性，使得关系型数据库中可以适用于一切要求一致性比较高的系统中。比如：银行系统。

但是在网页应用中，对这种一致性的要求不是那么的严格，允许有一定的时间间隔，所以关系型数据库这个特点不是那么的重要了。相反，关系型数据库为了维护一致性所付出的巨大代价就是读写性能比较差。而像微博、facebook这类应用，对于并发读写能力要求极高，关系型数据库已经无法应付。所以必须用一种新的数据结构存储来替代关系型数据库。所以非关系型数据库应用而生。

### 14.13.3 非关系型数据库

#### 基本介绍

 NoSQL非关系型数据库，主要指那些非关系型的、分布式的，且一般不保证ACID的数据存储系统，主要代表MongoDB，Redis、HBase。

   NoSQL提出了另一种理念，以键值来存储，且结构不稳定，每一个元组都可以有不一样的字段，这种就不会局限于固定的结构，可以减少一些时间和空间的开销。使用这种方式，为了获取用户的不同信息，不需要像关系型数据库中，需要进行多表查询。仅仅需要根据key来取出对应的value值即可。

#### 分类

非关系数据库大部分是开源的，实现比较简单，大都是针对一些特性的应用需求出现的。根据结构化方法和应用场景的不同，分为以下几类。

1）面向高性能并发读写的**key-value**数据库，例如Redis

2）面向海量数据访问的**面向文档**数据库，例如ES

3）面向可拓展的**分布式数据库**，例如HBase

#### 缺点

 但是由于Nosql约束少，所以也不能够像sql那样提供where字段属性的查询。因此适合存储较为简单的数据。有一些不能够持久化数据，所以需要和关系型数据库结合。

### 14.13.4 关系型数据库和非关系型数据库的对比

#### 1）存储角度

Sql通常以数据库表的形式存储，例如存储用户信息，SQL中增加外部关系的话，需要在原表中增加一个外键，来关联外部数据表。

NoSql采用key-value的形式存储

#### 2）事务

SQL中如果多张表需要同批次被更新，即如果其中一张表更新失败的话，其他表也不会更新成功。这种场景可以通过事务来控制，可以在所有命令完成之后，再统一提交事务。

在Nosql中没有事务这个概念。

#### 3）预定义结构和动态结构

在sql中，必须定义好字段和表结构之后，才能够添加数据，例如定义表的主键、索引、外键等。表结构可以在定义之后更新，但是如果有比较大的结构变更，就会变的比较复杂。

在Nosql数据库中，数据可以在任何时候任何地方添加。不需要预先定义。

### 14.13.5 如何选择？

目前许多大型互联网都会选用MySql+NoSql的组合方案，因为SQL和NoSql都有各自的优缺点。

关系型数据库适合存储结构化数据，比如：用户的账号、地址：

（1）这些数据通常需要做结构化查询，比如说Join，这个时候，关系型数据库就要胜出一筹。

（2）这些数据的规模、增长的速度通常是可以预期的。

（3）事务性、一致性,适合存储比较复杂的数据。

NoSql适合存储非结构化数据，比如：文章、评论：

（1）这些数据通常用于模糊处理，例如全文搜索、机器学习，适合存储较为简单的数据。

（2）这些数据是海量的，并且增长的速度是难以预期的。

（3）按照key获取数据效率很高，但是对于join或其他结构化查询的支持就比较差。

## 14.14 最近有没有学过什么技术或者是看了什么书？

数据密集型应用系统设计

数据密集和计算密集是当今两大典型负载类型，前者以大数据为代表，后者以深度学习和机器学习等为主要代表。两者各有自己的使命，也面临一些共同的挑战。其中大数据是开展深度学习的重要前提，也是当前云计算和Web服务的核心支撑技术。

当今许多新型的互联网应用都是数据密集型应用，对于这类应用，CPU的处理能力往往不是第一限制性因素，**关键在于数据量、数据的复杂度及数据的快速多变性**。与其他应用一样，数据密集型应用也是基于标准模块构建而成，每个模块负责单一的常用功能。例如，许多应用系统都包含以下模块：

- 数据库：存储数据，以便应用再次访问
- 高速缓存：缓存那些复杂、频繁或操作代价昂贵的结果，以加快下一次访问
- 索引：按关键字搜索数据并支持各种过滤
- 流式处理：持续发送消息至另一个进程，异步处理
- 批处理：定期处理大量的累积数据