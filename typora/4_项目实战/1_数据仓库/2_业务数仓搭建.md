# 1. 电商业务流程

## 1.1 电商专业术语

SPU：一款商品（电商网页中的一条商品）。

SKU：一款商品的一个型号（电商网页中一条商品的一个型号）。

UV：user views 用户浏览量

PV：page views 页面浏览量（PV > UV ）



## 1.2 电商业务表介绍

| 表名              | 表描述                                                       | 关键字段     | 字段描述               |
| ----------------- | ------------------------------------------------------------ | ------------ | ---------------------- |
| order_info        | 用户每下一笔订单，记录在订单表                               | order_status | 订单的状态，会发生变化 |
|                   |                                                              | create_time  | 订单生成时间           |
|                   |                                                              | operate_time | 操作时间，修改时间     |
| order_detail      | 订单详情表                                                   | create_time  | 订单生成时间           |
|                   |                                                              | order_price  | 单价                   |
| sku_info          | 商品表                                                       | create_time  | 订单生成时间           |
| user_info         | 用户表                                                       | create_time  | 订单生成时间           |
|                   |                                                              | operate_time | 操作时间，修改时间     |
| payment_info      | 支付流水表                                                   | payment_time | 支付时间               |
| order_status_log  | 订单状态表，一旦某个订单的order_status字段发生了变化，那么就向此表插入一条记录 | operate_time | 操作时间大幅度         |
| comment_info      | 评论表                                                       | create_time  | 创建时间               |
| order_refund_info | 退单表                                                       | create_time  | 创建时间               |
| cart_info         | 加购表。记录用户每次在购物车加入商品的信息                   | operate_time | 操作时间               |
|                   |                                                              | create_time  | 创建时间               |
| favor_info        | 商品收藏表                                                   | create_time  | 创建时间               |
|                   |                                                              | cancel_time  | 取消时间               |

# 2. 模拟业务数据

## 2.1 安装 Mysql

### 2.1.1 安装

① 检查本机是否已经安装了 Mysql，或有其他和 Mysql 会冲突的模块。

```
rpm -qa | grep mysql
```

```
sudo rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64
```

```
rpm -qa | grep MySQL
```

② 安装

```
sudo rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm
sudo rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm
```

③ 注意事项

第一次安装后，在 /root/.mysql_secret 文件中寻找 root 用户的临时密码。

建议在 /usr/my.cnf 目录下，编辑 mysql 的配置文件。/usr/my.cnf 和 /etc/my.cnf 建议只选一个。



### 2.1.2 修改 root 用户密码

① 启动 mysql 的服务

```
sudo service mysql start
```

mysql 是开机自启动的，因此开机后，不需要再手动启动服务。

② 查看随机密码

```
sudo cat /root/.mysql_secret
```

③ 登录

```
mysql -u root -pxxxx
```

④ 修改密码

```
set password=password('root');
```

之后退出，重新登入即可。

⑤ 查看默认字符集

```sql
show variables like '%char%'
```



### 2.1.3 配置 root 用户可以在任意机器登录

① 查看当前有哪些可用的帐号

```
select host,user,password from mysql.user;
```

② 删除不是localhost的帐号

```
delete from mysql.user where host <> 'localhost';
```

③ 修改 localhost=%

```
update mysql.user set host='%' where user='root';
```

④ 刷新权限

```
flush privileges;
```

⑤ 测试 root 是否可以从 localhost 登录

```
mysql -uroot -p123456
```

⑥ 测试 root 是否可以从外部地址登录

```
mysql -h hadoop103  -uroot -proot
```

⑦ 查看是否从外部连接

```
sudo mysqladmin processlist -uroot -proot
```

## 2.2 模拟数据

① 先建库 gmall，字符集 utf8，排序规则 utf8-general-ci

② 执行 SQL 脚本 gmall2020-03-16.sql，建表，插入初始数据

③ 执行

```
java -jar gmall-mock-db-2020-03-16-SNAPSHOT.jar
```

确保 application.properties 和 jar 包位于同一目录。

修改 mysql 的连接参数。

修改 mock.date = 模拟的数据的日期

修改 mock.clear=0，如果为0，代表不清空表中的数据，如果为1代表，先清空表中的数据，再插入。

## 2.3 安装 sqoop

① 解压

```
tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/
```

② 改名

```
mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop
```

③ 配置 SQOOP_HOME 环境变量

④ 拷贝 MySQL 驱动

```bash
# 解压
tar -zxvf mysql-connector-java-5.1.27.tar.gz
# 拷贝
cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/
```



## 2.4 数据的同步策略

**原则**：数据的同步策略只取决于数据量。

**数据同步的周期**：每天一次。

全量同步： 适用于表中数据量很小，每天可能会有数据新增或数据修改的情形。

增量同步： 适用于数据量大，每天只会向表中新增数据的场景，例如支付流水表。

新增和变化同步： 适用于数据量大，每天不仅会新增数据，还会有历史数据发送变化，例如：订单表，用户表等。

特殊表：例如地区表，省份表，日期表，可以只导入一次即可。

## 2.5 数据的同步时机

数据分为两种：

① 业务数据

② 用户行为数据

业务数据比用户行为数据要少，需要等用户行为数据和业务数据都已经采集到 hdfs 时，才可以在数仓中建表，导入数据，分析计算。以极端情况为例，如果在当日的 23:59 分，采集了当日的100G 数据，这些用户行为数据采集到 hdfs 需要花费 0.5 到 1 小时。因此可以在次日的 0:30 或 1:00 采集 MySQL 中的业务数据，此时用户行为数据已经全部导入到了 HDFS。

## 2.6 Sqoop 导入脚本

### 2.6.1 -n 参数

​	-n 可以判断后面的参数是否赋值，如果赋值 [ -n 参数 ] 返回 true，否则返回 false。

```shell
#!/bin/bash
#脚本可以传入要导入数据的日期，如果没有导入日期，取当前时间的前一天
if [ -n "$1" ]
then
        do_date=$1
else
        do_date=$(date -d '-1 day' '+%F')
fi
```



### 2.6.2 导入数据的主要函数

```shell
import_data(){
$sqoop import \
--connect jdbc:mysql://hadoop102:3306/gmall \
--username root \
--password 000000 \
--target-dir /origin_data/gmall/db/$1/$do_date \
--delete-target-dir \
--query "$2 and  \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
# 使用压缩，和指定压缩格式为lzop
--compress \
--compression-codec lzop \

--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/gmall/db/$1/$do_date
}

```

$1 代表调用函数时，传入的第一个参数，指数据要导入到 hdfs 的哪个目录下。

--null-string '\\N'，将MySQL中的 String 类型列的 NULL 值，在导入到 hive 时，使用指定的值 '\\N' 代替。当在 hive 中使用 NULL 类型进行过滤时，HQL 语句一般这样写：

```
select * from xxx where id is null;
```

hive 中默认使用 \N 代替 NULL，上述SQL在运行时，HIVE 会匹配 id 列值为 \N 的所有数据。

--null-non-string '\\N'：将 MySQL 中的非 String 类型列的 NULL 值，在导入到 hive 时，使用指定的值 '\\N' 代替。

在 export 时，可以使用

| `--input-null-string `     | The string to be interpreted as null for string columns.     |
| -------------------------- | ------------------------------------------------------------ |
| `--input-null-non-string ` | The string to be interpreted as null for non string columns. |

`--input-null-string ` xx： hive 中 String 类型的值为 xx，导出到 MySQL 时，使用 NULL 类型来代替。

--input-null-non-string xx: hive 中非 String 类型的值为 xx，导出到 MySQL 时，使用 NULL 类型来代替。

### 2.6.3 脚本的使用

mysql_to_hdfs.sh 传入两个参数

第一个参数可以传入要导入的表名，或 all，或 first。

第二个参数可以传入一个日期，如果不传入，取当日的前一天。

使用： 如果第一次导入数据，传入

```
mysql_to_hdfs.sh  first  日期
```

之后再导入数据

```
mysql_to_hdfs.sh all 日期
```

如果要单独导入某个表

```
mysql_to_hdfs.sh  表名 日期
```

## 2.7 安装Hive

① 解压hive，在 /etc/profile 中配置 HIVE_HOME。

② 拷贝驱动到 $HIVE_HOME/lib 目录

```bash
cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/
```

③ 编辑 $HIVE_HOME/conf/hive-site.xml（需新建）

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>
	    <name>javax.jdo.option.ConnectionURL</name>
	    <value>jdbc:mysql://hadoop103:3306/metastore?createDatabaseIfNotExist=true</value>
	    <description>JDBC connect string for a JDBC metastore</description>
	</property>

	<property>
	    <name>javax.jdo.option.ConnectionDriverName</name>
	    <value>com.mysql.jdbc.Driver</value>
	    <description>Driver class name for a JDBC metastore</description>
	</property>

	<property>
	    <name>javax.jdo.option.ConnectionUserName</name>
	    <value>root</value>
	    <description>username to use against metastore database</description>
	</property>

	<property>
	    <name>javax.jdo.option.ConnectionPassword</name>
	    <value>root</value>
	    <description>password to use against metastore database</description>
	</property>
    
    <property>
         <name>hive.metastore.warehouse.dir</name>
         <value>/user/hive/warehouse</value>
         <description>location of default database for the warehouse</description>
    </property>
    
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
    
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>
    
    <property>
        <name>datanucleus.schema.autoCreateAll</name>
        <value>true</value> 
    </property>
    
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://hadoop103:9083</value>
    </property>
</configuration>

```

注意：hive安装在哪个服务器节点，thrift://hadoop102:9083中的主机名就更换为相应的主机名。

④ 2.x以上的 hive 需要开启 metastore 元数据服务，才能访问 hive 的元数据。

```
nohup bin/hive --service metastore &
```



## 2.8 安装Hive On Tez

① 解压 Tez 并重命名

```bash
mv apache-tez-0.9.1-bin/ tez-0.9.1
```

② 将 Tez.tar.gz 上传到 HDFS 的指定目录下

```
hadoop fs -mkdir /tez
hadoop fs -put /opt/software/apache-tez-0.9.1-bin.tar.gz /tez
```

③ 编辑 $HIVE_HOME/conf/tez-site.xml（需新建）

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>tez.lib.uris</name>
        <value>${fs.defaultFS}/tez/apache-tez-0.9.1-bin.tar.gz</value>
    </property>
    <property>
         <name>tez.use.cluster.hadoop-libs</name>
         <value>true</value>
    </property>
    <property>
         <name>tez.history.logging.service.class</name>        
         <value>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService</value>
    </property>
</configuration>
```

④ 编辑 $HIVE_HOME/conf/hive-site.xml，配置 hive 的执行引擎为 tez

```xml
<property>
    <name>hive.execution.engine</name>
    <value>tez</value>
</property>
```

⑤ 配置 hive-env.sh，使得 hive 在启动时，加载 tez 的 jar 包，这样 hive 才能将 HQL 翻译为 tez job

```bash
export TEZ_HOME=/opt/module/tez-0.9.1    #是你的tez的解压目录
export TEZ_JARS=""
for jar in `ls $TEZ_HOME |grep jar`; do
    export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar
done
for jar in `ls $TEZ_HOME/lib`; do
    export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar
done

export HIVE_AUX_JARS_PATH=/opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS
```

⑥ 编辑 yarn-site.xml，关闭虚拟内存检查，之后分发，重启 hadoop 集群

```xml
<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
```

⑦ 测试

```sql
create table student(
id int,
name string);
insert into student values(1,"zhangsan");
select * from student;
```



## 2.9 Tez 的优势

​		Tez 的优势就是支持 DAG 运算。可以把一个复杂度任务，只用一个 Job 或者用更少的 Job 来实现，减少 Job 之间数据传输的网络 IO。Tez 也调大了组件之间的内存，减少期间的磁盘 IO。

​		Tez 重新设计了 MR 编程模型，其中已经不是我们熟悉的 Mapper 和 Reducer。但是 Tez 仍然运行在 YARN 上。

### 2.9.1 MapJoin 注意事项

tez 在进行 join 时，可能会出现出现丢数据的情况。此时需要修改如下 hive 变量：

```sql
-- 是否自动开启mapjoin,默认为true
set hive.auto.convert.join=true;

-- mapjoin小表和大表的阈值设置
set hive.mapjoin.smalltable.filesize=25000000;

-- 多个mapjoin 转换为1个时，限制输入的最大的数据量 影响tez，默认10m
set hive.auto.convert.join.noconditionaltask.size = 10000000;

-- 原则：hive.mapjoin.smalltable.filesize和hive.mapjoin.smalltable.filesize一致
-- 保证小表中所有的数据，都可以参与计算
```

