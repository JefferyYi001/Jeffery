# 1. 数据仓库概念

## 1.1 数据仓库

​		数据仓库是为企业的所有的决策提供数据支持的数据战略集合，企业的数据仓库需要汇集企业的各种系统、各种类型的数据，在数据仓库中对数据进行清洗、组合、聚合、拆分、计算等，为企业的其他业务需求提供复合的数据。



## 1.2 技术选型

数据传输： flume、kafka、sqoop

数据存储： mysql、hdfs

数据计算： hive、tez

即席查询(秒级查询)：presto、druid、kylin

可视化： echarts、superset

元数据管理： atlas

数据质量监控： griffin

集群监控： zabbix



## 1.3 项目架构

用户行为数据：用户在使用电商产品过程中发生的访问，点击，点赞，评论等行为的记录。

电商业务数据： 用户在使用电商平台提供的和电商业务相关的功能产生的数据，例如下单，支付等。

### 1.3.1 用户行为数据的采集

通常用户行为数据是通过APP(webapp、Android、iOS)端埋点的方式，收集到日志服务器中，以日志的形式进行记录：

​		使用 flume 将日志文件中的数据采集到 kafka；

​		再启动 flume 从 kafka 中将数据采集到 hdfs；

设计的目的： 

① 使用kakfa进行削峰。如果每台日志服务器都启动 flume 进程直接向 hdfs 写入数据，此时 hdfs 的压力较大。

② 分层解耦，项目后期维护性强。

③ 为实时分析业务场景保留设计。未来可以使用 SparkStreaming 直接消费 kafka 中的数据进行实时分析。

④ 权限问题。并不是每一台日志采集服务器都有权限向 HDFS 写数据。

⑤ 可达性问题。并不是每一台日志采集服务器都装有 hadoop 或 hadoop 的 jar 包。

### 1.3.2  电商业务数据的采集

用户在使用电商业务期间，产生的数据是存储在 mysql 中，使用 sqoop 将 mysql 中的数据导入到 hdfs 即可。

### 1.3.3  数据的分析

将用户的行为数据和业务数据汇总到数据仓库(基于 hive 创建)中，在数据仓库中分层建表分析。最后将企业关心的数据，通过 hive 计算后，将结果导出到 mysql 中。

### 1.3.4  数据的可视化

使用可视化系统从 mysql 中读取计算的结果，进行可视化，或被其他的业务系统使用。

## 1.4 测试集群的进程分配

### 1.4.1 分配的原则

① 重要的进程要错开规划。例如： NN、RM 进程尽量不要规划在一台机器。

② 性能消耗相同的进程要错开规划。例如：NN、RegionServer 都需要占用机器大量的内容，不要规划在一台机器。

③ 特殊或有关联的进程需要在一台机器配置，例如：当前使用 azkaban 调度 hive 时，azkaban 必须和 hive 安装在同一台机器。

④ 开启 flume 进程采集用户行为日志，那么 flume 的 agent 必须安装在日志服务器所在的机器。

⑤ 开启 flume 将数据采集到 hdfs，当前机器必须安装了 hadoop，hdfssink 才能调用 jar 包访问 hdfs。

⑥ 如果集群当前使用了HBase，那么 regionserver 进程尽量要安装在 Datanode 进程所在的节点。好处是可以实现数据读写的本地化，节省网络IO。

​		

# 2. 数据生成

## 2.1 用户行为日志数据的介绍

### 2.1.1 埋点数据介绍

只有用户发生了指定的行为，触发了埋点程序，才会记录用户的行为。

​		格式：  { "ap": "数据的来源的设备",

​					   "cm": {"公共字段属性名":"公共字段属性值" ,.... },

​					   "et":[{

​									"ett":事件发生的时间,

​									"en":事件的名称

​									"kv":{ 事件的各个属性 }

​							},{},....]

​					 }

埋点数据在发送到服务器后，还会拼接上发送的时间戳，因此在服务端收到的埋点数据格式：

​			时间戳 |｛埋点数据｝

### 2.1.2 启动日志数据

​		用户在打开应用的 APP，或第一次访问应用时，产生的日志信息称为启动日志。启动日志是非常重要的数据，可以用来分析用户的活跃度。

​		格式： { 公共字段，启动日志字段， "en":"start"   }

## 2.2 FastJson的使用

### 2.2.1 JSON

#### 2.2.1.1 JSON Object

JSON Object 转为 JSON 字符串后，格式如下：

```json
{"id":1001,"name":"zhangsan","age":20,"friends",[{},{}]}
```

#### 2.2.1.2 JSON Array

JSON Array 转为 JSON 字符串后，格式如下：

```json
[{"id":1001,"name":"zhangsan","age":20},
{"id":1001,"name":"zhangsan","age":20},
{"id":1001,"name":"zhangsan","age":20}]
```

### 2.2.2 FastJson 中常用的 API

#### 2.2.2.1 JSON

​		JSON 类是最常用的 API，常用方法：

![image-20200403154737216](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200403154737216.png)

​		① JSON.toJSONString(Object)：将 Java/ JSON 对象转为 JSON 字符串（同 JSON.toString）。

​		② JSON.parseObject(String, Class)：将 JSON 字符串，解析为指定的 java 类型。

​		③ JSON.parseArray(String, Class)：将 JSON 字符串数组，解析为指定的 java 类型的 List。

#### 2.2.2.2 JSONObject

​		JSONObject 就代表一个 JSONObject 对象，本质是使用 Map 实现，可在其构造器中传入一个 Map。

​		通过 JSONObject.get(属性名)，可以获取 JSONObject 对象中的某个属性对应的值。

​		通过 JSONObject.put(属性名,属性值)，可以向 JSONObject 对象中添加某个属性及对应的值。

#### 2.2.2.3 JSONArray

​		JSONArray 就代表一个 JSON Array 对象，本质使用 List 实现，可在其构造器中传入一个 List。

​		通过 JSONArray.get(index) 获取指定位置的参数。

​		通过 JSONArray.add(Object e) 添加参数。

​		

## 2.3 Logback

### 2.3.1 Logback 和 Log4j 的对比

- lo4j 比 logback 出现的早，作者是同一个人。显然 logback 是 log4j 的升级版，logback 比 log4j 速度更快、性能更强、更加稳定。提供了对 SLF4J 的实现，可以和 log4 j进行无缝切换，功能更加丰富。
- logback 使用 .xml 文件作为配置文件，log4j 使用 properties 文件作为配置文件。

### 2.3.2 Logback 的组件

**Logger**：日志记录器，在 Logger 上可以定义日志记录的级别、类型等。

**Appender**：日志追加器，只要定义日志输出的目的地，输出到控制台还是到文件还是到其他的进程等。通常必须传入 name( appender 的名称，可以自定义)，class( append 的实现类，可以使用系统的实现类或用自己定义的实现类)。

**Layout**：日志的样式，日志会参照 layout 的格式进行格式化输出。

## 2.4 打包插件

maven-compiler-plugin 插件，配置项目使用 JDK 的哪个版本进行编译，如果已经在 maven 的settings.xml 中配置了此参数，可以省略。

maven-assembly-plugin 负责打包。会将 pom 中的依赖导入 jar 包。如果jar包运行的环境，没有安装 maven，也不会把依赖的 Jar 包加入到 jar 包的类路径下，可以采用 maven-assembly-plugin 打的包。

maven-jar-plugin(默认)：负责打包。不会将 pom 中的依赖打入到 jar 包中。如果 jar 包的运行环境已经安装了 maven，jar 包运行时，可以根据 pom 中的 GAV 坐标自动去 maven 仓库中匹配 jar包。

消除控制台日志折叠操作：Setting -> Editor -> General -> Console -> 勾选 Use

# 3. 测试集群搭建

## 3.1 搭建步骤（参考完全分布式）

（1）准备三台虚拟机，在硬件允许的条件下，内存尽可能设大一点，磁盘上限设置为 50G。

​			16G 内存，可以考虑分配 4，4，4 或 3，5，3

​			32G 内存，可以考虑 8，8，8

​			24G 内存，可以考虑 6，6，6

​		之后克隆虚拟机即可。

（2）设置虚拟机可以联网

a) 设置每台虚拟机使用独立的 MAC 地址联网（CentOS7 无此步骤 ）

```bash
vim /etc/udev/rules.d/70-persistent-net.rules
```

b) 每台虚拟机有独立的 ip

```bash
vim /etc/sysconfig/network-scripts/ifcfg-eth0
```

```bash
DEVICE=eth0
TYPE=Ethernet
ONBOOT=yes
BOOTPROTO=static
NAME="eth0"
IPADDR=192.168.1.102
PREFIX=24
GATEWAY=192.168.1.2
DNS1=192.168.1.2
```

CentOS7 操作步骤：

```bash
vim /etc/sysconfig/network-scripts/ifcfg-ens33
```

```bash
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
BOOTPROTO="static"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="a6d6f539-be4a-47d7-beea-7b031d292e18"
DEVICE="ens33"
ONBOOT="yes"
IPADDR=192.168.10.200
GATEWAY=192.168.10.2
DNS1=192.168.10.2
```



c) 集群中的每台虚拟机可以相互联通

修改主机名

```bash
vim /etc/sysconfig/network
# CentOS7 命令
vi /etc/hostname
```

查看主机名：

```
hostname
```

查看当前的ip:

```
ifconfig
```

验证：

```
ping hadoop102 -c 3
```

d) 集群可以连接互联网

```
ping www.baidu.com -c 3
```

e) 集群中的虚拟机已经配置了 hosts 映射信息

```bash
vim /etc/hosts
# CentOS7 采用相同命令
```

```
192.168.1.100 hadoop100
192.168.1.101 hadoop101
192.168.1.102 hadoop102
192.168.1.103 hadoop103
192.168.1.104 hadoop104
192.168.1.105 hadoop105
192.168.1.106 hadoop106
192.168.1.107 hadoop107
192.168.1.108 hadoop108
```

f) 每天机器拥有自己的主机名

```bash
vim /etc/sysconfig/network
```

g) 修改启动级别为3，不会再启动图形化界面，节省内存

```bash
vim /etc/inittab
# 修改为
id:3:initdefault:
# CentOS7 不再需要修改配置文件，直接执行命令：
systemctl get-default	# 查看当前的开机默认运行方式
systemctl set-default multi-user.target	#设置开机不自启图形界面
systemctl set-default graphical.target	#设置开机启动图形界面
```

（4）为每台机器配置 atguigu 用户，赋予免密执行的 root 权限

添加用户:

```
useradd atguigu
```

为 atguigu 用户设置密码：

```
passwd atguigu
```

赋予免密执行的 root 权限：

```bash
vim /etc/sudoers
```

切换到92行：  92+shift+g 或 92+G

在92行，添加如下内容：

```bash
atguigu        ALL=(ALL)       NOPASSWD: ALL
```

（5）在/opt目录下，创建 software 和 module 两个目录，用于存在安装的软件，将目录的所属主修改为 atguigu

```bash
mkdir /opt/software
mkdir /opt/module
chown atguigu /opt/software /opt/module
```

（6）关闭防火墙的开机自启动

```bash
chkconfig iptables off
```

（7）若登陆后希望切换到图形界面，可输入

```
init 5
```

如果使用5号级别登录：

可以使用 alt+ctrl+F1 切换到命令行（3号），alt+ctrl+F2 切换到图形化界面(5号)。

## 3.2 编辑工具脚本

### 3.2.1 xsync

用于将本台机器的文件，分发到集群中相同的父目录下。

例如： 将本机的 /home/atguigu/a.log 分发到集群所有机器的 /home/atguigu目录下。

```shell
#!/bin/bash
#脚本必须跟一个文件的相对路径或绝对路径
if(($#!=1))
then
        echo 请输入要分发单个文件的路径!
        exit;
fi

#获取分发文件的物理绝对路径
dirpath=$(cd -P `dirname $1` ; pwd)
filename=`basename $1`

echo "要分发的绝对路径是:$dirpath/$filename"

#获取当前用户的名称
username=`whoami`

#分发
for((i=102;i<=104;i++))
do
        echo "--------------------hadoop$i--------------------"
        rsync -rvlt $dirpath/$filename $username@hadoop$i:$dirpath
done

```

### 3.2.2 配置 ssh 免密登录

​		配置 atguigu 用户和 root 用户到集群其他机器的 ssh 免密登录。

先使用 atguigu 用户执行以下操作：

```
ssh-keygen -t rsa
ssh-copy-id hadoop102
ssh-copy-id hadoop103
ssh-copy-id hadoop104
```

再切换到 root 用户，重复执行以上命令。

### 3.2.3 xcall

```shell
#!/bin/bash
#校验是否传入了命令
if(($#==0))
then
        echo 请输入要执行的命令!
        exit;
fi


echo "要执行的命令是:$@"


#使用ssh登录到目标机器执行命令
for((i=102;i<=104;i++))
do
        echo "--------------------hadoop$i--------------------"
        ssh hadoop$i $@
done
```

## 3.3 解压软件，配置环境变量

编辑 /etc/profile

```bash
sudo vim /etc/profile
```

```shell
JAVA_HOME=/opt/module/jdk1.8.0_144
HADOOP_HOME=/opt/module/hadoop-2.7.2
ZOOKEEPER_HOME=/opt/module/zookeeper
FLUME_HOME=/opt/module/flume
KAFKA_HOME=/opt/module/kafka

PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin:$FLUME_HOME/bin:$KAFK
A_HOME/bin
export JAVA_HOME HADOOP_HOME ZOOKEEPER_HOME FLUME_HOME KAFKA_HOME
```

验证之前，需要先编辑当前 atguigu 用户 /home/atguigu/.bashrc 文件，添加如下内容

```
source /etc/profile
```

分发 .bashrc 到集群。

## 3.4 配置 hadoop 集群

规划：hadoop102-NN、hadoop103-RM、hadoop104-2NN、hadoop102-mrjobhistory

编辑 core-site.xml

```xml
<!-- 指定HDFS中NameNode的地址 -->
<property>
<name>fs.defaultFS</name>
    <value>hdfs://hadoop102:9000</value>
</property>

<!-- 指定Hadoop运行时产生文件的存储目录 -->
<property>
	<name>hadoop.tmp.dir</name>
	<value>/opt/module/hadoop-2.7.2/data/tmp</value>
</property>

```

编辑 hdfs-site.xml

```xml
<!-- 指定Hadoop辅助名称节点主机配置 -->
<property>
      <name>dfs.namenode.secondary.http-address</name>
      <value>hadoop104:50090</value>
</property>
```

编辑 mapred-site.xml

注意：先执行

```bash
mv mapred-site.xml.template mapred-site.xml
```

```xml
<!-- 指定MR运行在YARN上 -->
<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
</property>
<!-- 历史服务进程运行的主机名和rpc端口号 -->
<property>
<name>mapreduce.jobhistory.address</name>
<value>hadoop102:10020</value>
</property>
<!-- 历史服务进程运行的主机名和web端口号 -->
<property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>hadoop102:19888</value>
</property>
<!--第三方框架使用yarn计算的日志聚集功能 ,适用于spark和tez-->
<property>    
    <name>yarn.log.server.url</name>         	       <value>http://hadoop102:19888/jobhistory/logs</value>
</property>

```

编辑 yarn-site.xml

```xml
<!-- reducer获取数据的方式 -->
<property>
 		<name>yarn.nodemanager.aux-services</name>
 		<value>mapreduce_shuffle</value>
</property>

<!-- 指定YARN的ResourceManager的地址 -->
<property>
<name>yarn.resourcemanager.hostname</name>
<value>hadoop103</value>
</property>

<!-- 日志聚集功能使能 -->
<property>
<name>yarn.log-aggregation-enable</name>
<value>true</value>
</property>

<!-- 日志保留时间设置7天 -->
<property>
<name>yarn.log-aggregation.retain-seconds</name>
<value>604800</value>
</property>

```

配置完成后，分发配置文件到集群。

### 3.4.1 其他操作

如果需要，可以配置公平调度器。

```properties
<!-- 设置公平调度器 -->
<property>
<name>yarn.resourcemanager.scheduler.class</name>
<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
</property>
```

## 3.5 初始化 hadoop

（1）格式化 NameNode

```bash
hadoop namenode -format
```

（2）在执行群起的机器上，编写 $HADOOP_HOME/etc/hadoop/slaves 文件。建议在RM(hadoop103) 所在的机器，执行群起脚本。

```bash
hadoop102
hadoop103
hadoop104
```

## 3.6 编写 hadoop 集群的群起脚本

当前使用 atguigu 用户执行 hadoop 集群的启动，将脚本放入到 /home/atguigu/bin。

```shell
#!/bin/bash
#hadoop集群的一键启动脚本，只接收单个start或stop参数
if(($#!=1))
then
        echo 请输入单个start或stop参数！
        exit
fi

#对传入的单个参数进行校验，且执行相应的启动和停止命令
if [ $1 = start ] || [ $1 = stop ]
then
        $1-dfs.sh
        $1-yarn.sh
        ssh hadoop102 mr-jobhistory-daemon.sh $1 historyserver
else
        echo 请输入单个start或stop参数！
fi

```



## 3.7 配置 zk 集群

编辑 $ZOOKEEPER_HOME/conf/zoo.cfg 文件，添加如下配置：

```properties
dataDir=/opt/module/zookeeper/datas

server.102=hadoop102:2888:3888
server.103=hadoop103:2888:3888
server.104=hadoop104:2888:3888
```

新建 /opt/module/zookeeper/datas/myid，并输入103，保存。

分发文件到集群，修改每台机器 /opt/module/zookeeper/datas/myid 文件中的 id 号为当前机器对应的 id。



## 3.8 编写 zk 集群的群起脚本

```shell
#!/bin/bash
#zk集群的一键启动脚本，只接收单个start或stop或status参数
if(($#!=1))
then
        echo 请输入单个start或stop或status参数！
        exit
fi

#对传入的单个参数进行校验，且执行相应的启动和停止命令
if [ $1 = start ] || [ $1 = stop ] || [ $1 = status ]
then
        xcall zkServer.sh $1
else
        echo 请输入单个start或stop参数或status！
fi
```

## 3.9 配置 hadoop 集群支持 lzo 压缩

### 3.9.1 安装 lzo

查看当前集群支持哪些压缩方式：

```
hadoop checknative
```

LZO 压缩格式，不支持 hadoop 平台，需要安装 hadoop-lzo 组件，才可以让 hadoop 平台使用LZO 压缩格式。

将 hadoop-lzo-0.4.20.jar 放入 hadoop-2.7.2/share/hadoop/common/，之后分发到集群。

编辑：core-site.xml

```xml
<property>
<name>io.compression.codecs</name>
<value>
org.apache.hadoop.io.compress.GzipCodec,
org.apache.hadoop.io.compress.DefaultCodec,
org.apache.hadoop.io.compress.BZip2Codec,
org.apache.hadoop.io.compress.SnappyCodec,
com.hadoop.compression.lzo.LzoCodec,
com.hadoop.compression.lzo.LzopCodec
</value>
</property>

<property>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
</property>

```

分发 core-site.xml，重启 hadoop 集群。

### 3.9.2 LZO 和 LZOP 的区别

（1）LZOP 依赖于 LZO，LZOP 是 LZO 的升级版。

（2）LZO 是原生的 LZO 压缩格式，不支持切片。

​		 LZOP 在 LZO 的基础上允许我们使用一个为压缩文件创建索引的程序，为每个压缩文件生成索引。LZOP 基于索引对压缩文件进行切片。如果使用了 LZOP 但是，没有创建索引，自然整个文件切一片（无法切片）。

（3）LZO 压缩格式，生成的压缩文件的后缀名为.lzo.deflate。

​		 LZOP 压缩格式，生成的压缩文件的后缀为.lzo。

（4）如果 MR 程序的输入，读取的.lzo.deflate 格式的数据，那么可能会出现乱码。

总结：在 shuffle 阶段，可以使用 lzo，但是在 reduce 的输出和 map 的输入阶段，使用 lzop。



### 3.9.3 测试 lzo

执行 lzo 压缩

```shell
yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount -Dmapreduce.output.fileoutputformat.compress=true -Dmapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzopCodec /test /output
```

为 lzo 文件创建索引

```shell
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar  com.hadoop.compression.lzo.DistributedLzoIndexer /output/part-r-00000.lzo
```

## 3.10 HDFS的性能测试

### 3.10.1 测试写性能

```
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB
```

测试结果：

```
20/03/27 15:34:21 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write
20/03/27 15:34:21 INFO fs.TestDFSIO:            Date & time: Fri Mar 27 15:34:21 CST 2020
20/03/27 15:34:21 INFO fs.TestDFSIO:        Number of files: 10
一共写了1280M数据
20/03/27 15:34:21 INFO fs.TestDFSIO: Total MBytes processed: 1280.0
集群在写文件时的吞吐量为10m/s
20/03/27 15:34:21 INFO fs.TestDFSIO:      Throughput mb/sec: 10.61685591765301
集群每个节点的平均写的速度为 10.7/s
20/03/27 15:34:21 INFO fs.TestDFSIO: Average IO rate mb/sec: 10.7449369430542
集群每个节点写入数据的标准差 为1.2
标准差反映一组数据举例平均值差值的离散程度，标准差越大，越离散
20/03/27 15:34:21 INFO fs.TestDFSIO:  IO rate std deviation: 1.202543289543699
20/03/27 15:34:21 INFO fs.TestDFSIO:     Test exec time sec: 41.937
```

### 3.10.2 测试读性能

```bash
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB
```

### 3.10.3 删除测试数据

```bash
hadoop jar /opt/module/hadoop-2.7.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar TestDFSIO -clean
```

# 4. 日志生成

## 4.1 日志生成 jar 试运行

将编译好的 jar 包拷贝到 /home/atguigu/bin 目录下，运行：

```bash
java -jar log.jar 20 5
```

运行结束后查看日志中的结果：

```bash
wc -l /tmp/logs/app-2020-03-27.log 
```



## 4.2 运行 jar 包时取消在控制台输出日志

第一种：从源头上解决，修改 logback 的配置文件，不使用 console appender。

第二种：可以指定 linux 在运行 jar 包程序时，将程序向控制台写出的内容定向到 /dev/null (黑洞)。



## 4.3 标准输入和标准输出和错误输出

在 linux 中，有三个常用的 IO 设备

- 0：代表 stdin 标准输入。类似 Java 中的 System.in.scan()。接收用户在键盘的信息，传递给标注输入设备。
- 1：代表 stdout 标准输出。类似 Java 中的 System.out.print()，将信息输出到标注输出设备。
- 2：代表 stderr 标准错误。类似 Java 中的 System.err.print()，将信息输出地嗷错误输出设备。

| 名称       | 用途         | 输出显色颜色 | 缓存功能 | log4j日志记录 |
| :--------- | ------------ | ------------ | -------- | ------------- |
| System.out | 标准输出     | 黑色字体     | 缓存     | 会            |
| System.err | 标准错误输出 | 红色字体     | 实时     | 不会          |

默认情况：命令 > 文件 等价于 命令 1>文件

```
faefafeafca > a.log 等价于 faefafeafca 1> a.log
```

```
faefafeafca 1> c.log 2> c.log 等价于  faefafeafca 1> d.log 2>&1
```

区别： faefafeafca 1> c.log 2> c.log 会打开 c.log 文件两次。

​			faefafeafca 1> d.log 2>&1 只会打开 c.log 文件一次，效率高，建议使用。

说明：“>” 仍代表覆盖写，若需要添加写请使用 “>>”。

```
java -jar log.jar 20 5  1> /dev/null 2>&1
```

代表程序运行期间产生的标注错误中的信息和标注输出的信息全部放入黑洞。

## 4.4 编写日志生成脚本

```shell
#!/bin/bash
#调用后，在hadoop102和hadoop103运行日志生成程序，在每台机器的/tmp/logs目录下生成日志数据
for i in hadoop102 hadoop103
do
        ssh $i java -jar /home/atguigu/log.jar $1 $2 > /dev/null 2>&1 &
done
```



## 4.5 shell命令中的空格问题

① 如果shell命令中有空格，可以使用单引号或双引号引起来。

② 单引号不会识别 $ 特殊符号，而双引号会识别 $ 符号，将 $ 解析为变量的引用。

③ 最外层是双引号，内嵌单引号，$ 等特殊符号依旧可以识别。

④ 最外层是单引号，内嵌双引号，$ 等特殊符号无法识别。

``反引号，将反引号中的内容作为 linux 命令执行，赋值给一个变量，等价于$()。区别是对转义字符 “\” 的对待上，反引号不进行转义，$() 中的 “\” 将 $ 转义成普通字符。



## 4.6 编写集群时间同步到指定日期的脚本

```shell
#!/bin/bash
#dt.sh 日期，可以让集群中所有机器的时间同步到此日期
#如果用户没有传入要同步的日期，同步日期到当前的最新时间
if(($#==0))
then
        xcall sudo ntpdate -u ntp1.aliyun.com
        exit;
fi

#dt.sh 日期，可以让集群中所有机器的时间同步到此日期
for((i=102;i<=104;i++))
do
        echo "--------------同步hadoop$i--------------"
        ssh hadoop$i "sudo date -s '$@'"
done
```

# 5. 采集通道软件的安装

## 5.1 安装 flume

环境要求：JAVA_HOME 变量，如果需要使用 HDFSSink，需要配置 HADOOP_HOME。

## 5.2 安装 kafka

### 5.2.1 配置

环境要求：配置 JAVA_HOME 变量，且安装了 zookeeper。

配置 config/server.properties

```properties
#21行，每台cluster中的broker都需要有唯一的id号，必须为整数
broker.id=103
#24行，打开注释,此行代表允许手动删除kafka中的主题
delete.topic.enable=true
#63行，配置kafka存储的数据文件的存放目录
log.dirs=/opt/module/kafka/datas
#126行，配置连接的zk集群的地址
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/mykafka
```

### 5.2.2 启动脚本

```shell
#!/bin/bash
#kafka集群的一键启动脚本，只接收单个start或stop参数
if(($#!=1))
then
        echo 请输入单个start或stop参数！
        exit
fi

#对传入的单个参数进行校验，且执行相应的启动和停止命令
if [ $1 = start ]
then
        xcall kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
        elif [ $1 = stop ]
                then xcall kafka-server-stop.sh
else
        echo 请输入单个start或stop参数！
fi
```

### 5.2.3 kafka 生产数据压测

压测脚本

```bash
kafka-producer-perf-test.sh  --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
```

压测结果：

```
100000 records sent, 59665.871122 records/sec (5.69 MB/sec), 463.76 ms avg latency, 626.00 ms max latency, 519 ms 50th, 608 ms 95th, 622 ms 99th, 626 ms 99.9th.
```

重点查看：

59665.871122 records/sec (5.69 MB/sec)，如果生产能力不足，可以对主题添加分区，或扩容集群，或设置 kafka 同时挂载多个磁盘目录，提高磁盘的 IO 能力（前提是要有多个分区）。

​	463.76 ms avg latency： 在测试时，延迟是否可以达到公司的业务场景要求。

### 5.2.4 kafka消费数据压测

压测脚本：

```bash
kafka-consumer-perf-test.sh --broker-list hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic test --fetch-size 10000 --messages 10000000 --threads 1
```

压测结构：

```
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
2020-03-28 15:19:21:653, 2020-03-28 15:19:26:206, 9.4906, 2.0845, 99516, 21857.2370
```

重点查看：

​	MB.sec： 一秒消费多少M数据

​	nMsg.sec： 一秒消费多少数据

如何提高消费性能：

① 提高消费者的线程数，前提是一个主题有多个可用的分区。

② 在消费者端配置一些参数，例如 offset 的异步提交等。

### 5.2.5 kafka 压测过程中 bug 解决

```
WARN Error while fetching metadata with correlation id 3 : {test=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
```

解决方案：修改 server.properties 如下，目的是让 kafka 监听的到

```properties
# 第34行
listeners=PLAINTEXT://hadoop103:9092
# 第39行
advertised.listeners=PLAINTEXT://hadoop103:9092
```



# 6. 第一层 Flume 采集通道

## 6.1 目的

​		将数据从日志文件中使用 flume 采集到 kafka 集群中。

## 6.2 选择 Agent 中的组件

### 6.2.1 Source

数据源是在日志文件中，读取日志中的数据，可以使用以下 source：

- ExecSource：通过运行 tail -f 命令监控文件中新写入的数据。不建议使用，可能丢失数据。

- SpoolingDirSource：通过监控目录中新写入的文件，将文件的内容读取，封装为 agent。要求目录中的文件，必须是一成不变的，不能出现重名。但日志中的数据是不断被追加的，因此不采用。

- TailDirSource：接近实时第读取文件中新增的内容，有断点续传功能。结合项目的实际要求，选择 TailDirSource。

必选参数

| **type**                          | –    | `TAILDIR`.                     |
| --------------------------------- | ---- | ------------------------------ |
| **filegroups**                    | –    | 组名                           |
| **filegroups.filegroup.filename** | –    | 一个组中可以配置多个文件的路径 |

可选参数

|              |                         |
| ------------ | ----------------------- |
| positionFile | 存放 postionfile 的路径 |

### 6.2.2 Channel

#### 6.2.2.1 KafkaChannel 的介绍

选择 Kafkachannel 的理由：因为 kafka 集群有高可用和副本机制，这样即便 agent 挂掉，或某个broker 宕机，sink 也可以立刻从新的 leader 上继续拉取 event。

#### 6.2.2.2 KafkaChannel 适用的场景

① source + KafkaChannel + sink，KafkaChannel 单次作为 channel 使用。

source ---> Event ----> KafkaProducer ---> ProducerRecord ----> Kafka Topic <----- kafkaConsumer ----> ConsumerRecord -----> Even t-----> Sink

可以在 KafkaChannel 中配置生产者和消费者的参数。

![image-20200329104301974](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200329104301974.png)

② source + (interceptor) + KafkaChannel , source 将数据写入到 kafka 即可。之后在 app 端自己写消费者消费 kafka 中的数据。

source ----> Event ----> KafkaProducer ----> ProducerRecord ----> Kafka Topic

可以在 KafkaChannel 中配置生产者参数。

③ KafkaChannel + sink ，使用 sink 从 kafka 中将 event 写入到其他的目的地。

Kafka Topic <---- kafkaConsumer ----> ConsumerRecord ----> Event ----> Sink

可以在 KafkaChannel 中配置消费者参数。

**补充说明：**

**flume 的 event 实质上是一个 map；flume 的 body 实质上是一个字节数组**

**kafka 的 生产者生产的是一个 ProducerRecord 对象，有 topic、partition、headers、key、value、timestamp，其中 headers、key、value 都是框架定义的数据类型。需要特别注意的是，这里的 header 实为一个字符串数组，不同于 flume event 中的 header。**

**kafka 的 消费者消费的是一个 ConsumerRecord 对象，除了 ProducerRecord 具备的属性，还有 offset、timestampType、serializedKeySize、serializedValueSize、checksum等。**



#### 6.2.2.3 kafkaChannel 的使用

必选配置：

| **type**                    | –    | `org.apache.flume.channel.kafka.KafkaChannel` |
| --------------------------- | ---- | --------------------------------------------- |
| **kafka.bootstrap.servers** | –    | kafka集群的地址                               |

可选配置：

| kafka.topic             | flume-channel | 将 event 保存到 kafka 的哪个主题中                           |
| ----------------------- | ------------- | ------------------------------------------------------------ |
| kafka.consumer.group.id | flume         | 如果要启动消费者，消费者的组id                               |
| parseAsFlumeEvent       | true          | 默认为 true 将 flume 的 event 的 body 和 header 一起写入到 kafka 的 value，使用特殊分隔符进行分隔。如果为 false，只将 body 写入到 kafka。（注：该项配置在 flume 1.7 之前不起作用） |

#### 6.2.2.4 消费者和生产者的配置

① channel 层面的参数，例如 channel 的类型或 channel 的 capicity 等和之前配置一样，在channel 层面配置，例如：a1.channnels.c1.type=xxx

② 和 kafka 集群相关的参数，以及 channel 如何操控集群的参数，需要在 channel 的名称后添加kafka，例如：a1.channnels.c1.kafka.bootstrap.servers

③ 和生产者和消费者相关的属性，需要添加 kafka.producer or kafka.consumer 前缀，例如： 

a1.channnels.c1.kafka.producer.acks=-1

a1.channnels.c1.kafka.consumer.group.id=atguigu

## 6.3 编写拦截器

### 6.3.1 拦截器的作用

作用一：将日志中的数据按照不同的类型分别存储到 kafka 的不同主题中。TailDirsouce 将日志中的每一行数据封装为一个 Event。封装后，我们需要调用拦截器对 event 进行拦截，拦截后根据 event 的类型，在 header 中添加属性，指定了当前 event 应该发送到哪个主题。

作用二：在拦截器中对数据进行ETL操作，过滤掉不合法的数据。

​	启动日志：  {}

​			验证是否以{}开头和结尾。

​	埋点事件日志： xxxxx | {}

​			可以按照 | 进行切分，切分后，判断时间戳是否复合要求：

​				① 长度符合13位。②都是是数字。

​			判断事件日志是否以 {} 开头和结尾。

### 6.3.2 拦截器的实现

```java
package com.atguigu.warehouse.interceptor;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * Created by VULCAN on 2020/3/28
 */
public class MyInterceptor implements Interceptor {

    private String startFlag="\"en\":\"start\"";

    private List<Event> results=new ArrayList<>();

    //初始化
    @Override
    public void initialize() {

    }

    //核心拦截逻辑
    @Override
    public Event intercept(Event event) {

        //获取body
        byte[] body = event.getBody();
        //转换body为string类型
        String bodyStr = new String(body, Charset.forName("GBK"));

        //根据 日志数据的类型，在header中添加要发往的topic名
        Map<String, String> headers = event.getHeaders();

        //声明一个变量判断数据是否合法
        boolean isLegal=true;

        //判断当前的event中的数据，是否是启动日志
        if (bodyStr.contains(startFlag)){

            headers.put("topic","topic_start");

            //进行启动日志的ETL处理
             isLegal = ETLUtil.validStartLog(bodyStr);

        }else{
            //埋点事件日志
            headers.put("topic","topic_event");

            //进行埋点事件日志的ETL处理
            isLegal = ETLUtil.validEventLog(bodyStr);

        }

        //通过标记判断数据是否合法
        if (!isLegal){
            return null;
        }

        return event;
    }

    //拦截，建议调用intercept(Event event)
    @Override
    public List<Event> intercept(List<Event> events) {

        //先清空results
        results.clear();

        for (Event event : events) {

            Event e = intercept(event);

            //判断拦截的数据是否合法
            if (e !=null){

                //将合法的数据放入到集合中
                results.add(e);

            }

        }

        return results;
    }

    //关闭时调用
    @Override
    public void close() {

    }

    public static class Builder implements Interceptor.Builder {

        // 返回一个拦截器对象
        @Override
        public Interceptor build() {
            return new MyInterceptor();
        }

        //读取agent配置文件中的参数
        @Override
        public void configure(Context context) {

        }
    }
}

```

```java
package com.atguigu.warehouse.interceptor;

import org.apache.commons.lang.StringUtils;
import org.apache.commons.lang.math.NumberUtils;

/**
 * Created by VULCAN on 2020/3/28
 */
public class ETLUtil {

    //处理启动日志
    public static  boolean  validStartLog(String str){

        //先判空
        if (StringUtils.isBlank(str)){
            return false;
        }

        //去前后空格
        String trimStr = str.trim();

        //验证是否以{}开头和结尾
        if (trimStr.startsWith("{") && trimStr.endsWith("}")){
            return true;
        }

        return false;

    }

    //处理事件日志
    public static  boolean  validEventLog(String str){

        //先判空
        if (StringUtils.isBlank(str)){

            return false;

        }

        //去前后空格
        String trimStr = str.trim();
        //按照|进行切分
        String[] words = trimStr.split("\\|");

        //判断格式是否残缺
        if (words.length !=2){
            return false;
        }

        //判断时间戳是否合法 a) 长度复合13位 b)都是是数字
        // NumberUtils.isDigits(): 要求字符串中必须全部为数字符号0-9
        // NumberUtils.isNumbers(): 只要是java支持的数字类型即可
        if (words[0].length() !=13 || !NumberUtils.isDigits(words[0]))
        {
            return  false;
        }

        //验证是否以{}开头和结尾
        if (words[1].startsWith("{") && words[1].endsWith("}")){
            return true;
        }
        return false;
    }
}

```

### 6.3.3 拦截器的测试

```properties
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# 配置source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
#^代表以什么开头，$代表以什么结尾，.代表匹配\n除外任意单个字符,+代表多个
a1.sources.r1.filegroups.f1 = /tmp/logs/^app.+.log$
a1.sources.r1.positionFile=/home/atguigu/taildir_position.json

#拦截器的配置
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.warehouse.interceptor.MyInterceptor$Builder
# 配置sink
a1.sinks.k1.type = logger

# 配置channel
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000

# 绑定和连接组件
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动命令

```bash
bin/flume-ng agent -c conf/ -n a1 -f myagents/interceptortest.conf -Dflume.root.logger=DEBUG,console
```



## 6.4 采集通道 FlumeAgent 编写

### 6.4.1 组件

TailDirsouce ----> 拦截器----> 2 个 KafkaChannel（启动日志放一个主题，事件日志放一个主题）

1个 source 对接多个 channel，需要选择 channel 选择器 (默认 ReplicatingChannelSelector )。

应使用 MultiPlexingChannelSelector，根据 Event 中 header 的 topic 属性值将 event 分发到不同的 channel。

### 6.4.2 配置

```properties
a1.sources = r1
a1.channels = c1 c2

# 配置source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
#^代表以什么开头，$代表以什么结尾，.代表匹配\n除外任意单个字符,+代表多个
a1.sources.r1.filegroups.f1 = /tmp/logs/^app.+.log$
a1.sources.r1.positionFile=/home/atguigu/taildir_position.json

#拦截器的配置
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.warehouse.interceptor.MyInterceptor$Builder

#配置MultiplexingChannelSelector
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_event = c2
a1.sources.r1.selector.mapping.topic_start = c1


# 配置channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c1.kafka.topic=topic_start
a1.channels.c1.parseAsFlumeEvent=false

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic=topic_event
a1.channels.c2.parseAsFlumeEvent=false

# 绑定和连接组件
a1.sources.r1.channels = c1 c2
```

启动命令

```bash
bin/flume-ng agent -c conf/ -n a1 -f myagents/mylogkfk.conf
```



## 6.5 第一层采集通道启动脚本

### 5.1 nohup

nuhup 保证终端启动的进程在终端断开后，依然在后台运行。

### 5.2 xargs

如果要使用管道符 |，通常写法是  precmd | postcmd，要求 postcmd 必须能读取 precmd 命令的输出才可以。而 xargs 可以读取 | 之前的内容，作为参数传递给后面的命令。

### 5.3 脚本

```shell
#!/bin/bash
#第一层采集通道的一键启动脚本，只接收单个start或stop参数
if(($#!=1))
then
        echo 请输入单个start或stop参数！
        exit
fi


#对传入的单个参数进行校验，在hadoop102和hadoop103且执行第一层采集通道的启动和停止命令
if [ $1 = start ]
then
	cmd="nohup flume-ng agent -c $FLUME_HOME/conf -n a1 -f $FLUME_HOME/myagents/f1.conf -Dflume.root.logger=INFO,console > /home/atguigu/f1.log 2>&1 &"
elif [ $1 = stop ]
then
	cmd="ps -ef | grep f1.conf | grep -v grep | awk  '{print \$2}' | xargs kill "
else
	echo 请输入单个start或stop参数！
fi

#在hadoop102和hadoop103且执行第一层采集通道的启动和停止命令
for i in hadoop102 hadoop103
do
        echo "--------------$i-----------------"
        ssh $i $cmd
done
```

awk 语句参数详解：

```bash
awk  '{print \$2}'
# 等价于
awk  -F" " '{print \$2}'
# 等价于
awk  -F " " '{print \$2}'
```

-F 指定分隔符，分隔符可以紧跟F，不写空格。若分隔符为空格，则可以省略 F 部分。

'{print \$2}' 表示输出第二列，\ 转义用于避免同函数传参 $2 混淆。

另外，若不加 grep -v grep，使用正则表达式匹配的方式也可以实现，例如：

```bash
awk '/flume/{print $2}'
# 正则表达式支持逻辑运算（与、或、非）
```

除了 awk，使用 cut 也可以实现文本切割，不过不推荐。

```bash
ps -ef | grep logkfk.conf | grep -v grep | cut -d " " -f 5
```

## 6.6 调试思路

本环节的作用是从日志文件中将日志数据通过 flume，存储到 kafka 不同的 topic 中，因此可以从数据的源头一步一步进行排查。

① 保证 /tmp/logs 中有日志数据。

② TailDirSource 会将上次读取文件的位置记录到 taildir_position.json，将 taildir_position.json 中每个日志文件读取的位置和日志文件当前的位置进行对比，验证 source 是否正常工作。

③ 运行测试拦截器的 agent，保证拦截器可以将每个 Event 添加 topic=xxx 的 header。

④ 查看 kafka 的 topic 中有多少数据，record 的个数和日志文件的行数是否一致。查看日志，定位到报错的位置，进行修改。

另外，还可以借鉴二分查找的思想，以 kafkachannel 为界进行排查，逐步定位异常。

# 7. 第二层 Flume 采集通道

## 7.1 目的

​		从 kafka 中读取日志数据，将日志数据使用 hdfssink 写入到 hdfs 的不同的目录中。

## 7.2 KafkaSource

​		数据源是 Kafka，选择 Source 对接 Kafka，可以使用 KafkaSource。

### 7.2.1 简介

KafkaSource 本质是一个可以从 kafka 集群的主题上消费数据的消费者。如果希望提高消费者速率，可以配置多个 KafkaSource，指定多个 KafkaSource 有相同的组 id。

### 7.2.2 必须配置的参数：

| **type**                    | –     | `org.apache.flume.source.kafka.KafkaSource`                  |
| --------------------------- | ----- | ------------------------------------------------------------ |
| **kafka.bootstrap.servers** | –     | kafka集群的地址                                              |
| kafka.consumer.group.id     | flume | 组id                                                         |
| **kafka.topics**            | –     | 要消费哪些主题，多个主题使用逗号间隔                         |
| **kafka.topics.regex**      | –     | 使用正则匹配要消费的主题。较 kafka.topics 有更高的优先级。如果和 kafka.topics 同时存在，会覆盖 kafka.topics 中设置的值。 |

### 7.2.3 可选的参数

| useFlumeEventFormat | false | 必须和前驱 agent 的 parseAsFlumeEvent 值一致 |
| ------------------- | ----- | -------------------------------------------- |
| batchSize           | 1000  | 一次向 channel 中写入一批 event 的数量       |
| batchDurationMillis | 1000  | 一次 put 操作间隔的最大时间                  |

注意：与 kafka 消费者相关的参数需添加 kafka.consumer. 前缀，例如：

​										kafka.consumer.security.protocol=PLAINTEXT

注意：kafkaSource 会自动添加时间戳。

### 7.2.4 注意事项

- KafkaSource 默认取消自动提交 offset。因此 kafkaSource 有可能会造成数据的重复消费。
- KafkaSource 默认使用 StringSerializer 作为 key 的反序列化器；使用 ByteArraySerializer 作为 value 的反序列化器。

## 7.3 FileChannel

### 7.3.1 FileChannel 的简介

​		FileChannel 将 event 存储在文件中，比 memorychannel 可靠，但效率低。

### 7.3.2 配置

| **type**            | –                                | `file`                                                       |
| ------------------- | -------------------------------- | ------------------------------------------------------------ |
| checkpointDir       | ~/.flume/file-channel/checkpoint | 在 filechannel 中有 checkpoint 线程，负责检查文件中哪些 event 已经被 sink 消费走了。这个线程在工作时，需要记录一些信息，信息会保存在  checkpiont 文件中，此目录代表 checkpiont 文件保存的路径。 |
| useDualCheckpoints  | false                            | 是否备份 checkpoint 目录，如果启用，需要设置backupCheckpointDir。 |
| backupCheckpointDir | –                                | 备份 checkpoint 目录的路径。                                 |
| dataDirs            | ~/.flume/file-channel/data       | 数据文件存储的目录，多个目录使用逗号间隔。如果多个目录对应多块不同的磁盘，则可以提升性能。 |
| keep-alive          | 3                                | 允许一次 put 操作可以花费的总的时间，如果在这个时间内，put 操作没有完成，此次 put 的数据就回滚。 |

## 7.4 HDFSSink

### 7.4.1 简介

- HDFSSink 将 event 写入到 hdfs。支持两种文件类型，分别为 TEXT、SEQUENCEFILE，这两种文件类型都支持压缩。
- HDFSSink 可以配置文件自动滚动，基于时间、基于 event 的数量、基于文件的大小。
- HDFSSink 还可以指定写入到 hdfs 的路径使用转义序列，如果是基于时间的转义序列，要求event 的 header 中必须有属性 timestamp=时间戳，时间戳会自动替换时间的转义序列。如果 event 的 header 中没有此属性，可以指定 useLocalTimeStamp=true，默认会使用当前服务器所在的时间作为时间戳。
- HDFSSink 可以指定目录的滚动策略，原理是将 event 的时间戳向下舍到最高分/ 秒/ 时的倍数的时间。

### 7.4.2 常见的配置

```properties
# 配置sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoop102:9000/flume/%Y%m%d/%H%M
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = logs-
#滚动目录 一分钟滚动一次目录
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 1
a1.sinks.k1.hdfs.roundUnit = minute
#是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#配置文件滚动
a1.sinks.k1.hdfs.rollInterval = 30
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

#使用压缩格式存储数据
a1.sinks.k1.hdfs.fileType=CompressedStream 
#指定文件使用LZO压缩
a1.sinks.k1.hdfs.codeC=lzop

```

## 7.5 第二层 FlumeAgent 的编写

kafkaSource×2 + FileChannel×2 + HDFSSink×2

```properties
a1.sources = r1 r2
a1.channels = c1 c2
a1.sinks = k1 k2

# 配置source
a1.sources.r1.type =org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics = topic_start
a1.sources.r1.kafka.consumer.group.id=CG_start
# kafka消费者默认从分区的最后一个位置消费，当前分区中已经有170条数据，如果不配置，只会从HW之后消费
# 控制kafka消费者从主题的最早的位置消费，此参数只会在一个从未提交过offset的组中生效
a1.sources.r1.kafka.consumer.auto.offset.reset=earliest

a1.sources.r2.type =org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r2.kafka.topics = topic_event
a1.sources.r2.kafka.consumer.group.id=CG_event
a1.sources.r2.kafka.consumer.auto.offset.reset=earliest

# 配置channel
a1.channels.c1.type = file
a1.channels.c1.dataDirs = /opt/module/flumedata/c1
a1.channels.c1.checkpointDir=/opt/module/flumedata/c1checkpoint
a1.channels.c1.useDualCheckpoints=true
a1.channels.c1.backupCheckpointDir=/opt/module/flumedata/c1backupcheckpoint

a1.channels.c2.type = file
a1.channels.c2.dataDirs = /opt/module/flumedata/c2
a1.channels.c2.checkpointDir=/opt/module/flumedata/c2checkpoint
a1.channels.c2.useDualCheckpoints=true
a1.channels.c2.backupCheckpointDir=/opt/module/flumedata/c2backupcheckpoint

# 配置sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path=/origin_data/gmall/log/topic_start/%Y-%m-%d
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = startlog-
a1.sinks.k1.hdfs.batchSize=1000
#配置文件滚动
a1.sinks.k1.hdfs.rollInterval = 30
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
#使用压缩格式存储数据
a1.sinks.k1.hdfs.fileType=CompressedStream 
#指定文件使用LZO压缩
a1.sinks.k1.hdfs.codeC=lzop

a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path=/origin_data/gmall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = eventlog-
a1.sinks.k2.hdfs.batchSize=1000
a1.sinks.k2.hdfs.rollInterval = 30
a1.sinks.k2.hdfs.rollSize = 134217700
a1.sinks.k2.hdfs.rollCount = 0
a1.sinks.k2.hdfs.fileType=CompressedStream 
a1.sinks.k2.hdfs.codeC=lzop

# 绑定和连接组件
a1.sources.r1.channels = c1
a1.sources.r2.channels = c2
a1.sinks.k1.channel=c1
a1.sinks.k2.channel=c2
```

启动命令：

```bash
flume-ng agent -c conf/ -n a1 -f myagents/loghdfs.conf -Dflume.root.logger=DEBUG,console
```



## 7.6 调试思路

与第一层采用类似的思路，沿着 kafkaSource -> FileChannel -> HDFSSink 的顺序排查和定位。

① 查看 topic_start 和 topic_event 主题中是否有数据。

② 验证 kafkaSource -> FileChannel 是否有问题。通过查看消费者的 lag 延迟判断，如果 lag=0，则说明已经消费到了数据；如果 lag!=0，则 kafkaSource 有问题。

也可以再写一个测试的 agent，使用 kafkaSource -> FileChanne -> LoggerSink 将 agent 的日志打印到屏幕上，方便调试。

注意：每次消费到数据后，只会从之前已经提交 offset 的最新位置继续消费，也可以通过重置 offset 来实现。

③ 验证 FileChannel -> HDFSSink，提高日志级别为 WARN，通过查看日志进行调试。



## 7.7 编写第二层采集通道的启动脚本

```shell
#!/bin/bash
#第二层采集通道的一键启动脚本，只接收单个start或stop参数
if(($#!=1))
then
        echo 请输入单个start或stop参数！
        exit
fi

#对传入的单个参数进行校验，在hadoop102和hadoop103且执行第一层采集通道的启动和停止命令
if [ $1 = start ]
then
        ssh hadoop104 "nohup flume-ng agent -c $FLUME_HOME/conf -n a1 -f $FLUME_HOME/myagents/loghdfs.conf -Dflume.root.logger=INFO,console > /home/atguigu/fa2.log 2>&1 &"
        elif [ $1 = stop ]
        then
                ssh hadoop104 "ps -ef | grep f2.conf | grep -v grep | awk  '{print \$2}' | xargs kill "
else
        echo 请输入单个start或stop参数！
fi

```



## 7.8 用户行为数据采集单元群起脚本

```shell
#!/bin/bash
#校验参数是否合法
if(($#!=1))
then
	echo 请输入单个start/stop作为参数
	exit;
fi

#判断kafka进程是否运行
function countKafkaBrokers(){
	count=0
	for((i=102;i<=104;i++))
	do
		result=$(ssh hadoop$i "jps | grep Kafka | wc -l")
		count=$[$count+$result]
	done
	return $count
}

if [ $1 = start ]
then
	zk.sh start
	kfk.sh start
	hd.sh start
	while [ 1 ]
	do
		countKafkaBrokers
		if(($?==3))
		then
			break
		fi
		sleep 2s
	done
	fa1.sh start
	fa2.sh start
	xcall jps
elif [ $1 = stop ]
then
	fa1.sh stop
	fa2.sh stop
	kfk.sh stop
	while [ 1 ]
        do
                countKafkaBrokers
                if(($?==0))
                then
                        break
                fi
                sleep 2s
        done
	zk.sh stop
	hd.sh stop
	xcall jps
else
	echo 请输入单个start/stop作为参数
fi

```

## 7.9 模拟造数据

遵循的原则： 将当前机器的时间修改为过去的时间，启动采集通道。如果希望造未来时间的数据，将时间向未来同步后造数据，不需要重新集群。

举例： 如果希望造3月29日的数据，3月22日的数据，3月31日的数据，4月15日的数据，需要将集群的时间，同步到3月22日之前，启动采集通道。依次执行 dt.sh 2020-03-22；log.sh；dt.sh 2020-03-29；log.sh；dt.sh 2020-03-31；log.sh。

## 7.10 时间戳解析拦截器 FlumeAgent

前面写的第二层 FlumeAgent 中，HDFSsink 最终采用的时间是 kafkaSource 所在服务器的时间，也即 HDFSsink 所在服务器的时间，以该时间为参照将日志存入指定文件夹，与日志的内部时间戳略有偏差。为了精确地分配文件夹，考虑在第二次的 source 中设置自定义拦截器，解析出 event 内的时间戳，并将其加入 header。

自定义拦截器的代码如下：

```java
package com.atguigu.bigdata;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONObject;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;
import java.nio.charset.Charset;
import java.util.List;
import java.util.Map;

public class MyTimeStampInterceptor implements Interceptor {
    private String startFlag="\"en\":\"start\"";
    //初始化
    @Override
    public void initialize() {
    }

    //核心拦截逻辑
    @Override
    public Event intercept(Event event) {
        //获取body
        byte[] body = event.getBody();
        //转换body为string类型
        String bodyStr = new String(body, Charset.forName("GBK"));
        //根据 日志数据的类型，在header中添加要发往的topic名
        Map<String, String> headers = event.getHeaders();

        //判断当前的event中的数据，是否是启动日志
        if (bodyStr.contains(startFlag)){
            JSONObject jsonObject = JSON.parseObject(bodyStr);
            String ts = jsonObject.getString("t");
            headers.put("topic","topic_start");
            headers.put("timestamp",ts);
        }else{
            String[] words = bodyStr.split("\\|");
            //埋点事件日志
            headers.put("topic","topic_event");
            headers.put("timestamp",words[0]);
        }

        return event;
    }

    //拦截，建议调用intercept(Event event)
    @Override
    public List<Event> intercept(List<Event> events) {
        for (Event event : events) {
            intercept(event);
        }
        return events;
    }

    //关闭时调用
    @Override
    public void close() {
    }

    public static class Builder implements Interceptor.Builder {
        // 返回一个拦截器对象
        @Override
        public Interceptor build() {
            return new MyTimeStampInterceptor();
        }
        //读取agent配置文件中的参数
        @Override
        public void configure(Context context) {
        }
    }
}
```

FlumeAgent 配置如下：

```properties
a1.sources = r1
a1.channels = c1 c2
a1.sinks = k1 k2

# 配置source
a1.sources.r1.type =org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics = topic_start,topic_event
#kafka消费者默认从分区的最后一个位置消费，当前分区中已经有170条数据，如果不配置，只会从170之后消费
#控制kafka消费者从主题的最早的位置消费，此参数只会在一个从未提交过offset的组中生效
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.atguigu.bigdata.MyTimeStampInterceptor$Builder

#配置MultiplexingChannelSelector
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_event = c2
a1.sources.r1.selector.mapping.topic_start = c1

# 配置channel
a1.channels.c1.type = file
a1.channels.c1.dataDirs = /opt/module/flumedata/c3
a1.channels.c1.checkpointDir=/opt/module/flumedata/c3checkpoint
a1.channels.c1.useDualCheckpoints=true
a1.channels.c1.backupCheckpointDir=/opt/module/flumedata/c3backupcheckpoint

a1.channels.c2.type = file
a1.channels.c2.dataDirs = /opt/module/flumedata/c4
a1.channels.c2.checkpointDir=/opt/module/flumedata/c4checkpoint
a1.channels.c2.useDualCheckpoints=true
a1.channels.c2.backupCheckpointDir=/opt/module/flumedata/c4backupcheckpoint

# 配置sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path=/origin_data/gmall/log/topic_start/%Y-%m-%d
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = startlog-
a1.sinks.k1.hdfs.batchSize=1000
#配置文件滚动
a1.sinks.k1.hdfs.rollInterval = 30
a1.sinks.k1.hdfs.rollSize = 134217700
a1.sinks.k1.hdfs.rollCount = 0
#使用压缩格式存储数据
a1.sinks.k1.hdfs.fileType=CompressedStream 
#指定文件使用LZO压缩
a1.sinks.k1.hdfs.codeC=lzop

a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path=/origin_data/gmall/log/topic_event/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix = eventlog-
a1.sinks.k2.hdfs.batchSize=1000
a1.sinks.k2.hdfs.rollInterval = 30
a1.sinks.k2.hdfs.rollSize = 134217700
a1.sinks.k2.hdfs.rollCount = 0
a1.sinks.k2.hdfs.fileType=CompressedStream 
a1.sinks.k2.hdfs.codeC=lzop

# 绑定和连接组件
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel=c1
a1.sinks.k2.channel=c2

```

