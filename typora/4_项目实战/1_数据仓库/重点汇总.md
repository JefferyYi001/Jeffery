# 1.和数仓相关的概念

## 1.1 什么是数仓

把企业的数据进行收集和统一管理，为企业进行战略分析与决策提供数据支持的一套系统。

## 1.2 你们公司数仓的数据的输入和输出分别是什么？

输入：用户日志数据、业务系统数据、爬虫系统数据。

输出：报表系统、用户画像系统、推荐系统、机器学习系统、风控系统。

## 1.3 你们公司数仓的每天的数据量有多少？集群规模是什么？用的是哪种平台（阿里云/物理机）？用了哪些版本的框架？

（1）每天日活跃用户100万，每人一天平均100条：100万×100条=1亿条；每条日志1K左右，每天1亿条：100000000 / 1024 / 1024 = 约100G；

（2）半年内不扩容服务器来算：100G×180天=约18T；保存3副本：18T×3=54T；预留20%~30%Buf=54T/0.7=77T；算到这：约8T*10台服务器。

（3）数仓用的是物理机。

（4）框架版本：

| 产品      | Apache 框架版本 | CDH 框架版本 |
| --------- | --------------- | ------------ |
| hadoop    | 2.7.2           | 2.6.0        |
| flume     | 1.7.0           | 1.6.0        |
| kafka     | 0.11.0.2        |              |
| hive      | 2.3.6           | 1.1.0        |
| sqoop     | 1.4.6           | 1.4.6        |
| MySQL     | 5.6.24          |              |
| azkaban   | 2.5.0           |              |
| java      | 1.8             |              |
| zookeeper | 3.4.10          | 3.4.5        |
| presto    | 0.189           |              |
| spark     |                 | 1.6.0        |
| oozie     |                 | 4.1.0        |
| impala    |                 | 2.9.0        |

# 2. Linux 相关

## 2.1 写出5个常用的Linux命令

| 序号 | 命令                           | 命令解释                                |
| ---- | ------------------------------ | --------------------------------------- |
| 1    | top                            | 查看内存                                |
| 2    | df -h                          | 查看磁盘存储情况                        |
| 3    | iotop                          | 查看磁盘IO读写(yum  install iotop安装） |
| 4    | iotop -o                       | 直接查看比较高的磁盘读写程序            |
| 5    | netstat -tunlp \|  grep 端口号 | 查看端口占用情况                        |
| 6    | uptime                         | 查看报告系统运行时长及平均负载          |
| 7    | ps aux                         | 查看进程                                |

## 2.2 awk, sed, cut, sort, wc 的参数

### 2.2.1 awk

```bash
awk  -F" " '/正则表达式/{print $2}'
-F：指定分隔符，分隔符可以紧跟F，不写空格。若分隔符为空格，则可以省略 F 部分。
'/正则表达式/{print $2}' 表示输出匹配到正则表达式的第二列。
$2：待输出的列序号。该位置可以是 awk 内置变量：FILENAME（文件名）、NR（已读记录数）、NF（切割后列数）中的一个或多个。
awk -v i=1 -F: '{print $3+i}' passwd
-v：定义一个用户自定义变量
案例：
awk '/^$/{print NR}' file1 //输出空行行号
awk -v sum=0 -F " " '{sum+=$2} END{print sum}' chengji.txt //成绩累加
```

### 2.2.2 sed

```bash
注意：sed 默认不改变文件内容，若需改变，用 -i
sed '2a mei nv' sed.txt
a：在第2行下方插入“mei nv”
sed '/wo/d' sed.txt
d：删除“wo”
sed 's/wo/ni/g' sed.txt
s：查找并替换
sed -e '2d' -e 's/wo/ni/g' sed.txt
-e：直接在指令列模式上进行sed的动作编辑。（-i：直接编辑文件）
```

### 2.2.3 cut

```bash
cut -d " " -f 5
-d：指定分隔符
-f：指定输出列号
```

### 2.2.4 sort

```bash
sort -t : -nrk 3  sort.sh
-n：依照数值的大小排序
-r：以相反的顺序来排序
-k：指定需要排序的列序号
-t：设置排序时所用的分隔字符
```

### 2.2.5 wc

```bash
wc -c passwd
-c：显示文件字节数
-l：显示文件行数
-w：显示字数
```



# 3. Hadoop相关

## 3.1 hadoop 基础

### 1. 简要描述如何安装配置 apache 的一个开源 Hadoop。

① 根据Hadoop版本安装匹配的JDK，配置JAVA_HOME。

② 解压安装Hadoop，配置HADOOP_HOME，将bin/sbin加入到path，方便以后使用。

③ 配置hadoop的配置文件，core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml。

④ 如果为完全分布式的集群，则需要配置所有机器的hosts映射信息，配置rm所在机器到其他机器的ssh免密登录。

⑤ 在RM所在主机编辑$HADOOP_HOME/etc/hadoop/slave文件，配置集群中的所有的主机名。

⑥ 分发安装的hadoop到其他节点。

### 2. Hadoop中需要哪些配置文件，其作用是什么？

xxx-env.sh：配置hadoop中各个组件运行的环境信息。

core-site.xml： 用户自定义核心组件，例如namenode的rpc地址。

hdfs-site.xml： 用户自定义和hdfs相关的参数。

mapred-site.xml： 用户自定义和mapreduce相关的参数。

yarn-site.xml： 用户自定义和yarn相关的参数。

### 3. 请列出正常工作的Hadoop集群中Hadoop都分别需要启动哪些进程，它们的作用分别是什么?

ResourceManager：负责整个集群中所有计算资源（cpu,内存，IO，硬盘）的管理!

NodeManager：负责单个节点中所有计算资源（cpu,内存，IO，硬盘）的管理!领取RM中的Task任务，分配container运行Task

Namenode：负责HDFS中元数据的管理和处理客户端的请求

Datanode:以块为单位存储HDFS中文件。

SecondaryNamenode: 帮助namenode定期合并fsimage文件和edits文件，HA中可以省略此进程！

### 4. 简述Hadoop的几个默认端口及其含义。

| 50070 | namenode的http服务端口           |
| ----- | -------------------------------- |
| 9000  | namenode的接受客户端rpc调用端口  |
| 8088  | yarn的http服务端口               |
| 19888 | MR运行历史收集服务的http服务端口 |
| 8042  | nodemanager的http服务端口        |

 

## 3.2 HDFS

### 1. HDFS的存储机制（读写流程）

写流程：

![image-20200416191422722](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200416191422722.png)

① 客户端创建一个分布式文件系统客户端对象，向nn发送请求，请求上传文件。

② NN处理请求，对请求进行合法性检查（权限，文件路径是否存在）。请求合法后，响应客户端，通知进行写操作。

③ 客户端创建一个输出流，输出流在写文件时，以块(128M)为单位，块又由packet(64k)作为基本单位，packet由多个chunk(512B+4B校验位)组成。

④ 开始第一块的上传，在上传时，会请求NN，根据网络拓扑距离，和上传的副本数，分配指定数量的距离客户端最近的 Datanode 节点列表。

⑤ 客户端请求距离最近的一个DN建立通道，DN列表中的DN依次请求建立通道。全部通道建立完成，开始传输。客户端将第一个块的0-128M信息，以packet形式进行封装，将封装好的packet放入 data_queue的队列中。输出流在传输时，会新建一个ack_queue，将data_queue要传输的packet依次放入ack_queue中。

⑥ 客户端只负责将当前的packet发送给距离最近的DN，DN会在收到packet后进行存盘，并将packet依次发往下一个DN。当所有的DN在收到当前的packet后，向客户端的流对象发送ack指令，当ack_queue中的packet已经被所有的DN收到，那么在当前队列中就会删除此packet。

⑦ 第一块上传完毕后，会上报NN（当前块已经成功发送到了哪些DN上），并开始传输第二块（128M-...），和第一块的流程一样。

⑧ 当所有的数据都上传完毕，关闭流，等待NN的一个响应。

![image-20200416191548729](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200416191548729.png)

读流程：

① 客户端向NN发送请求，请求读取指定路径的文件。

② NN处理请求，返回当前文件所有的块列表信息。

③ 客户端创建一个输入流，根据块的信息，从第一块开始读取，根据拓扑距离选择距离最近的一个节点进行读取，剩余块依次读取。

④ 所有块信息读取完毕，关闭流。

 

### 2. SecondaryNameNode 工作机制

![image-20200416191706608](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200416191706608.png)

2NN和NN不是主从关系，是两个不同的进程。2NN负责辅助NN工作，定期合并NN中产生的edits文件和fsimage文件。

第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。客户端对元数据进行增删改时，NameNode首先记录操作日志，更新滚动日志，再在内存中对元数据进行增删改。

2NN基于两个触发条件执行CheckPoint(合并)，每间隔 dfs.namenode.checkpoint.period 秒合并一次，默认为1小时，每间隔 dfs.namenode.checkpoint.txns 次合并一次，默认为100w。

2NN默认每间隔60秒向NN发送请求，判断CheckPoint的条件是否满足，如果满足，向NN发送请求立刻滚动日志（产生一个新的日志，之后的操作都向新日志写入）。将历史日志和fsiamge文件拷贝到2NN工作目录中，加载到内存进行合并。合并后，将新的fsiamge文件，传输给NN，覆盖老的fsiamge文件。

### 3. NameNode与SecondaryNameNode 的区别与联系

联系：

① 2NN需要配合NN工作，NN启动，2NN工作才有意义。

② 2NN可能会保存部分和NN一致的元数据，可以用来NN的容灾恢复。

区别：

这是两个功能不同的进程，不是主备关系。

### 4. 服役新数据节点和退役旧节点步骤

服役新节点： 加机器，安装软件，配置环境，启动进程。

退役旧节点： 使用黑白名单。

### 5. NameNode 元数据损坏应对

① 如果配置了NN的多目录配置（在 hdfs-site.xml 中配置了 dfs.namenode.name.dir 为多目录），还是可以照常启动。

② 如果多个目录的元数据都损坏，可以查看是否启用了HA，或者查看是否启用2NN。

③ 可以通过另外一个NN或者2NN中的元数据进行恢复。

### 6. HDFS 调优参数

① dfs.namenode.handler.count=20 * log2(Cluster Size)，比如集群规模为8台时，此参数设置为60。该参数默认值为 10。

② 参数优先级排序：（1）客户端代码中设置的值 >（2）ClassPath下的用户自定义配置文件 >（3）服务器的site配置 > （4）服务器的框架默认配置

## 3.3 MapReduce

### 1. Hadoop序列化和反序列化及自定义bean对象实现序列化

① 在Hadoop中如果有reduce阶段，那么需要将Mapper和Reducer中的key-value实现序列化。

② Hadoop采用的是自己的序列化机制，即Writeable机制，是一种轻量级的序列化机制，保存的信息少，适合大数据量的网络传输。

③ 无论是Map还是Reduce阶段，key-value需要实现Writeable接口即可，重写readFileds()和write()方法即可。

### 2. FileInputFormat 切片机制

① 将输入目录中的所有文件，以文件为单位进行切片。

② 根据 isSplitable() 方法，以文件的后缀名为依据，判断文件是否使用了压缩格式。如果是普通文件，可切；如果是压缩文件，则判断是否是一个可切的压缩格式。之后在 getSplits() 方法中对其进行切片。

③ 如果文件不可切，整个作为一片。

④ 可切，确定每片的大小（默认是块大小），之后以此大小为根据，循环进行切片。

⑤ 除了最后一片有可能为切片大小的1.1倍，其余每片切片大小为片大小。

### 3.    自定义 InputFormat 流程

需要继承 InputFormat 类，通常可以继承 FileInputFormat 以节省方法的实现。

① 如果需要实现自定义的切片逻辑，实现或重写 getSplits() 方法。

② 实现 createRecordReader() 方法，返回一个 RecordReader 对象。

③ RecordReader 负责将切片中的数据以 key-value 形式读入到 Mapper 中。

④ 其核心方法是 nextKeyValue()，这个方法负责获取一对 key-value，有则返回 true，否则返回 false。

⑤ 可以根据需要实现 isSplitable() 方法。

### 4.    确定一个 job 的 map 和 reduce 的数量

maptask 数量取决于切片数，可以通过调整切片的大小和类型，来控制 map 的数量。

reducetask 数量取决于Job.setNumReduceTasks() 的值。

### 5.    MapTask 工作机制

① Map 阶段：使用 InputFormat 中的 RecordReader 读取切片中的信息并封装为 key-value。每一对 key-value 都会调用 mapper 的 map() 方法进行处理。

② Sort 阶段：在 Mapper 的 map() 方法处理后，输出的 key-value 会先进行分区，之后被收集到缓冲区，当缓冲区达到一定的溢写阀值时，每个区的 key-value 会进行排序，之后溢写到磁盘。多次溢写的文件，最后会进行合并为一个总的文件，这个文件包含若干区，而且每个区内都是有序的。

### 6.    ReduceTask 工作机制

① copy阶段：ReduceTask 启动 shuffle 进程，到指定的 maptask 拷贝指定区的数据。拷贝之后，会进行合并，合并为一个总的文件。

② sort阶段：在合并时，保证所有的数据合并后都是有序的，所以会进行归并排序。

③ reduce阶段：合并后的数据，会进行分组，每一组数据，调用 reducer 的 reduce() 方法，之后reduce 通过 OutputFormat 的 RecordWriter 将数据输出。

### 7.    请描述 mapReduce 有几种排序及排序发生的阶段

两种排序： 快速排序，归并排序

MapTask 处理的阶段：每次溢写前，进行快速排序。最后合并时，使用归并排序。

ReduceTask 处理的阶段：在 sort 和 merge 时，使用归并排序。

### 8.    请描述 mapReduce 中 shuffle 阶段的工作流程，如何优化 shuffle 阶段

shuffle阶段：从 mapper 的 map() 处理完，到 reducer 的 reduce() 方法前属于 shuffle。

工作流程：sort() — copy() — sort()

优化：本质就是减少磁盘IO（减少溢写次数和减少每次溢写的数据量）和网络IO（减少网络数据传输量）

MapTask端优化：

① map 端减少溢写次数：调大mapreduce.task.io.sort.mb 和mapreduce.map.sort.spill.percent。

② map 端减少合并的次数：调大 io.sort.factor。

③ 在合适的情况下使用 Combiner 对数据在 map 端进行局部合并。

④ 使用压缩，减少数据传输量。

ReduceTask端优化：

⑤ reduce 端减少溢写次数：调大 mapred.job.reduce.input.buffer.percent

### 9.    请描述 mapReduce 中 combiner 的作用是什么，一般使用情景，哪些情况不需要，及和 reduce 的区别。

① 作用就是在每次溢写数据到磁盘时，对数据进行局部合并，减少溢写数据量。

② 求和，汇总等场景适用，不适合使用的场景例如求平均数之类。

③ 和 reducer 的唯一区别，就是 Combiner 运行在 shuffle 阶段，且主要是 MapTask 端的 shuffle 阶段，而 reducer 运行在 reduce 阶段。

### 10.  如果没有定义 partitioner，那数据在被送达 reducer 前是如何被分区的？

① 如果 reduceTask 个数为1，那么所有的 key-value 都是0号区。

② 如果 reduceTask 个数大于1，默认使用 HashPartitioner，根据 key 的 hashcode 和 Interger 最大值做&运算，之后模除 reduceTask 的个数。

③ 所有数据的区号介于 0 和 reduceTask 个数 -1 的范围内。

### 11.  MapReduce 如何实现 TopN

（1）在 Map 端对数据，根据排名的字段，进行排序：

​		① 合理设置Map的key，key中需要包含排序的字段

​		② 通过使 key 实现 WritableComparable 接口或者自定义 key 的 RawComparator 类型比较器。归根到底，在排序时都是使用用户实现的 compareTo() 方法进行比较。 

（2）在reduce端，输出数据：

​		③ reduce 端处理的数据已经自动排序完成，只需要控制输出N个 key-value 即可。

### 12.  有可能使 Hadoop 任务输出到多个目录中么？如果可以，怎么做？

可以，通过自定义 OutPutFormat 进行实现。核心是实现 OutPutFormat 中的 RecordWriter () 方法获取自定义的 RecordWriter。通过实现 RecordWriter 中的 write() 方法按照需求将数据输出到指定目录。

### 13.  简述 hadoop 实现 join 的几种方法及每种方法的实现方法

ReduceJoin：在 Map 阶段，对所有的输入文件进行组装，打标记输出。到 reduce 阶段只处理需要 join 的字段，进行合并即可。

MapJoin：在Map阶段，将小文件以分布式缓存的形式进行存储。在 Mapper 的 map() 方法处理前，读取小文件的内容，和大文件进行合并即可，不需要有 reduce 阶段。

### 14.  请简述 hadoop 怎样实现二级排序

key 实现 WritableComparable() 接口，实现 CompareTo() 方法，先根据一个字段比较，如果当前字段相等继续按照另一个字段进行比较。

### 15.  参考下面的MR系统的场景:

-- hdfs块的大小为64MB

-- 输入类型为FileInputFormat

-- 有三个文件的大小分别是:64KB 65MB 127MB

请问Hadoop框架会把这些文件切多少片？

-- 4片

### 16.  Hadoop 中 RecordReader 的作用是什么？

读取每一片中的记录为 key-value，传给 Mapper。

### 17.  给你一个1G的数据文件。分别有 id、name、mark、source 四个字段，按照 mark 分组，id 排序。简述排序的核心逻辑思路，其中启动几个 MapTask？

① Map 阶段 key 的比较器，使用根据 mark 和 id 进行二次排序。

② Reduce 阶段调用分组比较器，根据 mark 进行比较，mark 相同视为 key 相同。

③ 默认启动8个 MapTask。

## 3.4 YARN

### 1. 简述 Hadoop1 与 Hadoop2 架构的异同

① Hadoop1 使用 JobTracker 调度MR的运行。

② Hadoop2 提供 YARN 框架进行资源的调度。

③ Hadoop2 支持HA集群搭建。

### 2. 为什么会产生yarn，它解决了什么问题，有什么优势？

① YARN为了将MR编程模型和资源调度分层解耦。

② 使用YARN后软件维护方便，YARN还可以为其他的计算框架例如spark等提供资源的调度。

### 3. MR作业提交全过程

![image-20200417083000814](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200417083000814.png)

​	（1）MR程序提交到客户端所在的节点。

​    （2）YarnRunner向ResourceManager申请一个Application。

​    （3）RM将该应用程序的资源路径返回给YarnRunner。

​    （4）该程序将运行所需资源提交到HDFS上。

​    （5）程序资源提交完毕后，申请运行mrAppMaster。

​    （6）RM将用户的请求初始化成一个Task。

​    （7）其中一个NodeManager领取到Task任务。

​    （8）该NodeManager创建容器Container，并产生MRAppmaster。

​    （9）Container从HDFS上拷贝资源到本地。

​    （10）MRAppmaster向RM 申请运行MapTask资源。

​    （11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。

​    （12）MRAppmaster向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。

​	（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。

​    （14）ReduceTask向MapTask获取相应分区的数据。

​    （15）程序运行完毕后，MRAppmaster会向RM申请注销自己。

### 4. HDFS的数据压缩算法？

系统自带：deflate、gzip 和 bzip2

额外安装：lzo、snappy

压缩率高：bzip2

速度快：snappy，lzo

可切片的：lzo，bzip2

使用麻烦的：lzo

### 5. Hadoop 的调度器总结

（1）FIFO 调度器：

单队列，按照 Job 提交的顺序先进先出，容易出现单个用户的大 Job 独占资源，而其他用户的小 Job 无法被及时处理的情况。

（2）容量调度器： 

​	① 多个队列，队列内部 FIFO，每个队列可以指定容量。

​	② 资源利用率高，处理灵活，空闲队列的资源可以补充到繁忙队列。

​	③ 可以设置单个用户的资源限制，防止单个用户独占资源。

​	④ 动态调整，维护方便。

（3）公平调度器：

在容量调度器的基础上，改变了 FIFO 的调度策略，默认参考集群中内存资源使用最大最小公平算法，保证小 Job 可以及时处理，大 Job 不至于饿死，小 Job 有优势。

### 6. mapreduce 推测执行算法及原理。

![image-20200417171504004](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200417171504004.png)

## 3.5 hadoop 优化

### 1. mapreduce 跑的慢的原因

① Task 运行申请的资源少，可以通过调节相关参数解决。

② 程序逻辑复杂，可以将一个复杂逻辑拆分为多个 Job，串行执行。

③ 产生了数据倾斜，可以通过合理设置切片策略和设置分区及调节 reducetask 数量解决。

④ shuffle 过程漫长，可以通过合理使用 combiner，使用压缩，调大 Map 端缓冲区大小等解决。

### 2. mapreduce 优化方法

详见 Hadoop（MapReduce）资料的第 6 章 6.2。常见调优参数如下：

① yarn.nodemanager.resource.memory-mb：表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。

② yarn.scheduler.maximum-allocation-mb：RM 可以给应用程序 Container分 配的最大内存，默认是8192（MB）。

③ mapreduce.map.memory.mb：一个 MapTask 可使用的资源上限（单位:MB），默认为1024。如果MapTask实际使用的资源量超过该值，则会被强制杀死。

④ mapreduce.reduce.memory.mb：一个 ReduceTask 可使用的资源上限（单位:MB），默认为1024。如果 ReduceTask 实际使用的资源量超过该值，则会被强制杀死。

⑤ mapreduce.map.cpu.vcores：每个MapTask可使用的最多cpu core数目，默认值: 1。

⑥ mapreduce.reduce.cpu.vcores：每个ReduceTask可使用的最多cpu core数目，默认值: 1。

⑦ mapreduce.reduce.input.buffer.percent：指定多少比例的内存用来存放Buffer中的数据，默认值是0.0。

⑧ mapreduce.reduce.shuffle.input.buffer.percent：Buffer大小占Reduce可用内存的比例。默认值0.7。

⑨ mapreduce.reduce.shuffle.merge.percent：Buffer中的数据达到多少比例开始写入磁盘。默认值0.66。

⑩ mapreduce.reduce.shuffle.parallelcopies：每个Reduce去Map中取数据的并行数。默认值是5



### 3. HDFS小文件优化方法

① 在源头处理，就小文件压缩或打包。

② 使用 Har 进行归档，Har 归档后的文件只能节省 Namenode 的内存空间，在进行MR计算时，依然以小文件的形式存在。

③ 使用 CombineTextInputFormat。

④ 使用紧凑的文件格式例如 SequeceFile。

### 4. MapReduce 怎么解决数据均衡问题，如何确定分区号？

① Map 端避免数据倾斜：抽样数据；避免有不可切分的数据；小文件过多，使用CombineTextInputFormat。

② Reduce 端避免数据倾斜：抽样数据，合理设置数据的分区，合理设置 reduceTask 的个数。

③ 继承 Partitioner 并重写其 getPartition() 方法以确定分区号。

### 5. Hadoop 中 job 和 Tasks 之间的区别是什么？

一个 Job 在运行期间，会启动多个 task 来完成每个阶段的具体任务。



# 4. Zookeeper相关



## 4.1 zk 基于的 paxos 协议，集群半数可用，Leader 选举机制。

① 只有 zookeeper 以集群模式启动时，需要选举 Leader

② 集群中 leader 在集群启动时，选举产生或者是 leader 挂掉，集群中的其他机器重新选举产生

③ 满足半数以上机制，按照启动顺序，id 大的有优势。



## 4.2 zk 的读写流程

![image-20200418225438218](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200418225438218.png)

ZooKeeper 的写数据流程主要分为以下几步：

① 比如 Client 向 ZooKeeper 的 Server1 上写数据，发送一个写请求。

② 如果 Server1 不是 Leader，那么 Server1 会把接受到的请求进一步转发给 Leader，因为每个ZooKeeper 的 Server 里面有一个是 Leader。这个 Leader 会将写请求广播给各个 Server，比如Server1 和 Server2， 各个 Server 写成功后就会通知 Leader。

③ 当 Leader 收到大多数 Server 数据写成功了，那么就说明数据写成功了。如果这里三个节点的话，只要有两个节点数据写成功了，那么就认为数据写成功了。写成功之后，Leader 会告诉 Server1 数据写成功了。

④ Server1 会进一步通知 Client 数据写成功了，这时就认为整个写操作成功。ZooKeeper 整个写数据流程就是这样的。

## 4.3 zk 的常见操作 

rmr, ls, ls2, stat, get, create, set 

## 4.4 zk 的监听器原理

① 首先要有一个 main() 线程，在 main 线程中创建 Zookeeper 客户端。这时就会创建两个线程，一个负责和 zookeeper 服务端通信（connet），一个负责监听 zookeeper 服务端的通知（listener）。

② 通过 connect 线程将注册的监听事件发送给 Zookeeper。

③ 在 Zookeeper 的注册监听器列表中将该客户端注册的监听事件添加到列表中。

④ Zookeeper 监听到有数据或路径变化，通知 listener 线程。

⑤ listener 线程内部调用了 Watcher 的 process() 方法。

# 5. Flume 相关

## 5.1 flume 中的组件以及 event 的数据流传输过程

![image-20200325104412150](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200325104412150.png)

flume 的组件有 source, channel, sink, interceptor, channel selector, sink processor

source：对接不同的数据源，将数据封装为 event，传输给 channel。

sink：负责从 channel 中获取 event，将 event 写入到指定的目标。

channel：介于 source 和 sink 中的一个缓冲，负责临时存储 event。

例：taildirSource, spoolingDirSouece, hdfssink, memeryChannnel, FileChannel, KafkaChannel

## 5.2 Flume 的 put 事务和 take 事务

event 从 source 到 channel 为 put 事务。put 事务在一批 event 被拦截器处理后，准备存储到 channel 时开启事务，全部存储完毕后提交事务，如果失败则回滚事务。

event 从 channel 到 sink 为 take 事务。同理，sink 开始写入数据时开启事务，一批 event 被 sink 从 channel 中全部写入到指定目标后提交事务，然后清除在 channel 中存储的 event。发生异常时则回滚事务，保证 event 的安全。

### 5.2.1 事务中的数量关系

batchSize：每个 source 和 sink 都可以配置 batchSize，batchSize 代表一批数据的数量。

transcationCapacity：putList 和 takeList 的大小（在channel的参数中配置）。

capacity：指 channel 中存储 event 的容量。

**batchSize <= transcationCapacity <= capacity**

## 5.3 基于 JMX 的 flume 数据传输监控

JMXTrans + InfluxDB + Grafana

## 5.4 flume 存在的丢数据风险

memeryChannnel 在 agent 故障时丢数据。

使用异步 Source，在客户端无法缓存数据，也会丢数据，典型的 ExecSource。

## 5.5 flume 存在的数据重复风险

put 时，如果一个 source 对接多个 channel，可能出现一批数据中某些 channel put 完了，但是另一些失败了。此时 channel Processor 会重试对此批数据的 put，包括之前已经写成功的 channel。此时，之前写成功的 channel 就会出现重复。

take 时，假如一个事务中，一部分 event 已经写入到目的地，但是随着事务的回滚，这些 event 可能重复写入。

使用 TailDirSource 时，如果不注意文件命名方式，可能会出现数据重复的情况。

## 5.6 Flume 参数调优

① Source

增加Source个数（使用Tair Dir Source时可增加FileGroups个数）可以增大Source的读取数据的能力。例如：当某一个目录产生的文件过多时需要将这个文件目录拆分成多个文件目录，同时配置好多个Source 以保证Source有足够的能力获取到新产生的数据。

batchSize 参数决定 Source 一次批量运输到Channel的event条数，适当调大这个参数可以提高Source 搬运 Event 到 Channel 时的性能。

② Channel 

type 选择memory时Channel的性能最好，但是如果Flume进程意外挂掉可能会丢失数据。type选择file时Channel的容错性更好，但是性能上会比memory channel差。

使用 Kafka Channel 兼顾性能和安全。

使用file Channel时dataDirs配置多个不同盘下的目录可以提高性能。

Capacity 参数决定Channel可容纳最大的event条数。transactionCapacity 参数决定每次Source往channel里面写的最大event条数和每次Sink从channel里面读的最大event条数。transactionCapacity需要大于Source和Sink的batchSize参数。

③ Sink 

增加Sink的个数可以增加Sink消费event的能力。Sink也不是越多越好够用就行，过多的Sink会占用系统资源，造成系统资源不必要的浪费。

batchSize 参数决定 Sink 一次批量从 Channel 读取的 event 条数，适当调大这个参数可以提高 Sink 从 Channel 搬出 event 的性能。

使用 Kafka Sink 构建合理的数据流拓扑结构，提高数据并发消费能力。

④ Flume 在数据量大的时候挂掉，可以在 $FLUME_HOME/conf/flume.env.sh 配置：

```bash
export JAVA_OPTS="-Xms4000m -Xmx4000m"
```



# 6. kafka 相关

## 6.1 系统基本架构图

![image-20200419141315528](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200419141315528.png)

## 6.2 消费者的消费一致性如何保证

![image-20200316202822164](C:\Users\Jeffery\AppData\Roaming\Typora\typora-user-images\image-20200316202822164.png)

LEO：log end offset。 指每个分布副本 log 文件最后的 offset 的值，即当前分区副本的最后一条记录的 offset。消费者提交的最新 offset 为其 offset + 1.

HW：high watermark，**ISR 中 LEO 最小的值**。

① HW 可以保证 consumer 在消费时，只有 HW 之前的数据可以消费，保证 leader 故障时，不会由于 leader 的更换造成消费数据的不一致。

② HW 还可以在 leader 重新选举时，使 ISR 中所有的 follower 都参照 HW，将之后多余的数据截掉，保持和 leader 的数据同步。

**注意：**对于从 OSR 切换到 ISR 的 blocker，会先截掉自己之前的 HW 后的数据，在与 leader 进行同步。

## 6.3 生产者分区策略

producer 将发送的数据封装成一个**ProducerRecord**对象。


（1）ProducerRecord 对象中指明 partition 的情况下，直接将指明的值直接作为 partiton 值；

（2）没有指明 partition 值但有 key 的情况下，将 key **序列化后**的 hash 值与 topic 的 partition 数进行取余得到 partition 值；

（3）既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是上文中说的 round-robin 算法。

## 6.4 消费者分区策略

独立消费者直接指定分区，且自行维护 off_set；非独立消费者由系统自动实现再平衡，且由 kafka 维护 off_set。

一个非独立消费者组中如果新加入了消费者或有消费者挂掉，此时都会由系统自动再进行主题分区的分配，这个过程称为再平衡。若创建的消费者是非独立消费者，此时会由 kafka 集群自动帮消费者组中的每个消费者分配要消费的分区自动分配时，有两种分配的策略：round_rabin(轮询分区) 和 range（范围分区）。

## 6.5 和 broker 端调优相关的参数

① 运行 kafka 压力测试，找到限制系统速度的瓶颈。

② 增加主题分区数，可以提高吞吐量。配置多目录，将分区放置在不同的磁盘，可以进一步提高吞吐量。

③ 增加消费者线程数（前提是有多个分区）。

④ 在消费者端配置优化参数，例如 offset 的异步提交等。

# 7. hive 相关

### 1. Hive 表关联查询，如何解决数据倾斜的问题？

① 提前清洗数据，将不合法数据例如 key 为 null 的数据进行过滤。

② 如果没用清洗则在查询时先过滤再关联表。

③ 如果一个数据中有很多数量较多的 key，可以增加 reducetask 个数，避免多个大 key 集中到一个 reducetask。

④ 将无意义的 nullkey 进行随机替换处理，同时增加 reducetask 的个数，以分散数据。

⑤ 转换为 mapjoin。

⑥ 大 key 和其他数据分开处理。

⑦ hive.optimize.skewjoin：会将一个join sql 分为两个job。另外可以同时设置下hive.skewjoin.key，默认为10000。参考：

https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties

参数对 full outer join 无效。

⑧ 调整内存设置：适用于那些由于内存超限内务被 kill 掉的场景。通过加大内存起码能让任务跑起来，不至于被杀掉。该参数不一定会明显降低任务执行时间。如：

setmapreduce.reduce.memory.mb=5120 ;

setmapreduce.reduce.java.opts=-Xmx5000M -XX:MaxPermSize=128m ;

### 2. 请谈一下 Hive 的特点，Hive 和 RDBMS 有什么异同？

Hive 的特点：基于OLAP设计的数据仓库软件，不适合实时计算，适合大数据量的计算，底层本质是MR。

和RDBMS的异同：

核心异同为RDBMS为OLTP（事务，实时），Hive为OLAP（分析，延迟）。

具体可参考 Hive 文档第1章1.4，主要在以下几个方面：查询语言、存储位置、数据更新、索引、执行引擎、执行延迟、可扩展性、数据规模。

 

### 3. 请说明 Hive 中 Sort By，Order By，Cluster By，Distrbute By 各代表什么意思？

① Sort By：部分排序，一个 Job 有多个 Reducer，每个 Reducer 处理的数据内部有序；

② Order By：全局排序，一个 Job 只有一个 Reducer；

③ Distrbute By：类似 MR 中 partition，进行分区，结合 sort by 使用，Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前，即先分区再排序。

④ Cluster By：当 distribute by 和 sorts by 字段相同时，可以使用 cluster by 方式。但是排序只能是升序排序，不能指定排序规则为 ASC 或者 DESC。

### 4. Hive 有哪些方式保存元数据，各有哪些特点？

① 默认存放在 derby。

② 修改存储在其他的关系型数据库例如 mysql, oracle, mss, progresql。

③ 通过使用 thrift 调用 metaServer 服务进行元数据的读写。

### 5. Hive内部表外部表的区别？

内部表也成为管理表，可以管理数据的生命周期，删除管理表时，数据也会随之删除。

外部表，只删除表结构（元数据）。

### 6. 写出将 text.txt 文件放入 Hive中 test 表‘2016-10-10’ 分区的语句，test 的分区字段是 l_date。

load data local inpath ‘/text.txt’ into table test partition(l_date=’ 2016-10-10’);

### 7. Hive 自定义 UDF 函数的流程

① 自定义一个 Java 类，继承 UDF 类，重写一个或多个 evaluate 方法

② evaluate() 的返回值不能为 void，可以是 null

③ 打成jar包，在 hive 中使用 add jar path_of_jar 或者直接将 jar 包放入到 hive 的 lib 目录中 

④ 在 hive中 使用 create function 函数名 as ‘全类名’ 声明函数

### 8. 对于Hive，你写过哪些 udf 函数，作用是什么？

传入 json 字符串和 key，获取 value。

### 9. Hive 中的压缩格式 TextFile、SequenceFile、RCfile 、ORCfile 各有什么区别？

TextFile：默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合压缩格式进行压缩。

SequenceFile: SequenceFile 是Hadoop API 提供的一种二进制文件，它将数据以<key,value>的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。Hive 中的 SequenceFile 继承自Hadoop API 的 SequenceFile，不过它的key为空，使用value 存放实际的值， 这样是为了避免MR 在运行map 阶段的排序过程。

RCFile: RCFile是Hive推出的一种专门面向列的数据格式。 它遵循“先按列划分，再垂直划分”的设计理念。当查询过程中，针对它并不关心的列时，它会在IO上跳过这些列。需要说明的是，RCFile在map阶段从 远端拷贝仍然是拷贝整个数据块，并且拷贝到本地目录后RCFile并不是真正直接跳过不需要的列，并跳到需要读取的列， 而是通过扫描每一个row group的头部定义来实现的，但是在整个HDFS Block 级别的头部并没有定义每个列从哪个row group起始到哪个row group结束。所以在读取所有列的情况下，RCFile的性能反而没有SequenceFile高。

ORCfile: ORC是列式存储，有多种文件压缩方式，并且有着很高的压缩比。文件是可切分（Split）的。因此，在Hive中使用ORC作为表的文件存储格式，不仅节省HDFS存储资源,查询任务的输入数据量减少，使用的MapTask也就减少了。提供了多种索引，row group index、bloom filter index。ORC可以支持复杂的数据结构（比如Map等）

### 10. Hive join 过程中大表小表的放置顺序？

小表 join 大表，当设置了 set hive.auto.convert.join=true 后，hive 会自动调整顺序。

### 11. Hive 的两张表关联，使用 MapReduce 怎么实现？

ReduceJoin: 

① 在 Map 端使用一个通用的 bean 来封装数据，这个 bean 中包含了两个表的所有字段。

② 在 Map 端处理时，为每个 bean 打上标记，标记当前数据的来源。

③ 在 Reduce 端，根据数据的来源将数据分类。

④ 在 Reduce 端进行字段的关联，且只处理需要处理的数据。

MapJoin:

① 根据两张表的数据的数据量，将两张表划分为大表和小表。

② 小表使用分布式缓存提前缓存，大表作为 MR 的输入，进行切片后读入到 MapTask。

③ 在 Mapper 的 map() 方法处理之前，提前从分布式缓存中读取小表中的数据。

④ 在 Mapper 的 map() 方法中，对大表的数据进行关联操作。

### 12. 所有的 Hive 任务都会有 MapReduce 的执行吗？

取决于 hive.fetch.task.conversion 的配置，默认配置为 more。

即当一个查询中只有 select、where，包括带分区字段的过滤查询和 limit 走 fetchtask，不走 MR。

### 13. Hive 的函数：UDF、UDAF、UDTF 的区别？

UDF：用户定义函数，一进一出。

UDAF：用户定义的聚集函数，多进一出。

UDTF：用户定义的表生成函数，一进多出。

### 14. 说说对Hive桶表的理解？

分桶也是为了分散数据。分桶的作用，可以按照字段将数据分散到多个文件。

可以使用抽样查询结合分桶操作，只选择指定的桶进行查询。

### 15. Hive 可以像关系型数据库那样建立多个库吗？

可以。

### 16. Hive 实现统计的查询语句是什么？

count()、max()、avg()、sum()、min()

rank()、row_number()、ntile()、dense_rank()

### 17. Hive 优化措施

① 在简单查询时使用 fetchtask。

② 在测试和小文件的试验中，本地模式速度快。

③ 能够使用 Mapjoin 尽量使用 mapjoin。

④ 聚合操作如果数据量过大，开启 map 端聚合，将原先的一个 MR 通过两个 MR 实现。

hive.map.aggr = true

hive.groupby.mapaggr.checkinterval = 100000

hive.groupby.skewindata = true

⑤ count(distinct)，在数据量过大时，可以先 group by 再执行 count 操作。

⑥ 开启严格模式，避免无效 hql 的执行。

⑦ 行列过滤

列过滤： 按需查询，避免select *。

行过滤： 在查询时尽量先通过 where 将数据集的范围缩小，再进行关联等计算。

⑧ 在执行hive时，提前对数据进行抽样和调查，合理设置 map 和 reduce 个数避免数据倾斜。

⑨ 在机器性能好的前提下，可以设置 MR 的并行执行。

​       set hive.exec.parallel=true; 

​       set hive.exec.parallel.thread.number=16;

⑩ 针对小文件过多造成的小任务过多，开启 jvm 重用。

mapreduce.job.jvm.numtasks=10-20

⑪ 在必要是更换hive的执行引擎，在 tez 或者 spark 上执行 hql 语句。

### 18. Hive中，建的表为压缩表，但是输入文件为非压缩格式，会产生怎样的现象或者结果？

如果使用 load 加载数据，那么文件在上传到表中时会报错；

如果使用 insert into 的方式插入，那么会以压缩格式存在。

### 19. 已知表 a 是一张内部表，如何将它转换成外部表？请写出相应的 hive 语句。

alter table a set tblproperties(‘EXTERNAL’=’TRUE’);

### 20. Hive 中 mapjoin 的原理和实际应用？

原理同 11 题的 MapJoin 原理。

作用是为了避免出现在 Reducer 端的数据倾斜。（shuffle 阶段会根据 key 值进行分区，而 key 值的分布不均会造成数据倾斜。如果使用了  MapJoin，那么就不会有 shuffle 阶段，进而就可以避免数据倾斜）

### 21. 订单详情表 ord_det(order_id 订单号，sku_id 商品编号，sale_qtty 销售数量，dt 日期分区)任务计算 2016年1月1日商品销量的 Top100，并按销量降级排序

select sku_id,sum(sale_qtty) sum_sale

from ord_det

where dt=20160101

group by sku_id

order by sum_sale desc

limit 100

### 21. hive 和 MapReduce 的关系

![img](E:\note\youdao\qq745C056D5E743D9039FEE344813AA557\ea611672aa0947749488d663a11c30f0\clipboard.png)